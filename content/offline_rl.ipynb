{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from typing import List\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Optional\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from minari import EpisodeData, MinariDataset\n",
    "import minari\n",
    "from gymnasium import Env\n",
    "import os\n",
    "from minari import get_normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation dim: 8, Action dim: 2\n",
      "Reward dim: 1, Value dim: 1\n",
      "Transition dim: 12\n",
      "Number of episodes: 100\n"
     ]
    }
   ],
   "source": [
    "def get_space_dim(space):\n",
    "    if isinstance(space, spaces.Discrete):\n",
    "        return 1\n",
    "    elif isinstance(space, spaces.Box):\n",
    "        return space.shape[0]\n",
    "    elif isinstance(space, spaces.Dict):\n",
    "        return sum([get_space_dim(v) for v in space.values()])\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported observation space\")\n",
    "\n",
    "\n",
    "dataset_ref = \"D4RL/pointmaze/medium-dense-v2\"\n",
    "base_m_dataset = minari.load_dataset(dataset_ref, download=True)\n",
    "wrapped_env = base_m_dataset.recover_environment(\n",
    "    render_mode=\"rgb_array\", continuing_task=False\n",
    ")\n",
    "env = wrapped_env.unwrapped\n",
    "# Environment parameters\n",
    "observation_dim = get_space_dim(env.observation_space)\n",
    "action_dim = get_space_dim(env.action_space)\n",
    "reward_dim = 1\n",
    "value_dim = 1\n",
    "transition_dim = observation_dim + action_dim + reward_dim + value_dim\n",
    "\n",
    "print(f\"Observation dim: {observation_dim}, Action dim: {action_dim}\")\n",
    "print(f\"Reward dim: {reward_dim}, Value dim: {value_dim}\")\n",
    "print(f\"Transition dim: {transition_dim}\")\n",
    "\n",
    "local = not torch.cuda.is_available()\n",
    "\n",
    "# Model parameters\n",
    "n_transitions = 10\n",
    "seq_len = n_transitions * transition_dim\n",
    "vocab_size = 100\n",
    "max_bins = vocab_size\n",
    "discount_factor = 0.99\n",
    "embedding_dim = 128 if not local else 32\n",
    "n_heads = 4\n",
    "n_blocks = 4\n",
    "n_epochs = 25 if not local else 1\n",
    "batch_size = 256\n",
    "lr = 0.0006\n",
    "eval_every = 5 if not local else 1\n",
    "\n",
    "# other parameters\n",
    "n_episodes: Optional[int] = None if not local else 100\n",
    "# create a directory to save the model\n",
    "base_dir = f\"data/{dataset_ref}\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "checkpoint_path = f\"{base_dir}/transformer.ckpt\"\n",
    "load_checkpoint = (\n",
    "    False  # set to False if you want to train from scratch even if a checkpoint exists\n",
    ")\n",
    "if n_episodes:\n",
    "    m_dataset = base_m_dataset.sample_episodes(n_episodes)\n",
    "else:\n",
    "    m_dataset = base_m_dataset\n",
    "\n",
    "print(f\"Number of episodes: {len(m_dataset)}\")\n",
    "\n",
    "device = torch.device(\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KBinsDiscretizer:\n",
    "    def __init__(self, dataset: np.ndarray, n_bins: int, strategy: str = \"ordinal\"):\n",
    "        self.n_bins = n_bins\n",
    "        self.strategy = strategy\n",
    "\n",
    "        # bin_edges shape: (n_features, n_bins + 1)\n",
    "        self.bin_edges = self._find_bin_edges(dataset)\n",
    "\n",
    "        self.bin_centers = (self.bin_edges[:, :-1] + self.bin_edges[:, 1:]) * 0.5\n",
    "        self.bin_centers_torch = torch.from_numpy(self.bin_centers).float()\n",
    "\n",
    "    def _find_bin_edges(self, dataset: np.ndarray):\n",
    "        bin_edges = []\n",
    "        if self.strategy == \"uniform\":\n",
    "            mins, maxs = np.min(dataset, axis=0), np.max(dataset, axis=0)\n",
    "            bin_edges = np.linspace(mins, maxs, self.n_bins + 1).T\n",
    "        elif self.strategy == \"quantile\":\n",
    "            quantiles = np.linspace(0, 100, self.n_bins + 1)\n",
    "            bin_edges = np.percentile(dataset, quantiles, axis=0).T\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {self.strategy}\")\n",
    "        return bin_edges\n",
    "\n",
    "    def encode(\n",
    "        self, X: np.ndarray, subslice: Optional[Tuple[int, int]] = None\n",
    "    ) -> np.ndarray:\n",
    "        if X.ndim == 1:\n",
    "            # this is to handle the case where we have a single sample\n",
    "            X = X[None]\n",
    "        # data shape: (n_samples, n_features)\n",
    "        edges = self.bin_edges\n",
    "        if subslice is not None:\n",
    "            start, end = subslice\n",
    "            edges = edges[start:end]\n",
    "\n",
    "        Xt = np.zeros(X.shape, dtype=np.long)\n",
    "\n",
    "        # See documentation of numpy.isclose for an explanation of ``rtol`` and ``atol``.\n",
    "        rtol = 1.0e-5\n",
    "        atol = 1.0e-8\n",
    "\n",
    "        for jj in range(X.shape[1]):\n",
    "            # Values which are close to a bin edge are susceptible to numeric\n",
    "            # instability. Add eps to X so these values are binned correctly\n",
    "            # with respect to their decimal truncation.\n",
    "            eps = atol + rtol * np.abs(X[:, jj])\n",
    "            # why [1:]? bins = edges - 1, but its unclear why we leave out the first element and not the last\n",
    "            Xt[:, jj] = np.digitize(X[:, jj] + eps, edges[jj][1:])\n",
    "\n",
    "        np.clip(Xt, 0, self.n_bins - 1, out=Xt)\n",
    "\n",
    "        return Xt\n",
    "\n",
    "    def decode(\n",
    "        self, Xt: np.ndarray, subslice: Optional[Tuple[int, int]] = None\n",
    "    ) -> np.ndarray:\n",
    "        if Xt.ndim == 1:\n",
    "            # this is to handle the case where we have a single sample\n",
    "            Xt = Xt[None]\n",
    "        # data shape: (n_samples, n_features)\n",
    "        centers = self.bin_centers\n",
    "        if subslice is not None:\n",
    "            start, end = subslice\n",
    "            centers = centers[start:end]\n",
    "\n",
    "        X = np.zeros(Xt.shape, dtype=np.float64)\n",
    "        for jj in range(Xt.shape[1]):\n",
    "            X[:, jj] = centers[jj, np.int_(Xt[:, jj])]\n",
    "\n",
    "        return X\n",
    "\n",
    "    def expectation(\n",
    "        self, probs: np.ndarray, subslice: Optional[Tuple[int, int]] = None\n",
    "    ) -> np.ndarray:\n",
    "        if probs.ndim == 1:\n",
    "            # this is to handle the case where we have a single sample\n",
    "            probs = probs[None]\n",
    "        # probs shape: (batch_size, n_features, n_bins)\n",
    "        # bin_centers shape: (n_features, n_bins) -> (1, n_features, n_bins)\n",
    "        if torch.is_tensor(probs):\n",
    "            bin_centers = self.bin_centers_torch.unsqueeze(0)\n",
    "        else:\n",
    "            # bin_centers shape: (n_features, n_bins) -> (1, n_features, n_bins)\n",
    "            bin_centers = self.bin_centers[None]\n",
    "\n",
    "        if subslice is not None:\n",
    "            start, end = subslice\n",
    "            bin_centers = bin_centers[:, start:end]\n",
    "\n",
    "        # (batch_size, n_features, n_bins) * (1, n_features, n_bins) -> sum (batch_size, n_features, n_bins) -> (batch_size, n_features)\n",
    "        X = (probs * bin_centers).sum(axis=-1)\n",
    "        return X\n",
    "\n",
    "    def to(self, device):\n",
    "        self.bin_centers_torch = self.bin_centers_torch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Test array\n",
    "test_arr = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Initialize the discretizer\n",
    "discretizer = KBinsDiscretizer(test_arr, 1000, strategy=\"uniform\")\n",
    "\n",
    "# Encode and decode the test array\n",
    "encoded = discretizer.encode(test_arr)\n",
    "decoded = discretizer.decode(encoded)\n",
    "\n",
    "# Check if the decoded array is close to the original array\n",
    "assert np.isclose(\n",
    "    decoded, test_arr, atol=1e-2\n",
    ").all(), f\"Decoded array {decoded} is not close to the original array {test_arr}\"\n",
    "\n",
    "# Generate random probabilities\n",
    "probs = F.softmax(torch.from_numpy(np.random.rand(3, 2, 1000)), dim=-1).numpy()\n",
    "\n",
    "# Calculate the expectation\n",
    "expectation = discretizer.expectation(probs)\n",
    "\n",
    "# Check if the expectation is close to the mean of the test array\n",
    "expected_mean = np.tile(np.mean(test_arr, axis=0), (3, 1))\n",
    "assert np.isclose(\n",
    "    expectation, expected_mean, atol=1e-1\n",
    ").all(), f\"Expectation {expectation} is not close to the expected mean {expected_mean}\"\n",
    "\n",
    "print(\"All tests passed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed successfully.\n"
     ]
    }
   ],
   "source": [
    "def flatten_space(s_dict: Any, space: spaces.Space) -> np.ndarray:\n",
    "    if isinstance(space, spaces.Discrete):\n",
    "        return s_dict\n",
    "    elif isinstance(space, spaces.Box):\n",
    "        return s_dict\n",
    "    elif isinstance(space, spaces.Dict):\n",
    "        return np.concatenate([s_dict[k] for k in space.spaces.keys()], axis=-1)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported observation space\")\n",
    "\n",
    "\n",
    "def unflatten_space(s_flat: np.ndarray, space: spaces.Space) -> dict:\n",
    "    if isinstance(space, spaces.Discrete):\n",
    "        return s_flat\n",
    "    elif isinstance(space, spaces.Box):\n",
    "        return s_flat\n",
    "    elif isinstance(space, spaces.Dict):\n",
    "        s_dict = {}\n",
    "        start = 0\n",
    "        for k, v in space.spaces.items():\n",
    "            end = start + get_space_dim(v)\n",
    "            s_dict[k] = s_flat[:, start:end]\n",
    "            start = end\n",
    "        return s_dict\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported observation space\")\n",
    "\n",
    "\n",
    "# Test the flatten_space_dict and unflatten_space_dict functions\n",
    "test_dict = {\"obs\": np.array([[1, 2, 3], [4, 5, 6]]), \"act\": np.array([[0], [1]])}\n",
    "test_space = spaces.Dict(\n",
    "    {\"obs\": spaces.Box(low=0, high=10, shape=(3,)), \"act\": spaces.Discrete(2)}\n",
    ")\n",
    "test_flat = flatten_space(test_dict, test_space)\n",
    "test_unflat = unflatten_space(test_flat, test_space)\n",
    "\n",
    "assert np.isclose(\n",
    "    test_flat, np.array([[0, 1, 2, 3], [1, 4, 5, 6]])\n",
    ").all(), f\"Flattened array {test_flat} is not as expected.\"\n",
    "assert np.isclose(\n",
    "    test_unflat[\"obs\"], test_dict[\"obs\"]\n",
    ").all(), f\"Unflattened observation {test_unflat['obs']} is not as expected.\"\n",
    "assert np.isclose(\n",
    "    test_unflat[\"act\"], test_dict[\"act\"]\n",
    ").all(), f\"Unflattened action {test_unflat['act']} is not as expected.\"\n",
    "\n",
    "# test discrete space\n",
    "test_dict = np.array([[0], [1]])\n",
    "test_space = spaces.Discrete(2)\n",
    "test_flat = flatten_space(test_dict, test_space)\n",
    "test_unflat = unflatten_space(test_flat, test_space)\n",
    "\n",
    "assert np.isclose(\n",
    "    test_flat, test_dict\n",
    ").all(), f\"Flattened array {test_flat} is not as expected.\"\n",
    "assert np.isclose(\n",
    "    test_unflat, test_dict\n",
    ").all(), f\"Unflattened array {test_unflat} is not as expected.\"\n",
    "\n",
    "# test box space\n",
    "test_dict = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "test_space = spaces.Box(low=0, high=10, shape=(3,))\n",
    "test_flat = flatten_space(test_dict, test_space)\n",
    "test_unflat = unflatten_space(test_flat, test_space)\n",
    "\n",
    "assert np.isclose(\n",
    "    test_flat, test_dict\n",
    ").all(), f\"Flattened array {test_flat} is not as expected.\"\n",
    "assert np.isclose(\n",
    "    test_unflat, test_dict\n",
    ").all(), f\"Unflattened array {test_unflat} is not as expected.\"\n",
    "\n",
    "print(\"All tests passed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_trajectory(env: Env, episode: EpisodeData, discount: float = 0.99):\n",
    "    trajectory_len = episode.rewards.shape[0]\n",
    "    observations = episode.observations\n",
    "    actions = episode.actions\n",
    "    rewards = episode.rewards\n",
    "\n",
    "    values = np.zeros(trajectory_len)\n",
    "    # calculate rewards to go with discount\n",
    "    for i in range(trajectory_len - 1, -1, -1):\n",
    "        values[i] = rewards[i] + (\n",
    "            discount * values[i + 1] if i + 1 < trajectory_len else 0\n",
    "        )\n",
    "\n",
    "    # drop the last state because we don't have a reward for it\n",
    "    states = flatten_space(observations, env.observation_space)\n",
    "    states = states[:-1, :].reshape(trajectory_len, -1)\n",
    "    actions = actions.reshape(trajectory_len, -1)\n",
    "    rewards = rewards.reshape(trajectory_len, -1)\n",
    "    values = values.reshape(trajectory_len, -1)\n",
    "\n",
    "    joined = np.concatenate([states, actions, rewards, values], axis=-1)\n",
    "\n",
    "    return joined\n",
    "\n",
    "\n",
    "class DiscretizeDataset(Dataset):\n",
    "    # Each input into the sequence model needs to be (batch_size, tokens)\n",
    "    # output should be in groups of transitions\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: Env,\n",
    "        m_dataset: MinariDataset,\n",
    "        n_transitions: int,\n",
    "        discount: float = 0.99,\n",
    "        max_bins: int = 1000,\n",
    "    ):\n",
    "        self.m_dataset = m_dataset\n",
    "        self.n_transitions = n_transitions\n",
    "\n",
    "        self.joined_trajectories = []\n",
    "        for episode in m_dataset:\n",
    "            self.joined_trajectories.append(join_trajectory(env, episode, discount))\n",
    "\n",
    "        self.discretizer = KBinsDiscretizer(\n",
    "            n_bins=max_bins,\n",
    "            strategy=\"quantile\",\n",
    "            dataset=np.concatenate(self.joined_trajectories, axis=0),\n",
    "        )\n",
    "\n",
    "        indices = []\n",
    "        for traj_idx, joined_trajectory in enumerate(self.joined_trajectories):\n",
    "            traj_len = joined_trajectory.shape[0]\n",
    "            end = traj_len - 1\n",
    "            for i in range(end):\n",
    "                indices.append((traj_idx, i, i + n_transitions))\n",
    "\n",
    "        self.indices = np.array(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        traj_idx, start, end = self.indices[idx]\n",
    "        # shape (start - end)\n",
    "        joined = self.joined_trajectories[traj_idx][start:end]\n",
    "        loss_pad_mask = np.ones(\n",
    "            (self.n_transitions, joined.shape[-1]), dtype=np.float32\n",
    "        )\n",
    "        if joined.shape[0] < self.n_transitions:\n",
    "            joined = np.pad(\n",
    "                joined,\n",
    "                ((0, self.n_transitions - joined.shape[0]), (0, 0)),\n",
    "                mode=\"constant\",\n",
    "                constant_values=0,\n",
    "            )\n",
    "            loss_pad_mask[joined.shape[0] :] = 0\n",
    "\n",
    "        joined_discretized = self.discretizer.encode(joined)\n",
    "        joined_discretized = joined_discretized.reshape(-1).astype(np.long)\n",
    "        loss_pad_mask = loss_pad_mask.reshape(-1)\n",
    "        return joined_discretized[:-1], joined_discretized[1:], loss_pad_mask[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 22905\n",
      "Shape of dataset: (119,), (119,)\n"
     ]
    }
   ],
   "source": [
    "dataset = DiscretizeDataset(\n",
    "    env=env,\n",
    "    m_dataset=m_dataset,\n",
    "    n_transitions=n_transitions,\n",
    "    discount=discount_factor,\n",
    "    max_bins=max_bins,\n",
    ")\n",
    "\n",
    "print(f\"Length of dataset: {len(dataset)}\")\n",
    "print(f\"Shape of dataset: {dataset[0][0].shape}, {dataset[0][1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, seq_len, embedding_dim: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embedding_dim, n_heads, batch_first=True)\n",
    "        self.attn_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_dim, embedding_dim * 2)\n",
    "        self.fc2 = nn.Linear(embedding_dim * 2, embedding_dim)\n",
    "        self.fc_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # mask value of true means that the value is not allowed to be attended to\n",
    "        self.register_buffer(\"mask\", ~torch.tril(torch.ones(seq_len, seq_len)).bool())\n",
    "        # transition_dim - 1 stores rewards to go, we don't want to attend to them because they contain future information\n",
    "        self.mask[:, transition_dim - 1 :: transition_dim] = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape (batch_size, n_tokens, embedding_dim)\n",
    "        _, n_tokens, _ = x.shape\n",
    "        # attn_mask shape (seq_len, seq_len), but incoming shape is (batch_size, n_tokens, embedding_dim)\n",
    "        # so filter the mask to the correct size (n_tokens, n_tokens)\n",
    "        attn_mask = self.mask[:n_tokens, :n_tokens]\n",
    "        # attn_output shape (batch_size, n_tokens, embedding_dim)\n",
    "        attn_output, _ = self.attn(x, x, x, attn_mask=attn_mask)\n",
    "        # add and norm\n",
    "        # attn_output shape (batch_size, n_tokens, embedding_dim)\n",
    "        attn_output = self.attn_norm(attn_output + x)\n",
    "\n",
    "        # fc_output shape (batch_size, n_tokens, embedding_dim)\n",
    "        fc_output = self.fc1(attn_output)\n",
    "        fc_output = torch.relu(fc_output)\n",
    "        fc_output = self.fc2(fc_output)\n",
    "        fc_output = self.fc_norm(fc_output + attn_output)\n",
    "\n",
    "        return fc_output\n",
    "\n",
    "\n",
    "class TrajectoryTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_len: int,\n",
    "        embedding_dim: int,\n",
    "        n_heads: int,\n",
    "        transition_dim: int,\n",
    "        n_blocks: int,\n",
    "        vocab_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.transition_dim = transition_dim\n",
    "        self.n_blocks = n_blocks\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.token_embedding = nn.Embedding(seq_len * vocab_size, self.embedding_dim)\n",
    "        self.positional_embedding = nn.Parameter(\n",
    "            torch.zeros(1, seq_len, self.embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(self.seq_len, self.embedding_dim, self.n_heads)\n",
    "                for _ in range(self.n_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(self.embedding_dim, vocab_size)\n",
    "\n",
    "    def get_seq_len(self):\n",
    "        return self.seq_len\n",
    "\n",
    "    def _offset_tokens(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        _, n_tokens = tokens.shape\n",
    "        n_transition = np.ceil(n_tokens / self.transition_dim).astype(int)\n",
    "\n",
    "        offsets = (\n",
    "            torch.arange(self.transition_dim, device=tokens.device) * self.vocab_size\n",
    "        )\n",
    "        # repeat the offset n_transition times\n",
    "        offsets = offsets.repeat(n_transition)\n",
    "        offset_idx = offsets[:n_tokens] + tokens\n",
    "        return offset_idx\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        # tokens shape (batch_size, n_tokens)\n",
    "        _, n_tokens = tokens.shape\n",
    "        assert (\n",
    "            n_tokens <= self.seq_len\n",
    "        ), f\"n_tokens {n_tokens} is greater than seq_len {self.seq_len}\"\n",
    "\n",
    "        # project each token into their vocab space, this is similar to tokenization\n",
    "        # (batch_size, n_tokens)\n",
    "        offset_idx = self._offset_tokens(tokens)\n",
    "\n",
    "        # (batch_size, n_tokens, embedding_dim)\n",
    "        tokens = self.token_embedding(offset_idx)\n",
    "        positional_embedding = self.positional_embedding[:, :n_tokens]\n",
    "        tokens += positional_embedding\n",
    "\n",
    "        for block in self.blocks:\n",
    "            tokens = block(tokens)\n",
    "        # (batch_size, n_tokens, embedding_dim) -> (batch_size, n_tokens, vocab_size)\n",
    "        logits = self.fc(tokens)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_token_from_logits(\n",
    "    logits: torch.Tensor,\n",
    "    temperature: float = 1.0,\n",
    "    greedy: bool = False,\n",
    "    top_k: Optional[int] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Sample the next token from the logits using temperature scaling and top-k sampling.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): The model's predicted logits.\n",
    "        temperature (float): The temperature scaling factor.\n",
    "        greedy (bool): Whether to sample greedily.\n",
    "        top_k (Optional[int]): The top-k sampling parameter.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The sampled token.\n",
    "    \"\"\"\n",
    "    # Apply temperature scaling\n",
    "    if temperature != 1.0:\n",
    "        logits = logits / temperature\n",
    "\n",
    "    # Apply top-k sampling\n",
    "    if top_k is not None:\n",
    "        # Apply top-k sampling\n",
    "        # (batch_size, vocab_size) -> (batch_size, top_k)\n",
    "        v, indices = torch.topk(logits, top_k, dim=-1)\n",
    "        # set all logits to -inf except the top-k indices\n",
    "        logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # Sample from the top-k indices\n",
    "        # (batch_size, top_k) -> (batch_size, 1)\n",
    "        idx = torch.multinomial(probs, num_samples=1)\n",
    "    else:\n",
    "        # Sample from the logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        if not greedy:\n",
    "            idx = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            # Greedy sampling\n",
    "            _, idx = torch.max(probs, dim=-1)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def round_to_multiple(number, multiple):\n",
    "    pad = (multiple - number % multiple) % multiple\n",
    "    return number + pad\n",
    "\n",
    "\n",
    "def sample_tokens(\n",
    "    model: nn.Module,\n",
    "    context: nn.Module,\n",
    "    n_steps: int,\n",
    "    temperature: float = 1.0,\n",
    "    greedy: bool = False,\n",
    "    top_k: Optional[int] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Sample a sequence of tokens from the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to sample from.\n",
    "        context (nn.Module): The context to condition the sampling on.\n",
    "        n_steps (int): The number of steps to sample.\n",
    "        temperature (float): The temperature scaling factor.\n",
    "        greedy (bool): Whether to sample greedily.\n",
    "        top_k (Optional[int]): The top-k sampling parameter.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The sampled tokens.\n",
    "    \"\"\"\n",
    "    raw_logits = torch.zeros(\n",
    "        context.shape[0], n_steps, vocab_size, device=context.device\n",
    "    )\n",
    "    for i in range(n_steps):\n",
    "        # crop the context so that it doesn't exceed the seq_len\n",
    "        curr_context_len = context.shape[1]\n",
    "        n_crop = round_to_multiple(\n",
    "            max(0, curr_context_len - model.get_seq_len()), transition_dim\n",
    "        )\n",
    "        context = context[:, n_crop:]\n",
    "        # Get the model's prediction\n",
    "        # (batch_size, seq_len, vocab_size)\n",
    "        logits = model(context)\n",
    "        # Sample the next token\n",
    "        # (batch_size, 1)\n",
    "        token = sample_token_from_logits(\n",
    "            logits[:, -1], temperature=temperature, greedy=greedy, top_k=top_k\n",
    "        )\n",
    "\n",
    "        context = torch.cat([context, token], dim=-1)\n",
    "\n",
    "        raw_logits[:, i] = logits[:, -1]\n",
    "    return context, raw_logits\n",
    "\n",
    "\n",
    "def beam_plan(\n",
    "    model: nn.Module,\n",
    "    discretizer: KBinsDiscretizer,\n",
    "    context: torch.Tensor,\n",
    "    beam_width: int,\n",
    "    beam_steps: int,\n",
    "    beam_context: int,\n",
    "    sample_expansion: int,\n",
    "    observation_dim: int,\n",
    "    action_dim: int,\n",
    "    reward_dim: int,\n",
    "    value_dim: int,\n",
    "    transition_dim: int,\n",
    "    obs_top_k: Optional[int] = None,\n",
    "    act_top_k: Optional[int] = None,\n",
    "    rew_top_k: Optional[int] = None,\n",
    "    temperature: float = 1.0,\n",
    "    greedy: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    tokens_context_size = beam_context * transition_dim\n",
    "    n_crop = round_to_multiple(\n",
    "        max(0, context.shape[1] - tokens_context_size), transition_dim\n",
    "    )\n",
    "    context = context[:, n_crop:]\n",
    "    # context shape (seq_len) -> (beam_width, seq_len)\n",
    "    plan = context.repeat(beam_width, 1)\n",
    "\n",
    "    rewards = torch.zeros(beam_width, beam_steps + 1, device=context.device)\n",
    "    discounts = discount_factor ** torch.arange(beam_steps + 1, device=context.device)\n",
    "\n",
    "    for t in trange(beam_steps, desc=\"Beam Search\", leave=False):\n",
    "        # (beam_width, n_tokens) -> (beam_width * sample_expansion, n_tokens)\n",
    "        plan = plan.repeat(sample_expansion, 1)\n",
    "        rewards = rewards.repeat(sample_expansion, 1)\n",
    "\n",
    "        # sample actions\n",
    "        plan, _ = sample_tokens(\n",
    "            model,\n",
    "            plan,\n",
    "            n_steps=action_dim,\n",
    "            top_k=act_top_k,\n",
    "            temperature=temperature,\n",
    "            greedy=greedy,\n",
    "        )\n",
    "\n",
    "        # sample rewards and values\n",
    "        # plan (beam_width * sample_expansion, n_tokens) -> (beam_width * sample_expansion, n_tokens + reward_dim + value_dim)\n",
    "        # logits shape (beam_width * sample_expansion, reward_dim + value_dim, vocab_size)\n",
    "        plan, logits = sample_tokens(\n",
    "            model,\n",
    "            plan,\n",
    "            n_steps=reward_dim + value_dim,\n",
    "            top_k=rew_top_k,\n",
    "            temperature=temperature,\n",
    "            greedy=greedy,\n",
    "        )\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        rewards_and_values = discretizer.expectation(\n",
    "            probs, subslice=(transition_dim - 2, transition_dim)\n",
    "        )\n",
    "\n",
    "        rewards[:, t : t + 2] = rewards_and_values\n",
    "        # Did you notice that rewards contains rewards at t and values at t+1, why?\n",
    "        #   when we want to calculate value (rewards to go) at t, we need to consider discounted rewards from 0 to t\n",
    "        #   and also future discounted rewards from t+1 to end. It is a bit awkward to apply discount factor to value (at t+1)\n",
    "        #   because predicted value is already discounted.\n",
    "        # (beam_width * sample_expansion, beam_steps + 1) * (beam_steps + 1) -> (beam_width * sample_expansion)\n",
    "        values = (rewards * discounts).sum(dim=-1)\n",
    "\n",
    "        # select top-k sequences\n",
    "        # (beam_width * sample_expansion) -> (beam_width)\n",
    "        values, idx = torch.topk(values, beam_width)\n",
    "\n",
    "        plan, rewards = plan[idx], rewards[idx]\n",
    "\n",
    "        if t < beam_steps - 1:\n",
    "            # sample observations only if we are not at the last step, why?\n",
    "            # because beam plan has to end with a valid transition [...., obs, act, rew, val]\n",
    "            plan, _ = sample_tokens(\n",
    "                model,\n",
    "                plan,\n",
    "                n_steps=observation_dim,\n",
    "                top_k=obs_top_k,\n",
    "                temperature=temperature,\n",
    "                greedy=greedy,\n",
    "            )\n",
    "\n",
    "    best_idx = torch.argmax(values)\n",
    "    # only return the best plan without the context\n",
    "    best_plan = plan[best_idx, context.shape[1] :]\n",
    "\n",
    "    return best_plan\n",
    "\n",
    "\n",
    "def rollout(\n",
    "    model: nn.Module,\n",
    "    env: Env,\n",
    "    discretizer: KBinsDiscretizer,\n",
    "    beam_width: int,\n",
    "    beam_steps: int,\n",
    "    beam_context: int,\n",
    "    sample_expansion: int,\n",
    "    observation_dim: int,\n",
    "    action_dim: int,\n",
    "    reward_dim: int,\n",
    "    value_dim: int,\n",
    "    transition_dim: int,\n",
    "    max_steps: int,\n",
    "    plan_every: int,\n",
    "    obs_top_k: Optional[int] = None,\n",
    "    act_top_k: Optional[int] = None,\n",
    "    rew_top_k: Optional[int] = None,\n",
    "    temperature: float = 1.0,\n",
    "    greedy: bool = False,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    "    generate_gif: bool = False,\n",
    "):\n",
    "    trajectory = []\n",
    "    assert (\n",
    "        plan_every <= beam_steps\n",
    "    ), f\"plan_every {plan_every} should be less than or equal to beam_steps {beam_steps}\"\n",
    "    obs, _ = env.reset()\n",
    "    imgs = []\n",
    "    if generate_gif:\n",
    "        imgs.append(env.render())\n",
    "    obs = flatten_space(obs, env.observation_space)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    context = torch.zeros(\n",
    "        (1, (max_steps + 1) * transition_dim), device=device, dtype=torch.long\n",
    "    )\n",
    "\n",
    "    context_idx = 0\n",
    "\n",
    "    # discretize the observation\n",
    "    obs_token = discretizer.encode(\n",
    "        np.array([obs]), subslice=(0, observation_dim)\n",
    "    ).squeeze()\n",
    "\n",
    "    context[:, :observation_dim] = torch.tensor(obs_token, device=device)\n",
    "\n",
    "    for t in trange(max_steps, desc=\"Rollout\", leave=False):\n",
    "        if t % plan_every == 0:\n",
    "            # we need to plan a new trajectory, reset the context observation at t step\n",
    "            context_idx = (\n",
    "                ((t + 1) * transition_dim) - action_dim - reward_dim - value_dim\n",
    "            )\n",
    "            predicted_tokens = beam_plan(\n",
    "                model,\n",
    "                discretizer,\n",
    "                context[:, :context_idx],\n",
    "                beam_width,\n",
    "                beam_steps,\n",
    "                beam_context,\n",
    "                sample_expansion,\n",
    "                observation_dim,\n",
    "                action_dim,\n",
    "                reward_dim,\n",
    "                value_dim,\n",
    "                transition_dim,\n",
    "                obs_top_k=obs_top_k,\n",
    "                act_top_k=act_top_k,\n",
    "                rew_top_k=rew_top_k,\n",
    "                temperature=temperature,\n",
    "                greedy=greedy,\n",
    "            )\n",
    "        else:\n",
    "            predicted_tokens = predicted_tokens[transition_dim:]\n",
    "\n",
    "        # get the action from the predicted tokens\n",
    "        action_token = predicted_tokens[:action_dim].cpu().numpy()\n",
    "        # decode the action\n",
    "        action = discretizer.decode(\n",
    "            action_token, subslice=(observation_dim, observation_dim + action_dim)\n",
    "        ).squeeze()\n",
    "        action = unflatten_space(action, env.action_space)\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_obs = flatten_space(next_obs, env.observation_space)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if generate_gif:\n",
    "            imgs.append(env.render())\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # discretize the next observation\n",
    "        next_obs_token = discretizer.encode(next_obs, subslice=(0, observation_dim))\n",
    "        # discretize the reward and value\n",
    "        reward_value_tokens = discretizer.encode(\n",
    "            np.array([reward, 0]), subslice=(transition_dim - 2, transition_dim)\n",
    "        )\n",
    "\n",
    "        # update the context\n",
    "        context_idx = t * transition_dim\n",
    "        # add action\n",
    "        context[\n",
    "            :,\n",
    "            context_idx + observation_dim : context_idx + observation_dim + action_dim,\n",
    "        ] = torch.as_tensor(action_token, device=device)\n",
    "        # add reward and value\n",
    "        context[\n",
    "            :, context_idx + observation_dim + action_dim : context_idx + transition_dim\n",
    "        ] = torch.as_tensor(reward_value_tokens, device=device)\n",
    "        # add next observation\n",
    "        context[\n",
    "            :,\n",
    "            context_idx\n",
    "            + transition_dim : context_idx\n",
    "            + transition_dim\n",
    "            + observation_dim,\n",
    "        ] = torch.as_tensor(next_obs_token, device=device)\n",
    "\n",
    "        trajectory.append((obs, next_obs, action, reward, done))\n",
    "        obs = next_obs\n",
    "\n",
    "    if generate_gif:\n",
    "        return total_reward, trajectory, imgs, terminated\n",
    "    return total_reward, trajectory, _, terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Training model from scratch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff463b795014100a0f8d67d5cf4cec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04aeb9e97ee942ceb8aeb94e3d5cce8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0f9da3b0614effafa913186ae869fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79613c0e7a14b6d899fadfd649d5ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rollout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff5dbaceac844239d80e231a53673db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15970b1df5bf4d3cb6332404cc819395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to data/D4RL/pointmaze/medium-dense-v2/transformer.ckpt\n"
     ]
    }
   ],
   "source": [
    "def calculate_loss(\n",
    "    model: nn.Module,\n",
    "    batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
    "    vocab_size: int,\n",
    "    transition_dim: int,\n",
    "    observation_dim: int,\n",
    "    action_dim: int,\n",
    "    reward_dim: int,\n",
    "    value_dim: int,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> torch.Tensor:\n",
    "    # inputs shape (batch_size, seq_len)\n",
    "    # targets shape (batch_size, seq_len)\n",
    "    # loss_pad_mask shape (batch_size, seq_len)\n",
    "    inputs, targets, loss_pad_mask = batch\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    loss_pad_mask = loss_pad_mask.to(device)\n",
    "    # logits shape (batch_size, seq_len, vocab_size)\n",
    "    logits = model(inputs)\n",
    "    # flatten the logits and targets to shape (batch_size * seq_len, vocab_size)\n",
    "    logits = logits.view(-1, vocab_size)\n",
    "    # flatten the targets to shape (batch_size * seq_len)\n",
    "    targets = targets.view(-1)\n",
    "    # loss shape (batch_size * seq_len)\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    n_states = int(np.ceil(inputs.shape[1] / transition_dim))\n",
    "    weights = torch.cat(\n",
    "        [\n",
    "            torch.ones(observation_dim, device=inputs.device),\n",
    "            torch.ones(action_dim, device=inputs.device) * 5,\n",
    "            torch.ones(reward_dim, device=inputs.device) * 1,\n",
    "            torch.ones(value_dim, device=inputs.device) * 1,\n",
    "        ]\n",
    "    )\n",
    "    weights = weights.repeat(n_states)[1:].repeat(inputs.shape[0], 1)\n",
    "    loss = loss * weights.view(-1)\n",
    "    # apply the loss pad mask to the loss because we don't want to calculate the loss for padded values\n",
    "    loss = (loss * loss_pad_mask.view(-1)).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def eval(\n",
    "    env: Env,\n",
    "    model: nn.Module,\n",
    "    discretizer: KBinsDiscretizer,\n",
    "    num_episodes: int,\n",
    "    beam_width: int,\n",
    "    beam_steps: int,\n",
    "    beam_context: int,\n",
    "    sample_expansion: int,\n",
    "    observation_dim: int,\n",
    "    action_dim: int,\n",
    "    reward_dim: int,\n",
    "    value_dim: int,\n",
    "    transition_dim: int,\n",
    "    max_steps: int,\n",
    "    plan_every: int,\n",
    "    obs_top_k: Optional[int] = None,\n",
    "    act_top_k: Optional[int] = None,\n",
    "    rew_top_k: Optional[int] = None,\n",
    "    temperature: float = 1.0,\n",
    "    greedy: bool = False,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    total_rewards = []\n",
    "\n",
    "    dones = []\n",
    "    for _ in trange(num_episodes, desc=\"Evaluating episode\", leave=False):\n",
    "        total_reward, _, _, done = rollout(\n",
    "            model,\n",
    "            env,\n",
    "            discretizer,\n",
    "            beam_width,\n",
    "            beam_steps,\n",
    "            beam_context,\n",
    "            sample_expansion,\n",
    "            observation_dim,\n",
    "            action_dim,\n",
    "            reward_dim,\n",
    "            value_dim,\n",
    "            transition_dim,\n",
    "            max_steps,\n",
    "            plan_every,\n",
    "            obs_top_k=obs_top_k,\n",
    "            act_top_k=act_top_k,\n",
    "            rew_top_k=rew_top_k,\n",
    "            temperature=temperature,\n",
    "            greedy=greedy,\n",
    "            device=device,\n",
    "        )\n",
    "        total_rewards.append(total_reward)\n",
    "        dones.append(done)\n",
    "\n",
    "    mean_rewards = np.mean(total_rewards)\n",
    "    std_rewards = np.std(total_rewards)\n",
    "\n",
    "    scores = [get_normalized_score(base_m_dataset, r) for r in total_rewards]\n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "\n",
    "    done_ratio = np.mean(dones)\n",
    "\n",
    "    model.train()\n",
    "    return mean_rewards, std_rewards, mean_score, std_score, done_ratio\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    vocab_size: int,\n",
    "    n_epochs: int,\n",
    "    writer: SummaryWriter,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    "    eval_every: int = 10,\n",
    "    checkpoint_path: Optional[str] = None,\n",
    "):\n",
    "    model.train()\n",
    "    step = 0\n",
    "    for epoch in trange(n_epochs, desc=\"Training\"):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(\n",
    "            dataloader, desc=f\"Epoch {epoch + 1}/{n_epochs}\", leave=False\n",
    "        ):\n",
    "            optimizer.zero_grad()\n",
    "            loss = calculate_loss(\n",
    "                model,\n",
    "                batch,\n",
    "                vocab_size,\n",
    "                device=device,\n",
    "                transition_dim=transition_dim,\n",
    "                observation_dim=observation_dim,\n",
    "                action_dim=action_dim,\n",
    "                reward_dim=reward_dim,\n",
    "                value_dim=value_dim,\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar(\"Loss/train\", loss.item(), step)\n",
    "            total_loss += loss.item()\n",
    "            step += 1\n",
    "\n",
    "        if epoch % eval_every == 0:\n",
    "            mean_rewards, std_rewards, mean_score, std_score, done_ratio = eval(\n",
    "                env,\n",
    "                model,\n",
    "                dataset.discretizer,\n",
    "                num_episodes=10 if not local else 1,\n",
    "                beam_width=32 if not local else 2,\n",
    "                beam_steps=20 if not local else 3,\n",
    "                beam_context=5 if not local else 7,\n",
    "                sample_expansion=2 if not local else 1,\n",
    "                observation_dim=observation_dim,\n",
    "                action_dim=action_dim,\n",
    "                reward_dim=reward_dim,\n",
    "                value_dim=value_dim,\n",
    "                transition_dim=transition_dim,\n",
    "                max_steps=400 if not local else 4,\n",
    "                plan_every=10 if not local else 2,\n",
    "                obs_top_k=5,\n",
    "                act_top_k=5,\n",
    "                rew_top_k=None,\n",
    "                temperature=1.0,\n",
    "                greedy=False,\n",
    "                device=device,\n",
    "            )\n",
    "            writer.add_scalar(\"Reward/mean\", mean_rewards, step)\n",
    "            writer.add_scalar(\"Reward/std\", std_rewards, step)\n",
    "            writer.add_scalar(\"Score/mean\", mean_score, step)\n",
    "            writer.add_scalar(\"Score/std\", std_score, step)\n",
    "            writer.add_scalar(\"Done ratio\", done_ratio, step)\n",
    "\n",
    "        if checkpoint_path:\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "writer = SummaryWriter()\n",
    "dataset.discretizer.to(device)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "model = TrajectoryTransformer(\n",
    "    seq_len, embedding_dim, n_heads, transition_dim, n_blocks, vocab_size\n",
    ").to(device)\n",
    "\n",
    "if load_checkpoint and os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading model from {checkpoint_path}\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "else:\n",
    "    print(\"Training model from scratch\")\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train(\n",
    "        model,\n",
    "        dataloader,\n",
    "        optimizer,\n",
    "        vocab_size,\n",
    "        n_epochs,\n",
    "        writer,\n",
    "        device=device,\n",
    "        eval_every=eval_every,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "    )\n",
    "    print(f\"Saved model to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8dbd503bd874c25ae6aaf7c4e8815f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rollout:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb08982f38c42bda0e91336f1ec706e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ea94a6e8024597a3234dc040407033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 0.05490743210334348\n",
      "Terminated: False\n"
     ]
    }
   ],
   "source": [
    "total_reward, trajectory, imgs, terminated = rollout(\n",
    "    model=model,\n",
    "    env=env,\n",
    "    discretizer=dataset.discretizer,\n",
    "    beam_width=32 if not local else 4,\n",
    "    beam_steps=5 if not local else 3,\n",
    "    beam_context=5 if not local else 7,\n",
    "    sample_expansion=2 if not local else 1,\n",
    "    observation_dim=observation_dim,\n",
    "    action_dim=action_dim,\n",
    "    reward_dim=reward_dim,\n",
    "    value_dim=value_dim,\n",
    "    transition_dim=transition_dim,\n",
    "    max_steps=400 if not local else 2,\n",
    "    plan_every=1,\n",
    "    obs_top_k=1,\n",
    "    act_top_k=2,\n",
    "    rew_top_k=None,\n",
    "    temperature=1.0,\n",
    "    greedy=False,\n",
    "    device=device,\n",
    "    generate_gif=True,\n",
    ")\n",
    "print(f\"Total reward: {total_reward}\")\n",
    "print(f\"Terminated: {terminated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/gif": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import imageio\n",
    "from IPython.display import Image\n",
    "\n",
    "# save the images as a gif\n",
    "imageio.mimsave(f\"{base_dir}/trajectory.gif\", imgs, fps=30)\n",
    "# display the gif and repeat it forever\n",
    "Image(filename=f\"{base_dir}/trajectory.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog-FROQ9Grm-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
