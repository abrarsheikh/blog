{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from typing import List\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple, Any\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from typing import Optional\n",
    "from tqdm.auto import tqdm, trange\n",
    "from minari import EpisodeData, MinariDataset\n",
    "import minari\n",
    "from gymnasium import Env\n",
    "import os\n",
    "import time\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation dim: 46, Action dim: 26\n",
      "Reward dim: 1, Value dim: 1\n",
      "Transition dim: 74\n",
      "Number of episodes: 100\n"
     ]
    }
   ],
   "source": [
    "def get_space_dim(space):\n",
    "    if isinstance(space, spaces.Discrete):\n",
    "        return 1\n",
    "    elif isinstance(space, spaces.Box):\n",
    "        return space.shape[0]\n",
    "    elif isinstance(space, spaces.Dict):\n",
    "        return sum([get_space_dim(v) for v in space.values()])\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported observation space\")\n",
    "\n",
    "\n",
    "dataset_ref = \"D4RL/hammer/expert-v2\"\n",
    "env_name = \"AdroitHandHammer-v1\"\n",
    "\n",
    "base_m_dataset = minari.load_dataset(dataset_ref, download=True)\n",
    "wrapped_env = base_m_dataset.recover_environment(\n",
    "    render_mode=\"rgb_array\"\n",
    ")\n",
    "env = wrapped_env.unwrapped\n",
    "env.name = env_name\n",
    "\n",
    "# Environment parameters\n",
    "observation_dim = get_space_dim(env.observation_space)\n",
    "action_dim = get_space_dim(env.action_space)\n",
    "reward_dim = 1\n",
    "value_dim = 1\n",
    "transition_dim = observation_dim + action_dim + reward_dim + value_dim\n",
    "\n",
    "print(f\"Observation dim: {observation_dim}, Action dim: {action_dim}\")\n",
    "print(f\"Reward dim: {reward_dim}, Value dim: {value_dim}\")\n",
    "print(f\"Transition dim: {transition_dim}\")\n",
    "\n",
    "local = not torch.cuda.is_available()\n",
    "\n",
    "# Model parameters\n",
    "n_transitions = 10\n",
    "seq_len = n_transitions * transition_dim\n",
    "vocab_size = 100\n",
    "max_bins = vocab_size\n",
    "discount_factor = 0.99\n",
    "embedding_dim = 32 if not local else 32\n",
    "n_heads = 4\n",
    "n_blocks = 4\n",
    "n_epochs = 25 if not local else 2\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "eval_every = 2 if not local else 5\n",
    "\n",
    "# other parameters\n",
    "n_episodes: Optional[int] = None if not local else 100\n",
    "# create a directory to save the model\n",
    "base_dir = f\"data/{dataset_ref}\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "checkpoint_path = f\"{base_dir}/\"\n",
    "load_checkpoint = (\n",
    "    True  # set to False if you want to train from scratch even if a checkpoint exists\n",
    ")\n",
    "if n_episodes:\n",
    "    m_dataset = base_m_dataset.sample_episodes(n_episodes)\n",
    "else:\n",
    "    m_dataset = base_m_dataset\n",
    "\n",
    "print(f\"Number of episodes: {len(m_dataset)}\")\n",
    "\n",
    "device = torch.device(\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KBinsDiscretizer:\n",
    "    \"\"\"\n",
    "    This class is responsible for encoding and decoding continuous values into discrete bins.\n",
    "    Number of bins are fixed for all the features.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset: np.ndarray, n_bins: int, strategy: str = \"ordinal\"):\n",
    "        self.n_bins = n_bins\n",
    "        self.strategy = strategy\n",
    "\n",
    "        # bin_edges shape: (n_features, n_bins + 1)\n",
    "        self.bin_edges = self._find_bin_edges(dataset)\n",
    "        # bin_centers shape: (n_features, n_bins)\n",
    "        self.bin_centers = (self.bin_edges[:, :-1] + self.bin_edges[:, 1:]) * 0.5\n",
    "        self.bin_centers_torch = torch.from_numpy(self.bin_centers).float()\n",
    "\n",
    "    def _find_bin_edges(self, dataset: np.ndarray):\n",
    "        # dataset shape: (n_samples, n_features)\n",
    "        bin_edges = []\n",
    "        if self.strategy == \"uniform\":\n",
    "            # min and max values for each feature, shpae: (n_features,)\n",
    "            mins, maxs = np.min(dataset, axis=0), np.max(dataset, axis=0)\n",
    "            # bin_edges shape: (n_features, n_bins + 1)\n",
    "            bin_edges = np.linspace(mins, maxs, self.n_bins + 1).T\n",
    "        elif self.strategy == \"quantile\":\n",
    "            quantiles = np.linspace(0, 100, self.n_bins + 1)\n",
    "            # bin_edges shape: (n_features, n_bins + 1)\n",
    "            bin_edges = np.percentile(dataset, quantiles, axis=0).T\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {self.strategy}\")\n",
    "        return bin_edges\n",
    "\n",
    "    def encode(\n",
    "        self, X: np.ndarray, subslice: Optional[Tuple[int, int]] = None\n",
    "    ) -> np.ndarray:\n",
    "        # use subslice to encode only a part of the features in the X\n",
    "        if X.ndim == 1:\n",
    "            # this is to handle the case where we have a single feature\n",
    "            X = X[None]\n",
    "        # data shape: (n_samples, n_features)\n",
    "        edges = self.bin_edges\n",
    "        if subslice is not None:\n",
    "            start, end = subslice\n",
    "            edges = edges[start:end]\n",
    "\n",
    "        # Xt represents discretized data, shape: (n_samples, n_features)\n",
    "        Xt = np.zeros(X.shape, dtype=np.long)\n",
    "\n",
    "        # See documentation of numpy.isclose for an explanation of ``rtol`` and ``atol``.\n",
    "        rtol = 1.0e-5\n",
    "        atol = 1.0e-8\n",
    "\n",
    "        for jj in range(X.shape[1]):\n",
    "            # Values which are close to a bin edge are susceptible to numeric\n",
    "            # instability. Add eps to X so these values are binned correctly\n",
    "            # with respect to their decimal truncation.\n",
    "            eps = atol + rtol * np.abs(X[:, jj])\n",
    "            # why [1:]? bins = edges - 1, but its unclear why we leave out the first element and not the last\n",
    "            Xt[:, jj] = np.digitize(X[:, jj] + eps, edges[jj][1:])\n",
    "\n",
    "        # clip the values to be within the range [0, n_bins - 1]\n",
    "        np.clip(Xt, 0, self.n_bins - 1, out=Xt)\n",
    "\n",
    "        return Xt\n",
    "\n",
    "    def decode(\n",
    "        self, Xt: np.ndarray, subslice: Optional[Tuple[int, int]] = None\n",
    "    ) -> np.ndarray:\n",
    "        # use subslice to decode only a part of the features in the Xt\n",
    "        if Xt.ndim == 1:\n",
    "            # this is to handle the case where we have a single feature\n",
    "            Xt = Xt[None]\n",
    "        # data shape: (n_samples, n_features)\n",
    "        centers = self.bin_centers\n",
    "        if subslice is not None:\n",
    "            start, end = subslice\n",
    "            centers = centers[start:end]\n",
    "\n",
    "        X = np.zeros(Xt.shape, dtype=np.float64)\n",
    "        for jj in range(Xt.shape[1]):\n",
    "            X[:, jj] = centers[jj, np.int_(Xt[:, jj])]\n",
    "\n",
    "        return X\n",
    "\n",
    "    def expectation(\n",
    "        self, probs: np.ndarray, subslice: Optional[Tuple[int, int]] = None\n",
    "    ) -> np.ndarray:\n",
    "        # given the probabilities of each bin, calculate the expectation of the feature values\n",
    "        # perticularly useful when we have a distribution over the bins, maybe from a model after softmax\n",
    "        # from logits.\n",
    "        # probs shape: (n_samples, n_features, n_bins)\n",
    "        if probs.ndim == 1:\n",
    "            # this is to handle the case where we have a single feature\n",
    "            probs = probs[None]\n",
    "        # probs shape: (batch_size, n_features, n_bins)\n",
    "        # bin_centers shape: (n_features, n_bins) -> (1, n_features, n_bins)\n",
    "        if torch.is_tensor(probs):\n",
    "            bin_centers = self.bin_centers_torch.unsqueeze(0)\n",
    "        else:\n",
    "            # bin_centers shape: (n_features, n_bins) -> (1, n_features, n_bins)\n",
    "            bin_centers = np.expand_dims(self.bin_centers, axis=0)\n",
    "\n",
    "        if subslice is not None:\n",
    "            start, end = subslice\n",
    "            bin_centers = bin_centers[:, start:end]\n",
    "\n",
    "        # use formula E[X] = sum(p(x) * x) for all x\n",
    "        # (batch_size, n_features, n_bins) * (1, n_features, n_bins) -> sum (batch_size, n_features, n_bins) -> (batch_size, n_features)\n",
    "        X = (probs * bin_centers).sum(axis=-1)\n",
    "        return X\n",
    "\n",
    "    def to(self, device):\n",
    "        self.bin_centers_torch = self.bin_centers_torch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Test array\n",
    "test_arr = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Initialize the discretizer\n",
    "discretizer = KBinsDiscretizer(test_arr, 1000, strategy=\"uniform\")\n",
    "\n",
    "# Encode and decode the test array\n",
    "encoded = discretizer.encode(test_arr)\n",
    "decoded = discretizer.decode(encoded)\n",
    "\n",
    "# Check if the decoded array is close to the original array\n",
    "assert np.isclose(\n",
    "    decoded, test_arr, atol=1e-2\n",
    ").all(), f\"Decoded array {decoded} is not close to the original array {test_arr}\"\n",
    "\n",
    "# Generate random probabilities\n",
    "probs = F.softmax(torch.from_numpy(np.random.rand(3, 2, 1000)), dim=-1).numpy()\n",
    "\n",
    "# Calculate the expectation\n",
    "expectation = discretizer.expectation(probs)\n",
    "\n",
    "# Check if the expectation is close to the mean of the test array\n",
    "expected_mean = np.tile(np.mean(test_arr, axis=0), (3, 1))\n",
    "assert np.isclose(\n",
    "    expectation, expected_mean, atol=1e-1\n",
    ").all(), f\"Expectation {expectation} is not close to the expected mean {expected_mean}\"\n",
    "\n",
    "print(\"All tests passed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed successfully.\n"
     ]
    }
   ],
   "source": [
    "def flatten_space(s_dict: Any, space: spaces.Space) -> np.ndarray:\n",
    "    if isinstance(space, spaces.Discrete):\n",
    "        return s_dict\n",
    "    elif isinstance(space, spaces.Box):\n",
    "        return s_dict\n",
    "    elif isinstance(space, spaces.Dict):\n",
    "        return np.concatenate([flatten_space(s_dict[k], space.spaces[k]) for k in space.spaces.keys()], axis=-1)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported observation space\")\n",
    "\n",
    "\n",
    "def unflatten_space(s_flat: np.ndarray, space: spaces.Space) -> dict:\n",
    "    if isinstance(space, spaces.Discrete):\n",
    "        return s_flat\n",
    "    elif isinstance(space, spaces.Box):\n",
    "        return s_flat\n",
    "    elif isinstance(space, spaces.Dict):\n",
    "        s_dict = {}\n",
    "        start = 0\n",
    "        for k, v in space.spaces.items():\n",
    "            end = start + get_space_dim(v)\n",
    "            s_dict[k] = unflatten_space(s_flat[:, start:end], v)\n",
    "            start = end\n",
    "        return s_dict\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported observation space\")\n",
    "\n",
    "\n",
    "# Test the flatten_space_dict and unflatten_space_dict functions\n",
    "test_dict = {\"obs\": np.array([[1, 2, 3], [4, 5, 6]]), \"act\": np.array([[0], [1]])}\n",
    "test_space = spaces.Dict(\n",
    "    {\"obs\": spaces.Box(low=0, high=10, shape=(3,)), \"act\": spaces.Discrete(2)}\n",
    ")\n",
    "test_flat = flatten_space(test_dict, test_space)\n",
    "test_unflat = unflatten_space(test_flat, test_space)\n",
    "\n",
    "assert np.isclose(\n",
    "    test_flat, np.array([[0, 1, 2, 3], [1, 4, 5, 6]])\n",
    ").all(), f\"Flattened array {test_flat} is not as expected.\"\n",
    "assert np.isclose(\n",
    "    test_unflat[\"obs\"], test_dict[\"obs\"]\n",
    ").all(), f\"Unflattened observation {test_unflat['obs']} is not as expected.\"\n",
    "assert np.isclose(\n",
    "    test_unflat[\"act\"], test_dict[\"act\"]\n",
    ").all(), f\"Unflattened action {test_unflat['act']} is not as expected.\"\n",
    "\n",
    "# test discrete space\n",
    "test_dict = np.array([[0], [1]])\n",
    "test_space = spaces.Discrete(2)\n",
    "test_flat = flatten_space(test_dict, test_space)\n",
    "test_unflat = unflatten_space(test_flat, test_space)\n",
    "\n",
    "assert np.isclose(\n",
    "    test_flat, test_dict\n",
    ").all(), f\"Flattened array {test_flat} is not as expected.\"\n",
    "assert np.isclose(\n",
    "    test_unflat, test_dict\n",
    ").all(), f\"Unflattened array {test_unflat} is not as expected.\"\n",
    "\n",
    "# test box space\n",
    "test_dict = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "test_space = spaces.Box(low=0, high=10, shape=(3,))\n",
    "test_flat = flatten_space(test_dict, test_space)\n",
    "test_unflat = unflatten_space(test_flat, test_space)\n",
    "\n",
    "assert np.isclose(\n",
    "    test_flat, test_dict\n",
    ").all(), f\"Flattened array {test_flat} is not as expected.\"\n",
    "assert np.isclose(\n",
    "    test_unflat, test_dict\n",
    ").all(), f\"Unflattened array {test_unflat} is not as expected.\"\n",
    "\n",
    "print(\"All tests passed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_trajectory(env: Env, episode: EpisodeData, discount: float = 0.99):\n",
    "    # Convert the object of type EpisodeData to a numpy array. EpisodeData\n",
    "    # contains the following fields: observations, actions, rewards, other\n",
    "    # and each of these fields is a numpy array. We need to concatenate\n",
    "    # these arrays along the last axis to get a single array for each time.\n",
    "\n",
    "    trajectory_len = episode.rewards.shape[0]\n",
    "    # shape (trajectory_len, observation_dim)\n",
    "    observations = episode.observations\n",
    "    # shape (trajectory_len, action_dim)\n",
    "    actions = episode.actions\n",
    "    # shape (trajectory_len, action_dim)\n",
    "    rewards = episode.rewards\n",
    "\n",
    "    # use values to store the rewards to go\n",
    "    # for a given time step, the value is the sum of rewards from that time step\n",
    "    # to the end of the trajectory, discounted by discount factor at each time step\n",
    "    values = np.zeros_like(rewards, dtype=np.float32)\n",
    "    # calculate discounts for each time step\n",
    "    discounts = discount ** np.arange(trajectory_len)\n",
    "    # calculate rewards to go with discount\n",
    "    for t in range(trajectory_len):\n",
    "        values[t] = (rewards[t+1:].T * discounts[: -t - 1]).sum()\n",
    "\n",
    "    # drop the last state because we don't have a reward for it\n",
    "    states = flatten_space(observations, env.observation_space)\n",
    "    states = states[:-1, :].reshape(trajectory_len, -1)\n",
    "    actions = flatten_space(actions, env.action_space)\n",
    "    actions = actions.reshape(trajectory_len, -1)\n",
    "    rewards = rewards.reshape(trajectory_len, -1)\n",
    "    values = values.reshape(trajectory_len, -1)\n",
    "\n",
    "    # shape (trajectory_len, observation_dim + action_dim + reward_dim + value_dim)\n",
    "    joined = np.concatenate([states, actions, rewards, values], axis=-1)\n",
    "\n",
    "    return joined\n",
    "\n",
    "\n",
    "class DiscretizeDataset(Dataset):\n",
    "    # Each input into the sequence model needs to be (batch_size, tokens)\n",
    "    # output should be in groups of transitions\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: Env,\n",
    "        m_dataset: MinariDataset,\n",
    "        n_transitions: int,\n",
    "        discount: float = 0.99,\n",
    "        max_bins: int = 1000,\n",
    "    ):\n",
    "        self.m_dataset = m_dataset\n",
    "        self.n_transitions = n_transitions\n",
    "\n",
    "        # this list will contain the joined trajectories, each item in the list\n",
    "        # is a trajectory of shape (trajectory_len, observation_dim + action_dim + reward_dim + value_dim)\n",
    "        # and that trajectory is one episodedata from the m_dataset\n",
    "        self.joined_trajectories = []\n",
    "        for episode in m_dataset:\n",
    "            self.joined_trajectories.append(join_trajectory(env, episode, discount))\n",
    "\n",
    "        self.discretizer = KBinsDiscretizer(\n",
    "            n_bins=max_bins,\n",
    "            strategy=\"quantile\",\n",
    "            # concatenate all the trajectories\n",
    "            # shape (n_samples * trajectory_len, observation_dim + action_dim + reward_dim + value_dim)\n",
    "            dataset=np.concatenate(self.joined_trajectories, axis=0),\n",
    "        )\n",
    "\n",
    "        # we need a dataset for training sequence model\n",
    "        # given that we need a sequence of n_transitions, we need to generate\n",
    "        # indices such that we can get n_transitions from each trajectory\n",
    "        indices = []\n",
    "        for traj_idx, joined_trajectory in enumerate(self.joined_trajectories):\n",
    "            traj_len = joined_trajectory.shape[0]\n",
    "            end = traj_len - 1\n",
    "            for i in range(end):\n",
    "                indices.append((traj_idx, i, i + n_transitions))\n",
    "\n",
    "        self.indices = np.array(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        traj_idx, start, end = self.indices[idx]\n",
    "        # sample a sequence of n_transitions from trajectory at traj_idx\n",
    "        joined = self.joined_trajectories[traj_idx][start:end]\n",
    "        loss_pad_mask = np.ones(\n",
    "            (self.n_transitions, joined.shape[-1]), dtype=np.float32\n",
    "        )\n",
    "        # some sequences may be shorter than n_transitions, pad them with zeros\n",
    "        # and set the mask to zero for the padded part, this mask will be used\n",
    "        # to mask the loss when calculating the loss\n",
    "        if joined.shape[0] < self.n_transitions:\n",
    "            # pad along dimension 0, zero padding at the beginning\n",
    "            # and (self.n_transitions - joined.shape[0]) padding at the end\n",
    "            joined = np.pad(\n",
    "                joined,\n",
    "                ((0, self.n_transitions - joined.shape[0]), (0, 0)),\n",
    "                mode=\"constant\",\n",
    "                constant_values=0,\n",
    "            )\n",
    "            loss_pad_mask[joined.shape[0] :] = 0\n",
    "\n",
    "        # since transformer model expects discrete values, we need to encode the\n",
    "        # continuous values into discrete bins\n",
    "        # shape (n_transitions, transition_dim) -> (n_transitions, transition_dim)\n",
    "        joined_discretized = self.discretizer.encode(joined)\n",
    "        # shape (n_transitions, transition_dim) -> (n_transitions * transition_dim)\n",
    "        # i'e [s1, a1, r1, v1, s2, a2, r2, v2, ...]\n",
    "        joined_discretized = joined_discretized.reshape(-1).astype(np.long)\n",
    "        loss_pad_mask = loss_pad_mask.reshape(-1)\n",
    "        # return input, target, and mask\n",
    "        # since sequence model predicts the next token, target is the next token in the sequence\n",
    "        return joined_discretized[:-1], joined_discretized[1:], loss_pad_mask[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 19900\n",
      "Shape of dataset: (739,)\n"
     ]
    }
   ],
   "source": [
    "dataset = DiscretizeDataset(\n",
    "    env=env,\n",
    "    m_dataset=m_dataset,\n",
    "    n_transitions=n_transitions,\n",
    "    discount=discount_factor,\n",
    "    max_bins=max_bins,\n",
    ")\n",
    "\n",
    "print(f\"Length of dataset: {len(dataset)}\")\n",
    "print(f\"Shape of dataset: {dataset[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    # Transformer block\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_len,\n",
    "        embedding_dim: int,\n",
    "        n_heads: int,\n",
    "        attention_dropout: float,\n",
    "        residual_dropout: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embedding_dim, n_heads, batch_first=True, dropout=attention_dropout\n",
    "        )\n",
    "        self.attn_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.fc_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.drop = nn.Dropout(residual_dropout)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embedding_dim * 2, embedding_dim),\n",
    "            nn.Dropout(residual_dropout),\n",
    "        )\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # mask value of true means that the value is not allowed to be attended to\n",
    "        self.register_buffer(\"mask\", ~torch.tril(torch.ones(seq_len, seq_len)).bool())\n",
    "        # transition_dim - 1 stores rewards to go, we don't want to attend to them because they contain future information\n",
    "        self.mask[:, transition_dim - 1 :: transition_dim] = True\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, kv_cache: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        # x shape (batch_size, n_tokens, embedding_dim) in prefill mode else (batch_size, 1, embedding_dim)\n",
    "        # kv_cache shape (batch_size, n_tokens, embedding_dim) in inference mode else None\n",
    "        _, n_tokens, _ = x.shape\n",
    "\n",
    "        # normalize the input before passing it to the attention layer\n",
    "        x_norm = self.attn_norm(x)\n",
    "\n",
    "        if kv_cache is None:\n",
    "            # when kv_cache is None, we are in prefill mode\n",
    "\n",
    "            # attn_mask shape (seq_len, seq_len), but incoming shape is (batch_size, n_tokens, embedding_dim)\n",
    "            # so filter the mask to the correct size (n_tokens, n_tokens)\n",
    "            attn_mask = self.mask[:n_tokens, :n_tokens]\n",
    "            q, k, v = x_norm, x_norm, x_norm\n",
    "        else:\n",
    "            assert n_tokens == 1, \"kv_cache can only be None with a single token\"\n",
    "            # +1 because we are adding a new token\n",
    "            assert kv_cache.shape[1] + 1 <= self.seq_len, \"kv_cache is too large\"\n",
    "\n",
    "            # attn_mask is None because we are running in inference mode, processing one token at a time\n",
    "            # and this token is not allowed to attend to future tokens\n",
    "            attn_mask = None\n",
    "            q, k, v = (\n",
    "                x_norm,\n",
    "                # shape (batch_size, n_tokens + 1, embedding_dim)\n",
    "                torch.cat([kv_cache, x_norm], dim=1),\n",
    "                torch.cat([kv_cache, x_norm], dim=1),\n",
    "            )\n",
    "\n",
    "        new_kv_cache = k\n",
    "\n",
    "        # x shape (batch_size, n_tokens, embedding_dim) in prefill mode else (batch_size, 1, embedding_dim)\n",
    "        x = x + self.drop(\n",
    "            self.attn(q, k, v, attn_mask=attn_mask, need_weights=False)[0]\n",
    "        )\n",
    "\n",
    "        x = x + self.mlp(self.fc_norm(x))\n",
    "\n",
    "        return x, new_kv_cache\n",
    "\n",
    "\n",
    "class TrajectoryTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_len: int,\n",
    "        embedding_dim: int,\n",
    "        n_heads: int,\n",
    "        transition_dim: int,\n",
    "        n_blocks: int,\n",
    "        vocab_size: int,\n",
    "        dropout_embedding: float = 0.1,\n",
    "        attention_dropout: float = 0.1,\n",
    "        residual_dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.transition_dim = transition_dim\n",
    "        self.n_blocks = n_blocks\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # our input contains transition_dim types of tokens and each token is from a vocab of size vocab_size\n",
    "        # so the total number of tokens is transition_dim * vocab_size\n",
    "        self.token_embedding = nn.Embedding(seq_len * transition_dim, self.embedding_dim)\n",
    "        # learnable positional embedding\n",
    "        self.positional_embedding = nn.Parameter(\n",
    "            torch.zeros(1, seq_len, self.embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout_embedding = nn.Dropout(dropout_embedding)\n",
    "\n",
    "        # create n_blocks of transformer blocks\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    self.seq_len,\n",
    "                    self.embedding_dim,\n",
    "                    self.n_heads,\n",
    "                    attention_dropout,\n",
    "                    residual_dropout,\n",
    "                )\n",
    "                for _ in range(self.n_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # project the output of the transformer to the vocab size\n",
    "        # since each token type is from a vocab of size vocab_size\n",
    "        # we can do this. But for instance if every token type used different\n",
    "        # number of bins, then we would have handled this differently. But dont worry\n",
    "        # that is not the case here.\n",
    "        self.fc = nn.Linear(self.embedding_dim, vocab_size)\n",
    "\n",
    "    def get_seq_len(self):\n",
    "        return self.seq_len\n",
    "\n",
    "    def _offset_tokens(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        # for beginners, this function may be a bit confusing. So let me explain\n",
    "\n",
    "        # our input consists of transition_dim types of tokens\n",
    "        # and each token is from a vocab of size vocab_size. So total\n",
    "        # there are transition_dim * vocab_size unique tokens. In contrast\n",
    "        # to NLP where we have just one token type(the word) and each word\n",
    "        # is from a vocab of size vocab_size (50k in llama).\n",
    "        # So to bridge this gap, we need to project each token's local vocab\n",
    "        # into the global vocab space. And the way we do this is by offsetting\n",
    "        # each token type by a factor of vocab_size.\n",
    "        # eg. if we have 3 token types and vocab_size is 10, then the tokens\n",
    "        # will be offset by [0, 10, 20] respectively.\n",
    "        # given input [2, 6, 3, 1, 2, 5] and vocab_size 10,\n",
    "        # the output will be [2, 16, 23, 11, 12, 25]\n",
    "\n",
    "        _, n_tokens = tokens.shape\n",
    "        # calculate the number of transitions in the input\n",
    "        n_transition = np.ceil(n_tokens / self.transition_dim).astype(int)\n",
    "\n",
    "        # if transition_dim is 4, and vocab_size is 10, then the offsets will be\n",
    "        # [0, 10, 20, 30]\n",
    "        # shape (transition_dim,)\n",
    "        offsets = (\n",
    "            torch.arange(self.transition_dim, device=tokens.device) * self.vocab_size\n",
    "        )\n",
    "        # repeat the offset n_transition times\n",
    "        # shape (n_transition * transition_dim,)\n",
    "        offsets = offsets.repeat(n_transition)\n",
    "        # add the offsets to the tokens, and truncate the tokens to n_tokens\n",
    "        offset_idx = offsets[:n_tokens] + tokens\n",
    "        return offset_idx\n",
    "\n",
    "    def forward(\n",
    "        self, tokens: torch.Tensor, kv_caches: Optional[List] = None\n",
    "    ) -> torch.Tensor:\n",
    "        # tokens shape (batch_size, n_tokens) in prefill mode else (batch_size, 1)\n",
    "        _, n_tokens = tokens.shape\n",
    "        assert (\n",
    "            n_tokens <= self.seq_len\n",
    "        ), f\"n_tokens {n_tokens} is greater than seq_len {self.seq_len}\"\n",
    "\n",
    "        if kv_caches is not None:\n",
    "            assert n_tokens == 1, \"kv_caches can only be used with a single token\"\n",
    "\n",
    "        # project each token into their vocab space, this is similar to tokenization\n",
    "        # in NLP where we project each word into their vocab space\n",
    "        # (batch_size, n_tokens)\n",
    "        offset_idx = self._offset_tokens(tokens)\n",
    "\n",
    "        # (batch_size, n_tokens) -> (batch_size, n_tokens, embedding_dim)\n",
    "        tokens = self.token_embedding(offset_idx)\n",
    "\n",
    "        if kv_caches is not None:\n",
    "            # in inference mode\n",
    "            idx = kv_caches[0].shape[1]\n",
    "            # (1, 1, embedding_dim)\n",
    "            positional_embedding = self.positional_embedding[:, idx : idx + 1]\n",
    "        else:\n",
    "            # in prefill mode\n",
    "            # initialize kv_caches to None\n",
    "            kv_caches = [None for _ in range(self.n_blocks)]\n",
    "            # (1, n_tokens, embedding_dim)\n",
    "            positional_embedding = self.positional_embedding[:, :n_tokens]\n",
    "        tokens += positional_embedding\n",
    "\n",
    "        # (batch_size, n_tokens, embedding_dim) -> (batch_size, n_tokens, embedding_dim)\n",
    "        tokens = self.dropout_embedding(tokens)\n",
    "\n",
    "        new_kv_caches = []\n",
    "        for block, kv_cache in zip(self.blocks, kv_caches):\n",
    "            tokens, new_kv_cache = block(tokens, kv_cache)\n",
    "            new_kv_caches.append(new_kv_cache)\n",
    "        # (batch_size, n_tokens, embedding_dim) -> (batch_size, n_tokens, vocab_size)\n",
    "        logits = self.fc(tokens)\n",
    "        return logits, new_kv_caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_token_from_logits(\n",
    "    logits: torch.Tensor,\n",
    "    temperature: float = 1.0,\n",
    "    greedy: bool = False,\n",
    "    top_k: Optional[int] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This function return exactly one token from the logits.\n",
    "    We have options to sample from the logits using\n",
    "    1. Greedy sampling\n",
    "    2. Top-k sampling\n",
    "    3. Temperature scaling\n",
    "\n",
    "    \"\"\"\n",
    "    # logits shape (batch_size, vocab_size) representing the logits of the next token\n",
    "\n",
    "    # Apply temperature scaling, the higher the temperature, the more uniform the distribution\n",
    "    # the lower the temperature, the more peaked the distribution\n",
    "    if temperature != 1.0:\n",
    "        logits = logits / temperature\n",
    "\n",
    "    if top_k is not None:\n",
    "        # Apply top-k sampling\n",
    "        # (batch_size, vocab_size) -> (batch_size, top_k)\n",
    "        v, indices = torch.topk(logits, top_k, dim=-1)\n",
    "\n",
    "        # Next instruction is a bit tricky, but it simply selects the top-k tokens\n",
    "        # set all logits to -inf except the top-k indices\n",
    "        # v[:, [-1]] might be a bit confusing, but it simply selects the last element\n",
    "        # along dim=1, and the result is a tensor of shape (batch_size, 1)\n",
    "        logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "        # Calculate the probabilities from the logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # Sample from the top-k indices\n",
    "        # (batch_size, top_k) -> (batch_size, 1)\n",
    "        idx = torch.multinomial(probs, num_samples=1)\n",
    "    else:\n",
    "        # Calculate the probabilities from the logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        if not greedy:\n",
    "            idx = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            # Greedy sampling\n",
    "            _, idx = torch.max(probs, dim=-1)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def round_to_multiple(number, multiple):\n",
    "    \"\"\"\n",
    "    Rounds a given number up to the nearest multiple of a specified value.\n",
    "\n",
    "    Args:\n",
    "        number (int or float): The number to be rounded.\n",
    "        multiple (int or float): The multiple to which the number should be rounded.\n",
    "\n",
    "    Returns:\n",
    "        int or float: The number rounded up to the nearest multiple of the specified value.\n",
    "    \"\"\"\n",
    "    pad = (multiple - number % multiple) % multiple\n",
    "    return number + pad\n",
    "\n",
    "\n",
    "# Test the round_to_multiple function\n",
    "assert round_to_multiple(5, 3) == 6\n",
    "assert round_to_multiple(6, 3) == 6\n",
    "assert round_to_multiple(7, 3) == 9\n",
    "\n",
    "\n",
    "def sample_tokens(\n",
    "    model: nn.Module,\n",
    "    context: nn.Module,\n",
    "    kv_caches: Optional[List],\n",
    "    n_steps: int,\n",
    "    temperature: float = 1.0,\n",
    "    greedy: bool = False,\n",
    "    top_k: Optional[int] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Sample a sequence of tokens from the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to sample from.\n",
    "        context (nn.Module): The context to condition the sampling on.\n",
    "            shape (batch_size, n_tokens).\n",
    "        n_steps (int): The number of steps to sample.\n",
    "        temperature (float): The temperature scaling factor.\n",
    "        greedy (bool): Whether to sample greedily.\n",
    "        top_k (Optional[int]): The top-k sampling parameter.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The sampled tokens.\n",
    "    \"\"\"\n",
    "    # tensor to store the logits of the next   sampled tokens\n",
    "    raw_logits = torch.zeros(\n",
    "        context.shape[0], n_steps, vocab_size, device=context.device\n",
    "    )\n",
    "    if kv_caches is None:\n",
    "        # when kv_caches is None, we are in prefilling step\n",
    "        logits, kv_caches = model(context, kv_caches)\n",
    "        # Sample the next token\n",
    "        # (batch_size, 1)\n",
    "        token = sample_token_from_logits(\n",
    "            logits[:, -1], temperature=temperature, greedy=greedy, top_k=top_k\n",
    "        )\n",
    "\n",
    "        context = torch.cat([context, token], dim=-1)\n",
    "\n",
    "        raw_logits[:, 0] = logits[:, -1]\n",
    "        # since we already did one step, we need to sample n_steps - 1\n",
    "        steps = range(1, n_steps)\n",
    "    else:\n",
    "        steps = range(n_steps)\n",
    "\n",
    "    for i in steps:\n",
    "        # crop the context so that it doesn't exceed the seq_len\n",
    "        curr_context_len = context.shape[1]\n",
    "        n_crop = round_to_multiple(\n",
    "            max(0, curr_context_len - model.get_seq_len()), transition_dim\n",
    "        )\n",
    "        if n_crop > 0:\n",
    "            # since we are cropping from the left, we need to update the kv_caches\n",
    "            kv_caches = [kv[:, n_crop:] for kv in kv_caches]\n",
    "        # Get the model's prediction\n",
    "        # (batch_size, 1) -> (batch_size, 1, vocab_size)\n",
    "        logits, kv_caches = model(context[:, -1:], kv_caches)\n",
    "        # Sample the next token\n",
    "        # (batch_size, 1)\n",
    "        token = sample_token_from_logits(\n",
    "            logits[:, -1], temperature=temperature, greedy=greedy, top_k=top_k\n",
    "        )\n",
    "\n",
    "        context = torch.cat([context, token], dim=-1)\n",
    "\n",
    "        raw_logits[:, i] = logits[:, -1]\n",
    "    return context, kv_caches, raw_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions are probably the most important functions in this notebook and\n",
    "# also the most complex.\n",
    "\n",
    "def vec_beam_plan(\n",
    "    model: nn.Module,\n",
    "    discretizer: KBinsDiscretizer,\n",
    "    context: torch.Tensor,\n",
    "    beam_width: int,\n",
    "    beam_steps: int,\n",
    "    beam_context: int,\n",
    "    sample_expansion: int,\n",
    "    observation_dim: int,\n",
    "    action_dim: int,\n",
    "    reward_dim: int,\n",
    "    value_dim: int,\n",
    "    transition_dim: int,\n",
    "    obs_top_k: Optional[int] = None,\n",
    "    act_top_k: Optional[int] = None,\n",
    "    rew_top_k: Optional[int] = None,\n",
    "    temperature: float = 1.0,\n",
    "    greedy: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    In the most simplest terms, this function is responsible for planning a sequence of actions\n",
    "    that maximizes the expected rewards conditioned on the context.\n",
    "\n",
    "    It uses beam search to explore the space of possible plans. Beam search is a heuristic search\n",
    "    algorithm that explores a graph by expanding the most promising nodes in a limited set called the beam.\n",
    "\n",
    "    The concept of beam search is simple, but the implementation can be a bit tricky mainly because\n",
    "    we are processing multiple sequences in parallel. This is where the complexity comes from.\n",
    "    \"\"\"\n",
    "    batch_size = context.shape[0]\n",
    "    tokens_context_size = beam_context * transition_dim\n",
    "    n_crop = round_to_multiple(\n",
    "        max(0, context.shape[1] - tokens_context_size), transition_dim\n",
    "    )\n",
    "    context = context[:, n_crop:]\n",
    "    # context shape (batch_size, seq_len) -> (beam_width, beam_width, seq_len)\n",
    "    plan = context.unsqueeze(1).repeat(1, beam_width, 1)\n",
    "\n",
    "    # tensor to store the rewards obtained from environment\n",
    "    # the +1 is non-intuitive, but it is because we need to store the value at t+1\n",
    "    # you will see this later.\n",
    "    rewards = torch.zeros(batch_size, beam_width, beam_steps + 1, device=context.device)\n",
    "    discounts = discount_factor ** torch.arange(beam_steps + 1, device=context.device)\n",
    "\n",
    "    # because beam plan start with a fresh context, we need to prefill the model\n",
    "    # first with the context, hence kv_caches is None\n",
    "    kv_caches = None\n",
    "    for t in trange(beam_steps, desc=\"Beam Search\", leave=False):\n",
    "        # sample_expansion is not strictly necessary, but it is used to increase the number of samples\n",
    "        #   which should allow us to explore more diverse plans. The reason this works is because the way\n",
    "        #   we sample tokens is stochastic, so by sampling more tokens, we are able to explore more diverse plans.\n",
    "        # (batch_size, beam_width, n_tokens) -> (batch_size, beam_width * sample_expansion, n_tokens)\n",
    "        #   -> (batch_size * beam_width * sample_expansion, n_tokens)\n",
    "        plan = plan.repeat(1, sample_expansion, 1).flatten(0, 1)\n",
    "        # (batch_size, beam_width, beam_steps + 1) -> (batch_size * beam_width * sample_expansion, beam_steps + 1)\n",
    "        rewards = rewards.repeat(1, sample_expansion, 1).flatten(0, 1)\n",
    "\n",
    "        if kv_caches is not None:\n",
    "            # When we are in inference mode, we need to expand the kv_caches\n",
    "            # (batch_size * beam_width, n_tokens, embedding_dim) -> (batch_size * beam_width * sample_expansion, n_tokens, embedding_dim)\n",
    "            new_kv_caches = []\n",
    "            for kv in kv_caches:\n",
    "                _, n_tokens, embedding_dim = kv.shape\n",
    "                new_kv_cache = (\n",
    "                    kv.view(batch_size, beam_width, n_tokens, embedding_dim)\n",
    "                    .repeat(1, sample_expansion, 1, 1)\n",
    "                    .flatten(0, 1)\n",
    "                )\n",
    "                new_kv_caches.append(new_kv_cache)\n",
    "            kv_caches = new_kv_caches\n",
    "\n",
    "        # sample actions\n",
    "        # plan (batch_size * beam_width * sample_expansion, n_tokens) -> (batch_size * beam_width * sample_expansion, n_tokens + action_dim)\n",
    "        # kv_caches is updated with the new action tokens\n",
    "        plan, kv_caches, _ = sample_tokens(\n",
    "            model,\n",
    "            plan,\n",
    "            kv_caches,\n",
    "            n_steps=action_dim,\n",
    "            top_k=act_top_k,\n",
    "            temperature=temperature,\n",
    "            greedy=greedy,\n",
    "        )\n",
    "\n",
    "        # sample rewards and values\n",
    "        # plan (batch_size * beam_width * sample_expansion, n_tokens) -> (batch_size * beam_width * sample_expansion, n_tokens + reward_dim + value_dim)\n",
    "        # kv_caches is updated with the new reward and value tokens\n",
    "        # logits shape (batch_size * beam_width * sample_expansion, reward_dim + value_dim, vocab_size)\n",
    "        plan, kv_caches, logits = sample_tokens(\n",
    "            model,\n",
    "            plan,\n",
    "            kv_caches,\n",
    "            n_steps=reward_dim + value_dim,\n",
    "            top_k=rew_top_k,\n",
    "            temperature=temperature,\n",
    "            greedy=greedy,\n",
    "        )\n",
    "\n",
    "        # calculate probabilities from logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # calculate the expected rewards and values\n",
    "        # (batch_size * beam_width * sample_expansion, reward_dim + value_dim, vocab_size)\n",
    "        #   -> (batch_size * beam_width * sample_expansion, reward_dim + value_dim)\n",
    "        rewards_and_values = discretizer.expectation(\n",
    "            probs, subslice=(transition_dim - reward_dim - value_dim, transition_dim)\n",
    "        )\n",
    "\n",
    "        rewards[..., t : t + reward_dim + value_dim] = rewards_and_values\n",
    "        # Did you notice that rewards contains rewards at t and values at t+1, why?\n",
    "        #   This is only a trick to make it easier to calculate the value at t. In the next step, the value at t+1\n",
    "        #   will be overwritten by the actual reward at t+1.\n",
    "\n",
    "        # Let's talk about how we calculate the value, the value here represents the rewards to go starting beginning of beam plan.\n",
    "        #   when we want to calculate value (rewards to go) at t, we need to consider discounted rewards from 0 to t\n",
    "        #   and also future discounted rewards from t+1 to end.\n",
    "        # (batch_size * beam_width * sample_expansion, beam_steps + 1) * (beam_steps + 1) -> (batch_size * beam_width * sample_expansion)\n",
    "        # the reason we care of values is that it helps us to select the best plans\n",
    "        values = (rewards * discounts).sum(dim=-1)\n",
    "\n",
    "        # select the top-k values\n",
    "        values, idx = torch.topk(values.view(batch_size, -1), k=beam_width, dim=-1)\n",
    "        # (batch_size, beam_width) -> (batch_size, beam_width, 1)\n",
    "        idx = idx.unsqueeze(-1)\n",
    "\n",
    "        # shape (batch_size * beam_width * sample_expansion, beam_steps + 1) -> (batch_size, beam_width * sample_expansion, beam_steps + 1)\n",
    "        rewards = rewards.view(batch_size, beam_width * sample_expansion, -1)\n",
    "\n",
    "        # the gather operation is a bit tricky, but it is used to select the rewards corresponding to the top-k values\n",
    "        # for every batch, select the rewards corresponding to the top-k values\n",
    "        # since idx contains the indices along the beam_width * sample_expansion dimension\n",
    "        # we need to repeat the idx along the last dimension to match the rewards shape,\n",
    "        # and then use it to select the rewards\n",
    "        # (batch_size, beam_width * sample_expansion, beam_steps + 1) -> (batch_size, beam_width, beam_steps + 1)\n",
    "        rewards = torch.gather(rewards, 1, idx.repeat(1, 1, beam_steps + 1))\n",
    "\n",
    "        # select the top-k plans\n",
    "        # shape (batch_size * beam_width * sample_expansion, n_tokens) -> (batch_size, beam_width * sample_expansion, n_tokens)\n",
    "        plan = plan.view(batch_size, beam_width * sample_expansion, -1)\n",
    "        # shape (batch_size, beam_width * sample_expansion, n_tokens) -> (batch_size, beam_width, n_tokens)\n",
    "        plan = torch.gather(plan, 1, idx.repeat(1, 1, plan.shape[-1]))\n",
    "\n",
    "        # select the top-k kv_caches\n",
    "        best_kv_caches = []\n",
    "        for kv in kv_caches:\n",
    "            _, n_tokens, embedding_dim = kv.shape\n",
    "            kv = kv.view(\n",
    "                batch_size, beam_width * sample_expansion, n_tokens, embedding_dim\n",
    "            )\n",
    "            # same idea as above, repeat idx along the last 2 dimensions\n",
    "            # kv shape (batch_size, beam_width * sample_expansion, n_tokens, embedding_dim) -> (batch_size, beam_width, n_tokens, embedding_dim)\n",
    "            kv = torch.gather(\n",
    "                kv, 1, idx.unsqueeze(-1).repeat(1, 1, n_tokens, embedding_dim)\n",
    "            )\n",
    "            best_kv_caches.append(kv.flatten(0, 1))\n",
    "\n",
    "        if t < beam_steps - 1:\n",
    "            # sample observations only if we are not at the last step, why?\n",
    "            # because beam plan has to end with a valid transition [...., obs, act, rew, val]\n",
    "\n",
    "            # plan (batch_size, beam_width, n_tokens) -> (batch_size * beam_width, n_tokens)\n",
    "            plan = plan.view(batch_size * beam_width, -1)\n",
    "\n",
    "            # sample observations\n",
    "            # plan (batch_size * beam_width, n_tokens) -> (batch_size * beam_width, n_tokens + observation_dim)\n",
    "            plan, kv_caches, _ = sample_tokens(\n",
    "                model,\n",
    "                plan,\n",
    "                best_kv_caches,\n",
    "                n_steps=observation_dim,\n",
    "                top_k=obs_top_k,\n",
    "                temperature=temperature,\n",
    "                greedy=greedy,\n",
    "            )\n",
    "            # plan (batch_size * beam_width, n_tokens + observation_dim) -> (batch_size, beam_width, n_tokens + observation_dim)\n",
    "            plan = plan.view(batch_size, beam_width, -1)\n",
    "\n",
    "    # (batch_size, beam_width) -> (batch_size)\n",
    "    # for each batch, select the plan with the highest value and return it's index\n",
    "    argmax = torch.argmax(values, dim=-1)\n",
    "\n",
    "    # select the best plan\n",
    "    # (batch_size, beam_width, n_tokens) -> (batch_size, n_tokens)\n",
    "    best_plan = plan[torch.arange(batch_size), argmax]\n",
    "    # filter out the context tokens and return the best plan as obtained from the beam search\n",
    "    best_plan = best_plan[:, context.shape[1] :]\n",
    "    return best_plan\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def vec_rollout(\n",
    "    model: nn.Module,\n",
    "    env: DummyVecEnv,\n",
    "    discretizer: KBinsDiscretizer,\n",
    "    beam_width: int,\n",
    "    beam_steps: int,\n",
    "    beam_context: int,\n",
    "    sample_expansion: int,\n",
    "    observation_dim: int,\n",
    "    action_dim: int,\n",
    "    reward_dim: int,\n",
    "    value_dim: int,\n",
    "    transition_dim: int,\n",
    "    max_steps: int,\n",
    "    plan_every: int,\n",
    "    obs_top_k: Optional[int] = None,\n",
    "    act_top_k: Optional[int] = None,\n",
    "    rew_top_k: Optional[int] = None,\n",
    "    temperature: float = 1.0,\n",
    "    greedy: bool = False,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    "):\n",
    "    \"\"\"\n",
    "    What is a rollout? A rollout is a simulation of an agent interacting with the environment\n",
    "    by following a plan. The plan is generated by the model using beam search. The model\n",
    "    predicts the next action, reward, and observation conditioned on the context. The context\n",
    "    is the history of the agent's interaction with the environment.\n",
    "\n",
    "    This function is responsible for performing a rollout using the model and the environment\n",
    "    and returning the total rewards obtained by the agent.\n",
    "\n",
    "    Similar to the vec_beam_plan function, this function is a bit complex because it processes\n",
    "    multiple sequences in parallel. The complexity comes from the fact that we are using a vectorized\n",
    "    environment, which means that we are processing multiple environments in parallel.\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        plan_every <= beam_steps\n",
    "    ), f\"plan_every {plan_every} should be less than or equal to beam_steps {beam_steps}\"\n",
    "\n",
    "    # reset the environment amd get the initial observation\n",
    "    # in most environments, the initial observation selected randomly.\n",
    "    obs = env.reset()\n",
    "\n",
    "    # obs shape (num_envs, observation_dim)\n",
    "    obs = flatten_space(obs, env.observation_space)\n",
    "    total_rewards = np.zeros(env.num_envs)\n",
    "    context = torch.zeros(\n",
    "        (env.num_envs, (max_steps + 1) * transition_dim),\n",
    "        device=device,\n",
    "        dtype=torch.long,\n",
    "    )\n",
    "    # context_idx is used to keep track of the current index in the context\n",
    "    context_idx = 0\n",
    "\n",
    "    # discretize the observation\n",
    "    # obs_token shape (num_envs, observation_dim)\n",
    "    obs_token = discretizer.encode(obs, subslice=(0, observation_dim))\n",
    "\n",
    "    value_placeholder = np.ones((env.num_envs, value_dim)) * 1e6\n",
    "\n",
    "    # update the context with the initial observation\n",
    "    context[:, :observation_dim] = torch.tensor(obs_token, device=device)\n",
    "\n",
    "    # tensor to keep track of which environments are done\n",
    "    dones = np.zeros(env.num_envs, dtype=np.bool)\n",
    "\n",
    "    # usually max_steps is set to default max_num_steps in the environment\n",
    "    for t in trange(max_steps, desc=\"Rollout\", leave=False):\n",
    "        # Process one step in the environment\n",
    "        # one step consists of selecting an action, taking a step in the environment,\n",
    "        # and updating the context with the new observation, action, reward, and value.\n",
    "        if t % plan_every == 0:\n",
    "            # every plan_every steps, we generate a new plan using beam search\n",
    "            # and store the predicted tokens in plan_buffer.\n",
    "            # higher plan_every means we are using the same plan for longer\n",
    "            # as a result, we are putting more trust in the model's prediction\n",
    "            # of the future states, actions, and rewards.\n",
    "\n",
    "            context_idx = (\n",
    "                ((t + 1) * transition_dim) - action_dim - reward_dim - value_dim\n",
    "            )\n",
    "            context_not_dones = context[~dones, :context_idx]\n",
    "\n",
    "            # generate a new plan using beam search\n",
    "            # predicted_tokens shape (num_envs, beam_steps * transition_dim)\n",
    "            predicted_tokens = vec_beam_plan(\n",
    "                model,\n",
    "                discretizer,\n",
    "                context_not_dones,\n",
    "                beam_width,\n",
    "                beam_steps,\n",
    "                beam_context,\n",
    "                sample_expansion,\n",
    "                observation_dim,\n",
    "                action_dim,\n",
    "                reward_dim,\n",
    "                value_dim,\n",
    "                transition_dim,\n",
    "                obs_top_k=obs_top_k,\n",
    "                act_top_k=act_top_k,\n",
    "                rew_top_k=rew_top_k,\n",
    "                temperature=temperature,\n",
    "                greedy=greedy,\n",
    "            )\n",
    "            plan_buffer = torch.zeros(\n",
    "                env.num_envs,\n",
    "                predicted_tokens.shape[-1],\n",
    "                device=device,\n",
    "                dtype=predicted_tokens.dtype,\n",
    "            )\n",
    "            plan_buffer[~dones] = predicted_tokens\n",
    "        else:\n",
    "            # if we are not generating a new plan, we use the plan_buffer\n",
    "            # to get the next transition_dim number of tokens\n",
    "            plan_buffer = plan_buffer[:, transition_dim:]\n",
    "\n",
    "        # get the action from the predicted tokens\n",
    "        # action_token shape (num_envs, action_dim)\n",
    "        action_token = plan_buffer[:, :action_dim].cpu().numpy()\n",
    "        # decode the action\n",
    "        # action shape (num_envs, action_dim)\n",
    "        action = discretizer.decode(\n",
    "            action_token, subslice=(observation_dim, observation_dim + action_dim)\n",
    "        )\n",
    "        action = unflatten_space(action, env.action_space)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        # next_obs shape (num_envs, observation_dim)\n",
    "        next_obs = flatten_space(next_obs, env.observation_space)\n",
    "        # discretize the next observation\n",
    "        # next_obs_token shape (num_envs, observation_dim)\n",
    "        next_obs_token = discretizer.encode(\n",
    "            next_obs[~dones], subslice=(0, observation_dim)\n",
    "        )\n",
    "        # discretize the reward and value\n",
    "        # reward_value_tokens shape (num_envs, reward_dim + value_dim)\n",
    "        reward_value_tokens = discretizer.encode(\n",
    "            np.hstack([reward.reshape(-1, reward_dim), value_placeholder]),\n",
    "            subslice=(transition_dim - reward_dim - action_dim, transition_dim),\n",
    "        )\n",
    "\n",
    "        # update the context\n",
    "        context_idx = t * transition_dim\n",
    "        # add action\n",
    "        context[\n",
    "            ~dones,\n",
    "            context_idx + observation_dim : context_idx + observation_dim + action_dim,\n",
    "        ] = torch.as_tensor(action_token[~dones], device=device)\n",
    "        # add reward and value\n",
    "        context[\n",
    "            ~dones, context_idx + observation_dim + action_dim : context_idx + transition_dim\n",
    "        ] = torch.as_tensor(reward_value_tokens[~dones], device=device)\n",
    "        # add next observation\n",
    "        context[\n",
    "            ~dones,\n",
    "            context_idx\n",
    "            + transition_dim : context_idx\n",
    "            + transition_dim\n",
    "            + observation_dim,\n",
    "        ] = torch.as_tensor(next_obs_token, device=device)\n",
    "\n",
    "        total_rewards[~dones] += reward[~dones]\n",
    "\n",
    "        dones[done] = True\n",
    "        if np.all(dones):\n",
    "            break\n",
    "    return total_rewards, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(\n",
    "    model: nn.Module,\n",
    "    batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
    "    vocab_size: int,\n",
    "    transition_dim: int,\n",
    "    observation_dim: int,\n",
    "    action_dim: int,\n",
    "    reward_dim: int,\n",
    "    value_dim: int,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> torch.Tensor:\n",
    "    # inputs shape (batch_size, seq_len)\n",
    "    # targets shape (batch_size, seq_len)\n",
    "    # loss_pad_mask shape (batch_size, seq_len)\n",
    "    inputs, targets, loss_pad_mask = batch\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    loss_pad_mask = loss_pad_mask.to(device)\n",
    "    # logits shape (batch_size, seq_len, vocab_size)\n",
    "    logits, _ = model(inputs)\n",
    "    # flatten the logits and targets to shape (batch_size * seq_len, vocab_size)\n",
    "    logits = logits.view(-1, vocab_size)\n",
    "    # flatten the targets to shape (batch_size * seq_len)\n",
    "    targets = targets.view(-1)\n",
    "    # loss shape (batch_size * seq_len)\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    n_states = int(np.ceil(inputs.shape[1] / transition_dim))\n",
    "    weights = torch.cat(\n",
    "        [\n",
    "            torch.ones(observation_dim, device=inputs.device),\n",
    "            torch.ones(action_dim, device=inputs.device) * 5,\n",
    "            torch.ones(reward_dim, device=inputs.device) * 1,\n",
    "            torch.ones(value_dim, device=inputs.device) * 1,\n",
    "        ]\n",
    "    )\n",
    "    weights = weights.repeat(n_states)[1:].repeat(inputs.shape[0], 1)\n",
    "    loss = loss * weights.view(-1)\n",
    "    # apply the loss pad mask to the loss because we don't want to calculate the loss for padded values\n",
    "    loss = (loss * loss_pad_mask.view(-1)).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def vec_eval(\n",
    "    env: Env,\n",
    "    model: nn.Module,\n",
    "    discretizer: KBinsDiscretizer,\n",
    "    num_episodes: int,\n",
    "    beam_width: int,\n",
    "    beam_steps: int,\n",
    "    beam_context: int,\n",
    "    sample_expansion: int,\n",
    "    observation_dim: int,\n",
    "    action_dim: int,\n",
    "    reward_dim: int,\n",
    "    value_dim: int,\n",
    "    transition_dim: int,\n",
    "    plan_every: int,\n",
    "    obs_top_k: Optional[int] = None,\n",
    "    act_top_k: Optional[int] = None,\n",
    "    rew_top_k: Optional[int] = None,\n",
    "    temperature: float = 1.0,\n",
    "    greedy: bool = False,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    vec_env = DummyVecEnv(\n",
    "        [lambda: gym.make(\"AdroitHandHammer-v1\") for _ in range(num_episodes)]\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    total_rewards, dones = vec_rollout(\n",
    "        model,\n",
    "        vec_env,\n",
    "        discretizer,\n",
    "        beam_width,\n",
    "        beam_steps,\n",
    "        beam_context,\n",
    "        sample_expansion,\n",
    "        observation_dim,\n",
    "        action_dim,\n",
    "        reward_dim,\n",
    "        value_dim,\n",
    "        transition_dim,\n",
    "        vec_env.envs[0]._max_episode_steps, # doing this because we want the done to be True when the episode is done\n",
    "        plan_every,\n",
    "        obs_top_k=obs_top_k,\n",
    "        act_top_k=act_top_k,\n",
    "        rew_top_k=rew_top_k,\n",
    "        temperature=temperature,\n",
    "        greedy=greedy,\n",
    "        device=device,\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    mean_rewards = np.mean(total_rewards)\n",
    "    std_rewards = np.std(total_rewards)\n",
    "\n",
    "    done_ratio = np.mean(dones)\n",
    "\n",
    "    model.train()\n",
    "    return mean_rewards, std_rewards, done_ratio, end_time - start_time, 0\n",
    "\n",
    "\n",
    "def calculate_predictive_accuracy(\n",
    "    model: nn.Module,\n",
    "    dataloader: Subset,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for data in tqdm(dataloader, desc=\"Calculating predictive accuracy\", leave=False):\n",
    "        x, y, mask = data\n",
    "        x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
    "        logits, _ = model(x)\n",
    "        # (batch_size, seq_len, vocab_size) -> (batch_size * seq_len, vocab_size)\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        y = y.view(-1)\n",
    "        mask = mask.view(-1)\n",
    "        # only consider the tokens that are not masked\n",
    "        mask_idx = mask.nonzero(as_tuple=True)[0]\n",
    "        y = y[mask_idx]\n",
    "        logits = logits[mask_idx]\n",
    "        # (batch_size * seq_len) -> (batch_size)\n",
    "        y_pred = torch.argmax(logits, dim=-1)\n",
    "        correct = (y_pred == y).sum().item()\n",
    "        total_correct += correct\n",
    "        total_samples += y.shape[0]\n",
    "    model.train()\n",
    "    return total_correct / total_samples\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    train_dataloader: DataLoader,\n",
    "    test_dataloader: DataLoader,\n",
    "    discretizer: KBinsDiscretizer,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    vocab_size: int,\n",
    "    n_epochs: int,\n",
    "    writer: SummaryWriter,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    "    eval_every: int = 10,\n",
    "    checkpoint_path: Optional[str] = None,\n",
    "):\n",
    "    model.train()\n",
    "    step = 0\n",
    "    for epoch in trange(n_epochs, desc=\"Training\"):\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(\n",
    "            train_dataloader, desc=f\"Epoch {epoch + 1}/{n_epochs}\", leave=False\n",
    "        ):\n",
    "            optimizer.zero_grad()\n",
    "            loss = calculate_loss(\n",
    "                model,\n",
    "                batch,\n",
    "                vocab_size,\n",
    "                device=device,\n",
    "                transition_dim=transition_dim,\n",
    "                observation_dim=observation_dim,\n",
    "                action_dim=action_dim,\n",
    "                reward_dim=reward_dim,\n",
    "                value_dim=value_dim,\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar(\"Loss/train\", loss.item(), step)\n",
    "            total_loss += loss.item()\n",
    "            step += 1\n",
    "        writer.add_scalar(\"Epoch\", epoch, epoch)\n",
    "        end_time = time.time()\n",
    "        writer.add_scalar(\"Time/train\", end_time - start_time, epoch)\n",
    "\n",
    "        if epoch % eval_every == 0:\n",
    "            start_time = time.time()\n",
    "\n",
    "            train_accuracy = calculate_predictive_accuracy(\n",
    "                model, train_dataloader, device=device\n",
    "            )\n",
    "            test_accuracy = calculate_predictive_accuracy(\n",
    "                model, test_dataloader, device=device\n",
    "            )\n",
    "            writer.add_scalar(\"Accuracy/train\", train_accuracy, epoch)\n",
    "            writer.add_scalar(\"Accuracy/test\", test_accuracy, epoch)\n",
    "\n",
    "            (\n",
    "                mean_rewards,\n",
    "                std_rewards,\n",
    "                done_ratio,\n",
    "                mean_rollout_time,\n",
    "                std_rollout_time,\n",
    "            ) = vec_eval(\n",
    "                env,\n",
    "                model,\n",
    "                discretizer,\n",
    "                num_episodes=10 if not local else 10,\n",
    "                beam_width=32 if not local else 2,\n",
    "                beam_steps=5 if not local else 5,\n",
    "                beam_context=5 if not local else 7,\n",
    "                sample_expansion=2 if not local else 1,\n",
    "                observation_dim=observation_dim,\n",
    "                action_dim=action_dim,\n",
    "                reward_dim=reward_dim,\n",
    "                value_dim=value_dim,\n",
    "                transition_dim=transition_dim,\n",
    "                plan_every=2 if not local else 2,\n",
    "                obs_top_k=1,\n",
    "                act_top_k=20,\n",
    "                rew_top_k=None,\n",
    "                temperature=1.0,\n",
    "                greedy=False,\n",
    "                device=device,\n",
    "            )\n",
    "            writer.add_scalar(\"Reward/mean\", mean_rewards, epoch)\n",
    "            writer.add_scalar(\"Reward/std\", std_rewards, epoch)\n",
    "            writer.add_scalar(\"Done ratio\", done_ratio, epoch)\n",
    "            writer.add_scalar(\"Rollout time/mean\", mean_rollout_time, epoch)\n",
    "            writer.add_scalar(\"Rollout time/std\", std_rollout_time, epoch)\n",
    "\n",
    "            end_time = time.time()\n",
    "            writer.add_scalar(\"Time/eval\", end_time - start_time, epoch)\n",
    "\n",
    "        if checkpoint_path:\n",
    "            torch.save(model.state_dict(), checkpoint_path + \"model.pth\")\n",
    "            torch.save(optimizer.state_dict(), checkpoint_path + \"optimizer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92c68f6de194633aebc31b4cac7fd77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8e7982197a443b859af4c85b9c0608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/2:   0%|          | 0/249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21af9361b0d6437f97d7c21fb0829728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating predictive accuracy:   0%|          | 0/249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3198348fdd94a519918cd0e8a63469b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating predictive accuracy:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de2e0f467ba493c8a76c3bf9068f43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rollout:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31afd73b3fac4c2980cf6a8b9aa25b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b883a57193e146d0ac90f037aada1486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fe8fc3e84841db98bd47c80474a024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f09cba861c478297dea58c0bb605c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75ecd678607484b82399dde54550123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f139d289e1544fe4a5754444bd7ceec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a5c682c80f40fbb4560c5b8bdeb02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353723d3d85b410f81503d988d602bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef01be6fdf814d6a934e13559faa9a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2afaa737bab4473bbf282f89058b211b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83de1f19e984f279978b47b03331217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47aff5a312594209afc774287dd43ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57affe9daa14a76a52d1f63b19f1ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85bfdb9f3a341dc913738ac21c34ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313ee8f8e36e48ef893963aca5162c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399d757da8b64235812522699c33cf7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b05b9d842a4eca814d300d545097ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063b53a6f8404bd2b3d9cac145a14270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 35\u001b[0m\n\u001b[1;32m     31\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(\n\u001b[1;32m     32\u001b[0m         torch\u001b[38;5;241m.\u001b[39mload(checkpoint_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     33\u001b[0m     )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscretizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved checkpoint to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 264\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, discretizer, optimizer, vocab_size, n_epochs, writer, device, eval_every, checkpoint_path)\u001b[0m\n\u001b[1;32m    255\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy/train\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_accuracy, epoch)\n\u001b[1;32m    256\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy/test\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_accuracy, epoch)\n\u001b[1;32m    258\u001b[0m (\n\u001b[1;32m    259\u001b[0m     mean_rewards,\n\u001b[1;32m    260\u001b[0m     std_rewards,\n\u001b[1;32m    261\u001b[0m     done_ratio,\n\u001b[1;32m    262\u001b[0m     mean_rollout_time,\n\u001b[1;32m    263\u001b[0m     std_rollout_time,\n\u001b[0;32m--> 264\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mvec_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscretizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlocal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlocal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlocal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlocal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_expansion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlocal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservation_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreward_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransition_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransition_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplan_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlocal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobs_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mact_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrew_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReward/mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, mean_rewards, epoch)\n\u001b[1;32m    287\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReward/std\u001b[39m\u001b[38;5;124m\"\u001b[39m, std_rewards, epoch)\n",
      "Cell \u001b[0;32mIn[10], line 143\u001b[0m, in \u001b[0;36mvec_eval\u001b[0;34m(env, model, discretizer, num_episodes, beam_width, beam_steps, beam_context, sample_expansion, observation_dim, action_dim, reward_dim, value_dim, transition_dim, plan_every, obs_top_k, act_top_k, rew_top_k, temperature, greedy, device)\u001b[0m\n\u001b[1;32m    139\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m DummyVecEnv(\n\u001b[1;32m    140\u001b[0m     [\u001b[38;5;28;01mlambda\u001b[39;00m: gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdroitHandHammer-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes)]\n\u001b[1;32m    141\u001b[0m )\n\u001b[1;32m    142\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 143\u001b[0m total_rewards, dones \u001b[38;5;241m=\u001b[39m \u001b[43mvec_rollout\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvec_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscretizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_expansion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservation_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreward_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransition_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvec_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_episode_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# doing this because we want the done to be True when the episode is done\u001b[39;49;00m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplan_every\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobs_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobs_top_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mact_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mact_top_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrew_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrew_top_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgreedy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    166\u001b[0m mean_rewards \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(total_rewards)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/blog-FROQ9Grm-py3.11/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 538\u001b[0m, in \u001b[0;36mvec_rollout\u001b[0;34m(model, env, discretizer, beam_width, beam_steps, beam_context, sample_expansion, observation_dim, action_dim, reward_dim, value_dim, transition_dim, max_steps, plan_every, obs_top_k, act_top_k, rew_top_k, temperature, greedy, device)\u001b[0m\n\u001b[1;32m    534\u001b[0m context_idx \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    535\u001b[0m     ((t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m transition_dim) \u001b[38;5;241m-\u001b[39m action_dim \u001b[38;5;241m-\u001b[39m reward_dim \u001b[38;5;241m-\u001b[39m value_dim\n\u001b[1;32m    536\u001b[0m )\n\u001b[1;32m    537\u001b[0m context_not_dones \u001b[38;5;241m=\u001b[39m context[\u001b[38;5;241m~\u001b[39mdones, :context_idx]\n\u001b[0;32m--> 538\u001b[0m predicted_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mvec_beam_plan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscretizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_not_dones\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_expansion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservation_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreward_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransition_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobs_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobs_top_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mact_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mact_top_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrew_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrew_top_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgreedy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m plan_buffer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    558\u001b[0m     env\u001b[38;5;241m.\u001b[39mnum_envs,\n\u001b[1;32m    559\u001b[0m     predicted_tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    560\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    561\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mpredicted_tokens\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    562\u001b[0m )\n\u001b[1;32m    563\u001b[0m plan_buffer[\u001b[38;5;241m~\u001b[39mdones] \u001b[38;5;241m=\u001b[39m predicted_tokens\n",
      "Cell \u001b[0;32mIn[9], line 336\u001b[0m, in \u001b[0;36mvec_beam_plan\u001b[0;34m(model, discretizer, context, beam_width, beam_steps, beam_context, sample_expansion, observation_dim, action_dim, reward_dim, value_dim, transition_dim, obs_top_k, act_top_k, rew_top_k, temperature, greedy)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m<\u001b[39m beam_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;66;03m# sample observations only if we are not at the last step, why?\u001b[39;00m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;66;03m# because beam plan has to end with a valid transition [...., obs, act, rew, val]\u001b[39;00m\n\u001b[1;32m    335\u001b[0m         plan \u001b[38;5;241m=\u001b[39m plan\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m beam_width, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 336\u001b[0m         plan, kv_caches, _ \u001b[38;5;241m=\u001b[39m \u001b[43msample_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m            \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbest_kv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobs_top_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgreedy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m         plan \u001b[38;5;241m=\u001b[39m plan\u001b[38;5;241m.\u001b[39mview(batch_size, beam_width, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# (batch_size, beam_width) -> (batch_size)\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;66;03m# for each batch, select the plan with the highest value and return it's index\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 107\u001b[0m, in \u001b[0;36msample_tokens\u001b[0;34m(model, context, kv_caches, n_steps, temperature, greedy, top_k)\u001b[0m\n\u001b[1;32m    104\u001b[0m logits, kv_caches \u001b[38;5;241m=\u001b[39m model(context[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:], kv_caches)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Sample the next token\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# (batch_size, 1)\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[43msample_token_from_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgreedy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([context, token], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    113\u001b[0m raw_logits[:, i] \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[9], line 33\u001b[0m, in \u001b[0;36msample_token_from_logits\u001b[0;34m(logits, temperature, greedy, top_k)\u001b[0m\n\u001b[1;32m     30\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Sample from the top-k indices\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# (batch_size, top_k) -> (batch_size, 1)\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Sample from the logits\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Using device: {device}\")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "dataset.discretizer.to(device)\n",
    "# split the dataset into train and test\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, test_size]\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4 if not local else 0,\n",
    "    shuffle=True,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")\n",
    "model = TrajectoryTransformer(\n",
    "    seq_len, embedding_dim, n_heads, transition_dim, n_blocks, vocab_size\n",
    ").to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "if load_checkpoint and os.path.exists(checkpoint_path + \"model.pth\"):\n",
    "    print(f\"Loading model from {checkpoint_path}\")\n",
    "    model.load_state_dict(\n",
    "        torch.load(checkpoint_path + \"model.pth\", map_location=device)\n",
    "    )\n",
    "    optimizer.load_state_dict(\n",
    "        torch.load(checkpoint_path + \"optimizer.pth\", map_location=device)\n",
    "    )\n",
    "else:\n",
    "    train(\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        test_dataloader,\n",
    "        dataset.discretizer,\n",
    "        optimizer,\n",
    "        vocab_size,\n",
    "        n_epochs,\n",
    "        writer,\n",
    "        device=device,\n",
    "        eval_every=eval_every,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "    )\n",
    "    print(f\"Saved checkpoint to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    mean_rewards,\n",
    "    std_rewards,\n",
    "    done_ratio,\n",
    "    mean_rollout_time,\n",
    "    std_rollout_time,\n",
    ") = vec_eval(\n",
    "    env,\n",
    "    model,\n",
    "    dataset.discretizer,\n",
    "    num_episodes=10 if not local else 10,\n",
    "    beam_width=32 if not local else 2,\n",
    "    beam_steps=5 if not local else 5,\n",
    "    beam_context=5 if not local else 7,\n",
    "    sample_expansion=2 if not local else 1,\n",
    "    observation_dim=observation_dim,\n",
    "    action_dim=action_dim,\n",
    "    reward_dim=reward_dim,\n",
    "    value_dim=value_dim,\n",
    "    transition_dim=transition_dim,\n",
    "    plan_every=1 if not local else 2,\n",
    "    obs_top_k=1,\n",
    "    act_top_k=20,\n",
    "    rew_top_k=None,\n",
    "    temperature=1.0,\n",
    "    greedy=False,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean rewards: {mean_rewards}\")\n",
    "print(f\"Std rewards: {std_rewards}\")\n",
    "print(f\"Done ratio: {done_ratio}\")\n",
    "print(f\"Mean rollout time: {mean_rollout_time}\")\n",
    "print(f\"Std rollout time: {std_rollout_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from IPython.display import Image\n",
    "\n",
    "# save the images as a gif\n",
    "imageio.mimsave(f\"{base_dir}/trajectory.gif\", imgs, fps=30)\n",
    "# display the gif and repeat it forever\n",
    "Image(filename=f\"{base_dir}/trajectory.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_plan(\n",
    "    model: nn.Module,\n",
    "    discretizer: KBinsDiscretizer,\n",
    "    context: torch.Tensor,\n",
    "    beam_width: int,\n",
    "    beam_steps: int,\n",
    "    beam_context: int,\n",
    "    sample_expansion: int,\n",
    "    observation_dim: int,\n",
    "    action_dim: int,\n",
    "    reward_dim: int,\n",
    "    value_dim: int,\n",
    "    transition_dim: int,\n",
    "    obs_top_k: Optional[int] = None,\n",
    "    act_top_k: Optional[int] = None,\n",
    "    rew_top_k: Optional[int] = None,\n",
    "    temperature: float = 1.0,\n",
    "    greedy: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    tokens_context_size = beam_context * transition_dim\n",
    "    n_crop = round_to_multiple(\n",
    "        max(0, context.shape[1] - tokens_context_size), transition_dim\n",
    "    )\n",
    "    context = context[:, n_crop:]\n",
    "    # context shape (seq_len) -> (beam_width, seq_len)\n",
    "    plan = context.repeat(beam_width, 1)\n",
    "\n",
    "    rewards = torch.zeros(beam_width, beam_steps + 1, device=context.device)\n",
    "    discounts = discount_factor ** torch.arange(beam_steps + 1, device=context.device)\n",
    "\n",
    "    kv_caches = None\n",
    "\n",
    "    for t in trange(beam_steps, desc=\"Beam Search\", leave=False):\n",
    "        # (beam_width, n_tokens) -> (beam_width * sample_expansion, n_tokens)\n",
    "        plan = plan.repeat(sample_expansion, 1)\n",
    "        rewards = rewards.repeat(sample_expansion, 1)\n",
    "        if kv_caches is not None:\n",
    "            # shape of kv is (beam width, kv len, embedding dim)\n",
    "            # as repeat it will become (beam width * sample_expansion, kv len, embedding dim)\n",
    "            kv_caches = [kv.repeat(sample_expansion, 1, 1) for kv in kv_caches]\n",
    "\n",
    "        # sample actions\n",
    "        plan, kv_caches, _ = sample_tokens(\n",
    "            model,\n",
    "            plan,\n",
    "            kv_caches,\n",
    "            n_steps=action_dim,\n",
    "            top_k=act_top_k,\n",
    "            temperature=temperature,\n",
    "            greedy=greedy,\n",
    "        )\n",
    "\n",
    "        # sample rewards and values\n",
    "        # plan (beam_width * sample_expansion, n_tokens) -> (beam_width * sample_expansion, n_tokens + reward_dim + value_dim)\n",
    "        # logits shape (beam_width * sample_expansion, reward_dim + value_dim, vocab_size)\n",
    "        plan, kv_caches, logits = sample_tokens(\n",
    "            model,\n",
    "            plan,\n",
    "            kv_caches,\n",
    "            n_steps=reward_dim + value_dim,\n",
    "            top_k=rew_top_k,\n",
    "            temperature=temperature,\n",
    "            greedy=greedy,\n",
    "        )\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        rewards_and_values = discretizer.expectation(\n",
    "            probs, subslice=(transition_dim - 2, transition_dim)\n",
    "        )\n",
    "\n",
    "        rewards[:, t:t + 2] = rewards_and_values\n",
    "        # Did you notice that rewards contains rewards at t and values at t+1, why?\n",
    "        #   when we want to calculate value (rewards to go) at t, we need to consider discounted rewards from 0 to t\n",
    "        #   and also future discounted rewards from t+1 to end. It is a bit awkward to apply discount factor to value (at t+1)\n",
    "        #   because predicted value is already discounted.\n",
    "        # (beam_width * sample_expansion, beam_steps + 1) * (beam_steps + 1) -> (beam_width * sample_expansion)\n",
    "        values = (rewards * discounts).sum(dim=-1)\n",
    "\n",
    "        # select top-k sequences\n",
    "        # (beam_width * sample_expansion) -> (beam_width)\n",
    "        values, idx = torch.topk(values, beam_width)\n",
    "\n",
    "        plan, rewards = plan[idx], rewards[idx]\n",
    "\n",
    "        kv_caches = [kv[idx] for kv in kv_caches]\n",
    "\n",
    "        if t < beam_steps - 1:\n",
    "            # sample observations only if we are not at the last step, why?\n",
    "            # because beam plan has to end with a valid transition [...., obs, act, rew, val]\n",
    "            plan, kv_caches, _ = sample_tokens(\n",
    "                model,\n",
    "                plan,\n",
    "                kv_caches,\n",
    "                n_steps=observation_dim,\n",
    "                top_k=obs_top_k,\n",
    "                temperature=temperature,\n",
    "                greedy=greedy,\n",
    "            )\n",
    "\n",
    "    best_idx = torch.argmax(values)\n",
    "    # only return the best plan without the context\n",
    "    best_plan = plan[best_idx, context.shape[1] :]\n",
    "\n",
    "    return best_plan\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def rollout(\n",
    "    model: nn.Module,\n",
    "    env: Env,\n",
    "    discretizer: KBinsDiscretizer,\n",
    "    beam_width: int,\n",
    "    beam_steps: int,\n",
    "    beam_context: int,\n",
    "    sample_expansion: int,\n",
    "    observation_dim: int,\n",
    "    action_dim: int,\n",
    "    reward_dim: int,\n",
    "    value_dim: int,\n",
    "    transition_dim: int,\n",
    "    max_steps: int,\n",
    "    plan_every: int,\n",
    "    obs_top_k: Optional[int] = None,\n",
    "    act_top_k: Optional[int] = None,\n",
    "    rew_top_k: Optional[int] = None,\n",
    "    temperature: float = 1.0,\n",
    "    greedy: bool = False,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    "    generate_gif: bool = False,\n",
    "):\n",
    "    trajectory = []\n",
    "    assert (\n",
    "        plan_every <= beam_steps\n",
    "    ), f\"plan_every {plan_every} should be less than or equal to beam_steps {beam_steps}\"\n",
    "    obs, _ = env.reset()\n",
    "    imgs = []\n",
    "    if generate_gif:\n",
    "        imgs.append(env.render())\n",
    "    obs = flatten_space(obs, env.observation_space)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    context = torch.zeros(\n",
    "        (1, (max_steps + 1) * transition_dim), device=device, dtype=torch.long\n",
    "    )\n",
    "\n",
    "    context_idx = 0\n",
    "\n",
    "    # discretize the observation\n",
    "    obs_token = discretizer.encode(\n",
    "        np.array([obs]), subslice=(0, observation_dim)\n",
    "    ).squeeze()\n",
    "\n",
    "    context[:, :observation_dim] = torch.tensor(obs_token, device=device)\n",
    "\n",
    "    for t in trange(max_steps, desc=\"Rollout\", leave=False):\n",
    "        if t % plan_every == 0:\n",
    "            # we need to plan a new trajectory, reset the context observation at t step\n",
    "            context_idx = (\n",
    "                ((t + 1) * transition_dim) - action_dim - reward_dim - value_dim\n",
    "            )\n",
    "            predicted_tokens = beam_plan(\n",
    "                model,\n",
    "                discretizer,\n",
    "                context[:, :context_idx],\n",
    "                beam_width,\n",
    "                beam_steps,\n",
    "                beam_context,\n",
    "                sample_expansion,\n",
    "                observation_dim,\n",
    "                action_dim,\n",
    "                reward_dim,\n",
    "                value_dim,\n",
    "                transition_dim,\n",
    "                obs_top_k=obs_top_k,\n",
    "                act_top_k=act_top_k,\n",
    "                rew_top_k=rew_top_k,\n",
    "                temperature=temperature,\n",
    "                greedy=greedy,\n",
    "            )\n",
    "        else:\n",
    "            predicted_tokens = predicted_tokens[transition_dim:]\n",
    "\n",
    "        # get the action from the predicted tokens\n",
    "        action_token = predicted_tokens[:action_dim].cpu().numpy()\n",
    "        # decode the action\n",
    "        action = discretizer.decode(\n",
    "            action_token, subslice=(observation_dim, observation_dim + action_dim)\n",
    "        ).squeeze()\n",
    "        action = unflatten_space(action, env.action_space)\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_obs = flatten_space(next_obs, env.observation_space)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if generate_gif:\n",
    "            imgs.append(env.render())\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # discretize the next observation\n",
    "        next_obs_token = discretizer.encode(next_obs, subslice=(0, observation_dim))\n",
    "        # discretize the reward and value\n",
    "        reward_value_tokens = discretizer.encode(\n",
    "            np.array([reward, 1e6]), subslice=(transition_dim - 2, transition_dim)\n",
    "        )\n",
    "\n",
    "        # update the context\n",
    "        context_idx = t * transition_dim\n",
    "        # add action\n",
    "        context[\n",
    "            :,\n",
    "            context_idx + observation_dim : context_idx + observation_dim + action_dim,\n",
    "        ] = torch.as_tensor(action_token, device=device)\n",
    "        # add reward and value\n",
    "        context[\n",
    "            :, context_idx + observation_dim + action_dim : context_idx + transition_dim\n",
    "        ] = torch.as_tensor(reward_value_tokens, device=device)\n",
    "        # add next observation\n",
    "        context[\n",
    "            :,\n",
    "            context_idx\n",
    "            + transition_dim : context_idx\n",
    "            + transition_dim\n",
    "            + observation_dim,\n",
    "        ] = torch.as_tensor(next_obs_token, device=device)\n",
    "\n",
    "        trajectory.append((obs, next_obs, action, reward, done))\n",
    "        obs = next_obs\n",
    "\n",
    "    if generate_gif:\n",
    "        return total_reward, trajectory, imgs, terminated\n",
    "    return total_reward, trajectory, _, terminated\n",
    "\n",
    "\n",
    "def eval(\n",
    "    env: Env,\n",
    "    model: nn.Module,\n",
    "    discretizer: KBinsDiscretizer,\n",
    "    num_episodes: int,\n",
    "    beam_width: int,\n",
    "    beam_steps: int,\n",
    "    beam_context: int,\n",
    "    sample_expansion: int,\n",
    "    observation_dim: int,\n",
    "    action_dim: int,\n",
    "    reward_dim: int,\n",
    "    value_dim: int,\n",
    "    transition_dim: int,\n",
    "    max_steps: int,\n",
    "    plan_every: int,\n",
    "    obs_top_k: Optional[int] = None,\n",
    "    act_top_k: Optional[int] = None,\n",
    "    rew_top_k: Optional[int] = None,\n",
    "    temperature: float = 1.0,\n",
    "    greedy: bool = False,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    total_rewards = []\n",
    "    dones = []\n",
    "    rollout_times = []\n",
    "\n",
    "    for _ in trange(num_episodes, desc=\"Evaluating episode\", leave=False):\n",
    "        start_time = time.time()\n",
    "        total_reward, _, _, done = rollout(\n",
    "            model,\n",
    "            env,\n",
    "            discretizer,\n",
    "            beam_width,\n",
    "            beam_steps,\n",
    "            beam_context,\n",
    "            sample_expansion,\n",
    "            observation_dim,\n",
    "            action_dim,\n",
    "            reward_dim,\n",
    "            value_dim,\n",
    "            transition_dim,\n",
    "            max_steps,\n",
    "            plan_every,\n",
    "            obs_top_k=obs_top_k,\n",
    "            act_top_k=act_top_k,\n",
    "            rew_top_k=rew_top_k,\n",
    "            temperature=temperature,\n",
    "            greedy=greedy,\n",
    "            device=device,\n",
    "        )\n",
    "        end_time = time.time()\n",
    "\n",
    "        rollout_times.append(end_time - start_time)\n",
    "        total_rewards.append(total_reward)\n",
    "        dones.append(done)\n",
    "\n",
    "    mean_rewards = np.mean(total_rewards)\n",
    "    std_rewards = np.std(total_rewards)\n",
    "\n",
    "    done_ratio = np.mean(dones)\n",
    "\n",
    "    mean_rollout_time = np.mean(rollout_times)\n",
    "    std_rollout_time = np.std(rollout_times)\n",
    "\n",
    "    model.train()\n",
    "    return mean_rewards, std_rewards, done_ratio, mean_rollout_time, std_rollout_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog-FROQ9Grm-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
