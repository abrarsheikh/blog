{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from typing import List\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Optional\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from minari import EpisodeData, MinariDataset\n",
    "import minari\n",
    "from gymnasium import Env\n",
    "import os\n",
    "from minari import get_normalized_score\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation dim: 8, Action dim: 2\n",
      "Reward dim: 1, Value dim: 1\n",
      "Transition dim: 12\n",
      "Number of episodes: 100\n"
     ]
    }
   ],
   "source": [
    "def get_space_dim(space):\n",
    "    if isinstance(space, spaces.Discrete):\n",
    "        return 1\n",
    "    elif isinstance(space, spaces.Box):\n",
    "        return space.shape[0]\n",
    "    elif isinstance(space, spaces.Dict):\n",
    "        return sum([get_space_dim(v) for v in space.values()])\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported observation space\")\n",
    "\n",
    "\n",
    "dataset_ref = \"D4RL/pointmaze/medium-dense-v2\"\n",
    "base_m_dataset = minari.load_dataset(dataset_ref, download=True)\n",
    "wrapped_env = base_m_dataset.recover_environment(\n",
    "    render_mode=\"rgb_array\", continuing_task=False\n",
    ")\n",
    "env = wrapped_env.unwrapped\n",
    "# Environment parameters\n",
    "observation_dim = get_space_dim(env.observation_space)\n",
    "action_dim = get_space_dim(env.action_space)\n",
    "reward_dim = 1\n",
    "value_dim = 1\n",
    "transition_dim = observation_dim + action_dim + reward_dim + value_dim\n",
    "\n",
    "print(f\"Observation dim: {observation_dim}, Action dim: {action_dim}\")\n",
    "print(f\"Reward dim: {reward_dim}, Value dim: {value_dim}\")\n",
    "print(f\"Transition dim: {transition_dim}\")\n",
    "\n",
    "local = not torch.cuda.is_available()\n",
    "\n",
    "# Model parameters\n",
    "n_transitions = 10\n",
    "seq_len = n_transitions * transition_dim\n",
    "vocab_size = 100\n",
    "max_bins = vocab_size\n",
    "discount_factor = 0.99\n",
    "embedding_dim = 32 if not local else 32\n",
    "n_heads = 4\n",
    "n_blocks = 4\n",
    "n_epochs = 25 if not local else 1\n",
    "batch_size = 128\n",
    "lr = 0.0006\n",
    "eval_every = 2 if not local else 1\n",
    "\n",
    "# other parameters\n",
    "n_episodes: Optional[int] = None if not local else 100\n",
    "# create a directory to save the model\n",
    "base_dir = f\"data/{dataset_ref}\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "checkpoint_path = f\"{base_dir}/\"\n",
    "load_checkpoint = (\n",
    "    False  # set to False if you want to train from scratch even if a checkpoint exists\n",
    ")\n",
    "if n_episodes:\n",
    "    m_dataset = base_m_dataset.sample_episodes(n_episodes)\n",
    "else:\n",
    "    m_dataset = base_m_dataset\n",
    "\n",
    "print(f\"Number of episodes: {len(m_dataset)}\")\n",
    "\n",
    "device = torch.device(\n",
    "    \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KBinsDiscretizer:\n",
    "    def __init__(self, dataset: np.ndarray, n_bins: int, strategy: str = \"ordinal\"):\n",
    "        self.n_bins = n_bins\n",
    "        self.strategy = strategy\n",
    "\n",
    "        # bin_edges shape: (n_features, n_bins + 1)\n",
    "        self.bin_edges = self._find_bin_edges(dataset)\n",
    "\n",
    "        self.bin_centers = (self.bin_edges[:, :-1] + self.bin_edges[:, 1:]) * 0.5\n",
    "        self.bin_centers_torch = torch.from_numpy(self.bin_centers).float()\n",
    "\n",
    "    def _find_bin_edges(self, dataset: np.ndarray):\n",
    "        bin_edges = []\n",
    "        if self.strategy == \"uniform\":\n",
    "            mins, maxs = np.min(dataset, axis=0), np.max(dataset, axis=0)\n",
    "            bin_edges = np.linspace(mins, maxs, self.n_bins + 1).T\n",
    "        elif self.strategy == \"quantile\":\n",
    "            quantiles = np.linspace(0, 100, self.n_bins + 1)\n",
    "            bin_edges = np.percentile(dataset, quantiles, axis=0).T\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {self.strategy}\")\n",
    "        return bin_edges\n",
    "\n",
    "    def encode(\n",
    "        self, X: np.ndarray, subslice: Optional[Tuple[int, int]] = None\n",
    "    ) -> np.ndarray:\n",
    "        if X.ndim == 1:\n",
    "            # this is to handle the case where we have a single sample\n",
    "            X = X[None]\n",
    "        # data shape: (n_samples, n_features)\n",
    "        edges = self.bin_edges\n",
    "        if subslice is not None:\n",
    "            start, end = subslice\n",
    "            edges = edges[start:end]\n",
    "\n",
    "        Xt = np.zeros(X.shape, dtype=np.long)\n",
    "\n",
    "        # See documentation of numpy.isclose for an explanation of ``rtol`` and ``atol``.\n",
    "        rtol = 1.0e-5\n",
    "        atol = 1.0e-8\n",
    "\n",
    "        for jj in range(X.shape[1]):\n",
    "            # Values which are close to a bin edge are susceptible to numeric\n",
    "            # instability. Add eps to X so these values are binned correctly\n",
    "            # with respect to their decimal truncation.\n",
    "            eps = atol + rtol * np.abs(X[:, jj])\n",
    "            # why [1:]? bins = edges - 1, but its unclear why we leave out the first element and not the last\n",
    "            Xt[:, jj] = np.digitize(X[:, jj] + eps, edges[jj][1:])\n",
    "\n",
    "        np.clip(Xt, 0, self.n_bins - 1, out=Xt)\n",
    "\n",
    "        return Xt\n",
    "\n",
    "    def decode(\n",
    "        self, Xt: np.ndarray, subslice: Optional[Tuple[int, int]] = None\n",
    "    ) -> np.ndarray:\n",
    "        if Xt.ndim == 1:\n",
    "            # this is to handle the case where we have a single sample\n",
    "            Xt = Xt[None]\n",
    "        # data shape: (n_samples, n_features)\n",
    "        centers = self.bin_centers\n",
    "        if subslice is not None:\n",
    "            start, end = subslice\n",
    "            centers = centers[start:end]\n",
    "\n",
    "        X = np.zeros(Xt.shape, dtype=np.float64)\n",
    "        for jj in range(Xt.shape[1]):\n",
    "            X[:, jj] = centers[jj, np.int_(Xt[:, jj])]\n",
    "\n",
    "        return X\n",
    "\n",
    "    def expectation(\n",
    "        self, probs: np.ndarray, subslice: Optional[Tuple[int, int]] = None\n",
    "    ) -> np.ndarray:\n",
    "        if probs.ndim == 1:\n",
    "            # this is to handle the case where we have a single sample\n",
    "            probs = probs[None]\n",
    "        # probs shape: (batch_size, n_features, n_bins)\n",
    "        # bin_centers shape: (n_features, n_bins) -> (1, n_features, n_bins)\n",
    "        if torch.is_tensor(probs):\n",
    "            bin_centers = self.bin_centers_torch.unsqueeze(0)\n",
    "        else:\n",
    "            # bin_centers shape: (n_features, n_bins) -> (1, n_features, n_bins)\n",
    "            bin_centers = self.bin_centers[None]\n",
    "\n",
    "        if subslice is not None:\n",
    "            start, end = subslice\n",
    "            bin_centers = bin_centers[:, start:end]\n",
    "\n",
    "        # (batch_size, n_features, n_bins) * (1, n_features, n_bins) -> sum (batch_size, n_features, n_bins) -> (batch_size, n_features)\n",
    "        X = (probs * bin_centers).sum(axis=-1)\n",
    "        return X\n",
    "\n",
    "    def to(self, device):\n",
    "        self.bin_centers_torch = self.bin_centers_torch.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Test array\n",
    "test_arr = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Initialize the discretizer\n",
    "discretizer = KBinsDiscretizer(test_arr, 1000, strategy=\"uniform\")\n",
    "\n",
    "# Encode and decode the test array\n",
    "encoded = discretizer.encode(test_arr)\n",
    "decoded = discretizer.decode(encoded)\n",
    "\n",
    "# Check if the decoded array is close to the original array\n",
    "assert np.isclose(\n",
    "    decoded, test_arr, atol=1e-2\n",
    ").all(), f\"Decoded array {decoded} is not close to the original array {test_arr}\"\n",
    "\n",
    "# Generate random probabilities\n",
    "probs = F.softmax(torch.from_numpy(np.random.rand(3, 2, 1000)), dim=-1).numpy()\n",
    "\n",
    "# Calculate the expectation\n",
    "expectation = discretizer.expectation(probs)\n",
    "\n",
    "# Check if the expectation is close to the mean of the test array\n",
    "expected_mean = np.tile(np.mean(test_arr, axis=0), (3, 1))\n",
    "assert np.isclose(\n",
    "    expectation, expected_mean, atol=1e-1\n",
    ").all(), f\"Expectation {expectation} is not close to the expected mean {expected_mean}\"\n",
    "\n",
    "print(\"All tests passed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed successfully.\n"
     ]
    }
   ],
   "source": [
    "def flatten_space(s_dict: Any, space: spaces.Space) -> np.ndarray:\n",
    "    if isinstance(space, spaces.Discrete):\n",
    "        return s_dict\n",
    "    elif isinstance(space, spaces.Box):\n",
    "        return s_dict\n",
    "    elif isinstance(space, spaces.Dict):\n",
    "        return np.concatenate([s_dict[k] for k in space.spaces.keys()], axis=-1)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported observation space\")\n",
    "\n",
    "\n",
    "def unflatten_space(s_flat: np.ndarray, space: spaces.Space) -> dict:\n",
    "    if isinstance(space, spaces.Discrete):\n",
    "        return s_flat\n",
    "    elif isinstance(space, spaces.Box):\n",
    "        return s_flat\n",
    "    elif isinstance(space, spaces.Dict):\n",
    "        s_dict = {}\n",
    "        start = 0\n",
    "        for k, v in space.spaces.items():\n",
    "            end = start + get_space_dim(v)\n",
    "            s_dict[k] = s_flat[:, start:end]\n",
    "            start = end\n",
    "        return s_dict\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported observation space\")\n",
    "\n",
    "\n",
    "# Test the flatten_space_dict and unflatten_space_dict functions\n",
    "test_dict = {\"obs\": np.array([[1, 2, 3], [4, 5, 6]]), \"act\": np.array([[0], [1]])}\n",
    "test_space = spaces.Dict(\n",
    "    {\"obs\": spaces.Box(low=0, high=10, shape=(3,)), \"act\": spaces.Discrete(2)}\n",
    ")\n",
    "test_flat = flatten_space(test_dict, test_space)\n",
    "test_unflat = unflatten_space(test_flat, test_space)\n",
    "\n",
    "assert np.isclose(\n",
    "    test_flat, np.array([[0, 1, 2, 3], [1, 4, 5, 6]])\n",
    ").all(), f\"Flattened array {test_flat} is not as expected.\"\n",
    "assert np.isclose(\n",
    "    test_unflat[\"obs\"], test_dict[\"obs\"]\n",
    ").all(), f\"Unflattened observation {test_unflat['obs']} is not as expected.\"\n",
    "assert np.isclose(\n",
    "    test_unflat[\"act\"], test_dict[\"act\"]\n",
    ").all(), f\"Unflattened action {test_unflat['act']} is not as expected.\"\n",
    "\n",
    "# test discrete space\n",
    "test_dict = np.array([[0], [1]])\n",
    "test_space = spaces.Discrete(2)\n",
    "test_flat = flatten_space(test_dict, test_space)\n",
    "test_unflat = unflatten_space(test_flat, test_space)\n",
    "\n",
    "assert np.isclose(\n",
    "    test_flat, test_dict\n",
    ").all(), f\"Flattened array {test_flat} is not as expected.\"\n",
    "assert np.isclose(\n",
    "    test_unflat, test_dict\n",
    ").all(), f\"Unflattened array {test_unflat} is not as expected.\"\n",
    "\n",
    "# test box space\n",
    "test_dict = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "test_space = spaces.Box(low=0, high=10, shape=(3,))\n",
    "test_flat = flatten_space(test_dict, test_space)\n",
    "test_unflat = unflatten_space(test_flat, test_space)\n",
    "\n",
    "assert np.isclose(\n",
    "    test_flat, test_dict\n",
    ").all(), f\"Flattened array {test_flat} is not as expected.\"\n",
    "assert np.isclose(\n",
    "    test_unflat, test_dict\n",
    ").all(), f\"Unflattened array {test_unflat} is not as expected.\"\n",
    "\n",
    "print(\"All tests passed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_trajectory(env: Env, episode: EpisodeData, discount: float = 0.99):\n",
    "    trajectory_len = episode.rewards.shape[0]\n",
    "    observations = episode.observations\n",
    "    actions = episode.actions\n",
    "    rewards = episode.rewards\n",
    "\n",
    "    values = np.zeros(trajectory_len)\n",
    "    # calculate rewards to go with discount\n",
    "    for i in range(trajectory_len - 1, -1, -1):\n",
    "        values[i] = rewards[i] + (\n",
    "            discount * values[i + 1] if i + 1 < trajectory_len else 0\n",
    "        )\n",
    "\n",
    "    # drop the last state because we don't have a reward for it\n",
    "    states = flatten_space(observations, env.observation_space)\n",
    "    states = states[:-1, :].reshape(trajectory_len, -1)\n",
    "    actions = actions.reshape(trajectory_len, -1)\n",
    "    rewards = rewards.reshape(trajectory_len, -1)\n",
    "    values = values.reshape(trajectory_len, -1)\n",
    "\n",
    "    joined = np.concatenate([states, actions, rewards, values], axis=-1)\n",
    "\n",
    "    return joined\n",
    "\n",
    "\n",
    "class DiscretizeDataset(Dataset):\n",
    "    # Each input into the sequence model needs to be (batch_size, tokens)\n",
    "    # output should be in groups of transitions\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: Env,\n",
    "        m_dataset: MinariDataset,\n",
    "        n_transitions: int,\n",
    "        discount: float = 0.99,\n",
    "        max_bins: int = 1000,\n",
    "    ):\n",
    "        self.m_dataset = m_dataset\n",
    "        self.n_transitions = n_transitions\n",
    "\n",
    "        self.joined_trajectories = []\n",
    "        for episode in m_dataset:\n",
    "            self.joined_trajectories.append(join_trajectory(env, episode, discount))\n",
    "\n",
    "        self.discretizer = KBinsDiscretizer(\n",
    "            n_bins=max_bins,\n",
    "            strategy=\"quantile\",\n",
    "            dataset=np.concatenate(self.joined_trajectories, axis=0),\n",
    "        )\n",
    "\n",
    "        indices = []\n",
    "        for traj_idx, joined_trajectory in enumerate(self.joined_trajectories):\n",
    "            traj_len = joined_trajectory.shape[0]\n",
    "            end = traj_len - 1\n",
    "            for i in range(end):\n",
    "                indices.append((traj_idx, i, i + n_transitions))\n",
    "\n",
    "        self.indices = np.array(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        traj_idx, start, end = self.indices[idx]\n",
    "        # shape (start - end)\n",
    "        joined = self.joined_trajectories[traj_idx][start:end]\n",
    "        loss_pad_mask = np.ones(\n",
    "            (self.n_transitions, joined.shape[-1]), dtype=np.float32\n",
    "        )\n",
    "        if joined.shape[0] < self.n_transitions:\n",
    "            joined = np.pad(\n",
    "                joined,\n",
    "                ((0, self.n_transitions - joined.shape[0]), (0, 0)),\n",
    "                mode=\"constant\",\n",
    "                constant_values=0,\n",
    "            )\n",
    "            loss_pad_mask[joined.shape[0] :] = 0\n",
    "\n",
    "        joined_discretized = self.discretizer.encode(joined)\n",
    "        joined_discretized = joined_discretized.reshape(-1).astype(np.long)\n",
    "        loss_pad_mask = loss_pad_mask.reshape(-1)\n",
    "        return joined_discretized[:-1], joined_discretized[1:], loss_pad_mask[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 20503\n",
      "Shape of dataset: (119,), (119,)\n"
     ]
    }
   ],
   "source": [
    "dataset = DiscretizeDataset(\n",
    "    env=env,\n",
    "    m_dataset=m_dataset,\n",
    "    n_transitions=n_transitions,\n",
    "    discount=discount_factor,\n",
    "    max_bins=max_bins,\n",
    ")\n",
    "\n",
    "print(f\"Length of dataset: {len(dataset)}\")\n",
    "print(f\"Shape of dataset: {dataset[0][0].shape}, {dataset[0][1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, seq_len, embedding_dim: int, n_heads: int, attention_dropout: float, residual_dropout: float):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embedding_dim, n_heads, batch_first=True, dropout=attention_dropout)\n",
    "        self.attn_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.fc_norm = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.drop = nn.Dropout(residual_dropout)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embedding_dim * 2, embedding_dim),\n",
    "            nn.Dropout(residual_dropout),\n",
    "        )\n",
    "\n",
    "        # mask value of true means that the value is not allowed to be attended to\n",
    "        self.register_buffer(\"mask\", ~torch.tril(torch.ones(seq_len, seq_len)).bool())\n",
    "        # transition_dim - 1 stores rewards to go, we don't want to attend to them because they contain future information\n",
    "        self.mask[:, transition_dim - 1 :: transition_dim] = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x shape (batch_size, n_tokens, embedding_dim)\n",
    "        _, n_tokens, _ = x.shape\n",
    "\n",
    "        # normalize the input before passing it to the attention layer\n",
    "        x = self.attn_norm(x)\n",
    "\n",
    "        # attn_mask shape (seq_len, seq_len), but incoming shape is (batch_size, n_tokens, embedding_dim)\n",
    "        # so filter the mask to the correct size (n_tokens, n_tokens)\n",
    "        attn_mask = self.mask[:n_tokens, :n_tokens]\n",
    "        # attn_output shape (batch_size, n_tokens, embedding_dim)\n",
    "        attn_output, _ = self.attn(x, x, x, attn_mask=attn_mask, need_weights=False)\n",
    "        attn_output = self.drop(attn_output)\n",
    "\n",
    "        # attn_output shape (batch_size, n_tokens, embedding_dim)\n",
    "        attn_output = attn_output + x\n",
    "\n",
    "        # normalize the output before passing it to the feed forward layer\n",
    "        attn_output = self.fc_norm(attn_output)\n",
    "\n",
    "        # fc_output shape (batch_size, n_tokens, embedding_dim)\n",
    "        fc_output = self.mlp(attn_output)\n",
    "        fc_output = (fc_output + attn_output)\n",
    "\n",
    "        return fc_output\n",
    "\n",
    "\n",
    "class TrajectoryTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_len: int,\n",
    "        embedding_dim: int,\n",
    "        n_heads: int,\n",
    "        transition_dim: int,\n",
    "        n_blocks: int,\n",
    "        vocab_size: int,\n",
    "        dropout_embedding: float = 0.1,\n",
    "        attention_dropout: float = 0.1,\n",
    "        residual_dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.transition_dim = transition_dim\n",
    "        self.n_blocks = n_blocks\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.token_embedding = nn.Embedding(seq_len * vocab_size, self.embedding_dim)\n",
    "        self.positional_embedding = nn.Parameter(\n",
    "            torch.zeros(1, seq_len, self.embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout_embedding = nn.Dropout(dropout_embedding)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(self.seq_len, self.embedding_dim, self.n_heads, attention_dropout, residual_dropout)\n",
    "                for _ in range(self.n_blocks)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(self.embedding_dim, vocab_size)\n",
    "\n",
    "    def get_seq_len(self):\n",
    "        return self.seq_len\n",
    "\n",
    "    def _offset_tokens(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        _, n_tokens = tokens.shape\n",
    "        n_transition = np.ceil(n_tokens / self.transition_dim).astype(int)\n",
    "\n",
    "        offsets = (\n",
    "            torch.arange(self.transition_dim, device=tokens.device) * self.vocab_size\n",
    "        )\n",
    "        # repeat the offset n_transition times\n",
    "        offsets = offsets.repeat(n_transition)\n",
    "        offset_idx = offsets[:n_tokens] + tokens\n",
    "        return offset_idx\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor) -> torch.Tensor:\n",
    "        # tokens shape (batch_size, n_tokens)\n",
    "        _, n_tokens = tokens.shape\n",
    "        assert (\n",
    "            n_tokens <= self.seq_len\n",
    "        ), f\"n_tokens {n_tokens} is greater than seq_len {self.seq_len}\"\n",
    "\n",
    "        # project each token into their vocab space, this is similar to tokenization\n",
    "        # (batch_size, n_tokens)\n",
    "        offset_idx = self._offset_tokens(tokens)\n",
    "\n",
    "        # (batch_size, n_tokens, embedding_dim)\n",
    "        tokens = self.token_embedding(offset_idx)\n",
    "        positional_embedding = self.positional_embedding[:, :n_tokens]\n",
    "        tokens += positional_embedding\n",
    "\n",
    "        # (batch_size, n_tokens, embedding_dim)\n",
    "        tokens = self.dropout_embedding(tokens)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            tokens = block(tokens)\n",
    "        # (batch_size, n_tokens, embedding_dim) -> (batch_size, n_tokens, vocab_size)\n",
    "        logits = self.fc(tokens)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_token_from_logits(\n",
    "    logits: torch.Tensor,\n",
    "    temperature: float = 1.0,\n",
    "    greedy: bool = False,\n",
    "    top_k: Optional[int] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Sample the next token from the logits using temperature scaling and top-k sampling.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): The model's predicted logits.\n",
    "        temperature (float): The temperature scaling factor.\n",
    "        greedy (bool): Whether to sample greedily.\n",
    "        top_k (Optional[int]): The top-k sampling parameter.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The sampled token.\n",
    "    \"\"\"\n",
    "    # Apply temperature scaling\n",
    "    if temperature != 1.0:\n",
    "        logits = logits / temperature\n",
    "\n",
    "    # Apply top-k sampling\n",
    "    if top_k is not None:\n",
    "        # Apply top-k sampling\n",
    "        # (batch_size, vocab_size) -> (batch_size, top_k)\n",
    "        v, indices = torch.topk(logits, top_k, dim=-1)\n",
    "        # set all logits to -inf except the top-k indices\n",
    "        logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # Sample from the top-k indices\n",
    "        # (batch_size, top_k) -> (batch_size, 1)\n",
    "        idx = torch.multinomial(probs, num_samples=1)\n",
    "    else:\n",
    "        # Sample from the logits\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        if not greedy:\n",
    "            idx = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            # Greedy sampling\n",
    "            _, idx = torch.max(probs, dim=-1)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def round_to_multiple(number, multiple):\n",
    "    pad = (multiple - number % multiple) % multiple\n",
    "    return number + pad\n",
    "\n",
    "\n",
    "def sample_tokens(\n",
    "    model: nn.Module,\n",
    "    context: nn.Module,\n",
    "    n_steps: int,\n",
    "    temperature: float = 1.0,\n",
    "    greedy: bool = False,\n",
    "    top_k: Optional[int] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Sample a sequence of tokens from the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to sample from.\n",
    "        context (nn.Module): The context to condition the sampling on.\n",
    "        n_steps (int): The number of steps to sample.\n",
    "        temperature (float): The temperature scaling factor.\n",
    "        greedy (bool): Whether to sample greedily.\n",
    "        top_k (Optional[int]): The top-k sampling parameter.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The sampled tokens.\n",
    "    \"\"\"\n",
    "    raw_logits = torch.zeros(\n",
    "        context.shape[0], n_steps, vocab_size, device=context.device\n",
    "    )\n",
    "    for i in range(n_steps):\n",
    "        # crop the context so that it doesn't exceed the seq_len\n",
    "        curr_context_len = context.shape[1]\n",
    "        n_crop = round_to_multiple(\n",
    "            max(0, curr_context_len - model.get_seq_len()), transition_dim\n",
    "        )\n",
    "        context = context[:, n_crop:]\n",
    "        # Get the model's prediction\n",
    "        # (batch_size, seq_len, vocab_size)\n",
    "        logits = model(context)\n",
    "        # Sample the next token\n",
    "        # (batch_size, 1)\n",
    "        token = sample_token_from_logits(\n",
    "            logits[:, -1], temperature=temperature, greedy=greedy, top_k=top_k\n",
    "        )\n",
    "\n",
    "        context = torch.cat([context, token], dim=-1)\n",
    "\n",
    "        raw_logits[:, i] = logits[:, -1]\n",
    "    return context, raw_logits\n",
    "\n",
    "\n",
    "def beam_plan(\n",
    "    model: nn.Module,\n",
    "    discretizer: KBinsDiscretizer,\n",
    "    context: torch.Tensor,\n",
    "    beam_width: int,\n",
    "    beam_steps: int,\n",
    "    beam_context: int,\n",
    "    sample_expansion: int,\n",
    "    observation_dim: int,\n",
    "    action_dim: int,\n",
    "    reward_dim: int,\n",
    "    value_dim: int,\n",
    "    transition_dim: int,\n",
    "    obs_top_k: Optional[int] = None,\n",
    "    act_top_k: Optional[int] = None,\n",
    "    rew_top_k: Optional[int] = None,\n",
    "    temperature: float = 1.0,\n",
    "    greedy: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    tokens_context_size = beam_context * transition_dim\n",
    "    n_crop = round_to_multiple(\n",
    "        max(0, context.shape[1] - tokens_context_size), transition_dim\n",
    "    )\n",
    "    context = context[:, n_crop:]\n",
    "    # context shape (seq_len) -> (beam_width, seq_len)\n",
    "    plan = context.repeat(beam_width, 1)\n",
    "\n",
    "    rewards = torch.zeros(beam_width, beam_steps + 1, device=context.device)\n",
    "    discounts = discount_factor ** torch.arange(beam_steps + 1, device=context.device)\n",
    "\n",
    "    for t in trange(beam_steps, desc=\"Beam Search\", leave=False):\n",
    "        # (beam_width, n_tokens) -> (beam_width * sample_expansion, n_tokens)\n",
    "        plan = plan.repeat(sample_expansion, 1)\n",
    "        rewards = rewards.repeat(sample_expansion, 1)\n",
    "\n",
    "        # sample actions\n",
    "        plan, _ = sample_tokens(\n",
    "            model,\n",
    "            plan,\n",
    "            n_steps=action_dim,\n",
    "            top_k=act_top_k,\n",
    "            temperature=temperature,\n",
    "            greedy=greedy,\n",
    "        )\n",
    "\n",
    "        # sample rewards and values\n",
    "        # plan (beam_width * sample_expansion, n_tokens) -> (beam_width * sample_expansion, n_tokens + reward_dim + value_dim)\n",
    "        # logits shape (beam_width * sample_expansion, reward_dim + value_dim, vocab_size)\n",
    "        plan, logits = sample_tokens(\n",
    "            model,\n",
    "            plan,\n",
    "            n_steps=reward_dim + value_dim,\n",
    "            top_k=rew_top_k,\n",
    "            temperature=temperature,\n",
    "            greedy=greedy,\n",
    "        )\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        rewards_and_values = discretizer.expectation(\n",
    "            probs, subslice=(transition_dim - 2, transition_dim)\n",
    "        )\n",
    "\n",
    "        rewards[:, t : t + 2] = rewards_and_values\n",
    "        # Did you notice that rewards contains rewards at t and values at t+1, why?\n",
    "        #   when we want to calculate value (rewards to go) at t, we need to consider discounted rewards from 0 to t\n",
    "        #   and also future discounted rewards from t+1 to end. It is a bit awkward to apply discount factor to value (at t+1)\n",
    "        #   because predicted value is already discounted.\n",
    "        # (beam_width * sample_expansion, beam_steps + 1) * (beam_steps + 1) -> (beam_width * sample_expansion)\n",
    "        values = (rewards * discounts).sum(dim=-1)\n",
    "\n",
    "        # select top-k sequences\n",
    "        # (beam_width * sample_expansion) -> (beam_width)\n",
    "        values, idx = torch.topk(values, beam_width)\n",
    "\n",
    "        plan, rewards = plan[idx], rewards[idx]\n",
    "\n",
    "        if t < beam_steps - 1:\n",
    "            # sample observations only if we are not at the last step, why?\n",
    "            # because beam plan has to end with a valid transition [...., obs, act, rew, val]\n",
    "            plan, _ = sample_tokens(\n",
    "                model,\n",
    "                plan,\n",
    "                n_steps=observation_dim,\n",
    "                top_k=obs_top_k,\n",
    "                temperature=temperature,\n",
    "                greedy=greedy,\n",
    "            )\n",
    "\n",
    "    best_idx = torch.argmax(values)\n",
    "    # only return the best plan without the context\n",
    "    best_plan = plan[best_idx, context.shape[1] :]\n",
    "\n",
    "    return best_plan\n",
    "\n",
    "\n",
    "def rollout(\n",
    "    model: nn.Module,\n",
    "    env: Env,\n",
    "    discretizer: KBinsDiscretizer,\n",
    "    beam_width: int,\n",
    "    beam_steps: int,\n",
    "    beam_context: int,\n",
    "    sample_expansion: int,\n",
    "    observation_dim: int,\n",
    "    action_dim: int,\n",
    "    reward_dim: int,\n",
    "    value_dim: int,\n",
    "    transition_dim: int,\n",
    "    max_steps: int,\n",
    "    plan_every: int,\n",
    "    obs_top_k: Optional[int] = None,\n",
    "    act_top_k: Optional[int] = None,\n",
    "    rew_top_k: Optional[int] = None,\n",
    "    temperature: float = 1.0,\n",
    "    greedy: bool = False,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    "    generate_gif: bool = False,\n",
    "):\n",
    "    trajectory = []\n",
    "    assert (\n",
    "        plan_every <= beam_steps\n",
    "    ), f\"plan_every {plan_every} should be less than or equal to beam_steps {beam_steps}\"\n",
    "    obs, _ = env.reset()\n",
    "    imgs = []\n",
    "    if generate_gif:\n",
    "        imgs.append(env.render())\n",
    "    obs = flatten_space(obs, env.observation_space)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    context = torch.zeros(\n",
    "        (1, (max_steps + 1) * transition_dim), device=device, dtype=torch.long\n",
    "    )\n",
    "\n",
    "    context_idx = 0\n",
    "\n",
    "    # discretize the observation\n",
    "    obs_token = discretizer.encode(\n",
    "        np.array([obs]), subslice=(0, observation_dim)\n",
    "    ).squeeze()\n",
    "\n",
    "    context[:, :observation_dim] = torch.tensor(obs_token, device=device)\n",
    "\n",
    "    for t in trange(max_steps, desc=\"Rollout\", leave=False):\n",
    "        if t % plan_every == 0:\n",
    "            # we need to plan a new trajectory, reset the context observation at t step\n",
    "            context_idx = (\n",
    "                ((t + 1) * transition_dim) - action_dim - reward_dim - value_dim\n",
    "            )\n",
    "            predicted_tokens = beam_plan(\n",
    "                model,\n",
    "                discretizer,\n",
    "                context[:, :context_idx],\n",
    "                beam_width,\n",
    "                beam_steps,\n",
    "                beam_context,\n",
    "                sample_expansion,\n",
    "                observation_dim,\n",
    "                action_dim,\n",
    "                reward_dim,\n",
    "                value_dim,\n",
    "                transition_dim,\n",
    "                obs_top_k=obs_top_k,\n",
    "                act_top_k=act_top_k,\n",
    "                rew_top_k=rew_top_k,\n",
    "                temperature=temperature,\n",
    "                greedy=greedy,\n",
    "            )\n",
    "        else:\n",
    "            predicted_tokens = predicted_tokens[transition_dim:]\n",
    "\n",
    "        # get the action from the predicted tokens\n",
    "        action_token = predicted_tokens[:action_dim].cpu().numpy()\n",
    "        # decode the action\n",
    "        action = discretizer.decode(\n",
    "            action_token, subslice=(observation_dim, observation_dim + action_dim)\n",
    "        ).squeeze()\n",
    "        action = unflatten_space(action, env.action_space)\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        next_obs = flatten_space(next_obs, env.observation_space)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        if generate_gif:\n",
    "            imgs.append(env.render())\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        # discretize the next observation\n",
    "        next_obs_token = discretizer.encode(next_obs, subslice=(0, observation_dim))\n",
    "        # discretize the reward and value\n",
    "        reward_value_tokens = discretizer.encode(\n",
    "            np.array([reward, 0]), subslice=(transition_dim - 2, transition_dim)\n",
    "        )\n",
    "\n",
    "        # update the context\n",
    "        context_idx = t * transition_dim\n",
    "        # add action\n",
    "        context[\n",
    "            :,\n",
    "            context_idx + observation_dim : context_idx + observation_dim + action_dim,\n",
    "        ] = torch.as_tensor(action_token, device=device)\n",
    "        # add reward and value\n",
    "        context[\n",
    "            :, context_idx + observation_dim + action_dim : context_idx + transition_dim\n",
    "        ] = torch.as_tensor(reward_value_tokens, device=device)\n",
    "        # add next observation\n",
    "        context[\n",
    "            :,\n",
    "            context_idx\n",
    "            + transition_dim : context_idx\n",
    "            + transition_dim\n",
    "            + observation_dim,\n",
    "        ] = torch.as_tensor(next_obs_token, device=device)\n",
    "\n",
    "        trajectory.append((obs, next_obs, action, reward, done))\n",
    "        obs = next_obs\n",
    "\n",
    "    if generate_gif:\n",
    "        return total_reward, trajectory, imgs, terminated\n",
    "    return total_reward, trajectory, _, terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Training model from scratch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafd97e7480c4f6db2a574e5b700de01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224d474c818a49c986cc0aac36b1c943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33afa0ea9dd44939dc0bcd3283bae07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating episode:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6feafdf8771d41bf975873b0049fd7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rollout:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2ce51d8dd34fad88ca201933134409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e04f0568cd9402692ce4c76cfaba9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint to data/D4RL/pointmaze/medium-dense-v2/\n"
     ]
    }
   ],
   "source": [
    "def calculate_loss(\n",
    "    model: nn.Module,\n",
    "    batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
    "    vocab_size: int,\n",
    "    transition_dim: int,\n",
    "    observation_dim: int,\n",
    "    action_dim: int,\n",
    "    reward_dim: int,\n",
    "    value_dim: int,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    ") -> torch.Tensor:\n",
    "    # inputs shape (batch_size, seq_len)\n",
    "    # targets shape (batch_size, seq_len)\n",
    "    # loss_pad_mask shape (batch_size, seq_len)\n",
    "    inputs, targets, loss_pad_mask = batch\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    loss_pad_mask = loss_pad_mask.to(device)\n",
    "    # logits shape (batch_size, seq_len, vocab_size)\n",
    "    logits = model(inputs)\n",
    "    # flatten the logits and targets to shape (batch_size * seq_len, vocab_size)\n",
    "    logits = logits.view(-1, vocab_size)\n",
    "    # flatten the targets to shape (batch_size * seq_len)\n",
    "    targets = targets.view(-1)\n",
    "    # loss shape (batch_size * seq_len)\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    n_states = int(np.ceil(inputs.shape[1] / transition_dim))\n",
    "    weights = torch.cat(\n",
    "        [\n",
    "            torch.ones(observation_dim, device=inputs.device),\n",
    "            torch.ones(action_dim, device=inputs.device) * 5,\n",
    "            torch.ones(reward_dim, device=inputs.device) * 1,\n",
    "            torch.ones(value_dim, device=inputs.device) * 1,\n",
    "        ]\n",
    "    )\n",
    "    weights = weights.repeat(n_states)[1:].repeat(inputs.shape[0], 1)\n",
    "    loss = loss * weights.view(-1)\n",
    "    # apply the loss pad mask to the loss because we don't want to calculate the loss for padded values\n",
    "    loss = (loss * loss_pad_mask.view(-1)).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def eval(\n",
    "    env: Env,\n",
    "    model: nn.Module,\n",
    "    discretizer: KBinsDiscretizer,\n",
    "    num_episodes: int,\n",
    "    beam_width: int,\n",
    "    beam_steps: int,\n",
    "    beam_context: int,\n",
    "    sample_expansion: int,\n",
    "    observation_dim: int,\n",
    "    action_dim: int,\n",
    "    reward_dim: int,\n",
    "    value_dim: int,\n",
    "    transition_dim: int,\n",
    "    max_steps: int,\n",
    "    plan_every: int,\n",
    "    obs_top_k: Optional[int] = None,\n",
    "    act_top_k: Optional[int] = None,\n",
    "    rew_top_k: Optional[int] = None,\n",
    "    temperature: float = 1.0,\n",
    "    greedy: bool = False,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    total_rewards = []\n",
    "\n",
    "    dones = []\n",
    "    rollout_times = []\n",
    "    for _ in trange(num_episodes, desc=\"Evaluating episode\", leave=False):\n",
    "        start_time = time.time()\n",
    "        total_reward, _, _, done = rollout(\n",
    "            model,\n",
    "            env,\n",
    "            discretizer,\n",
    "            beam_width,\n",
    "            beam_steps,\n",
    "            beam_context,\n",
    "            sample_expansion,\n",
    "            observation_dim,\n",
    "            action_dim,\n",
    "            reward_dim,\n",
    "            value_dim,\n",
    "            transition_dim,\n",
    "            max_steps,\n",
    "            plan_every,\n",
    "            obs_top_k=obs_top_k,\n",
    "            act_top_k=act_top_k,\n",
    "            rew_top_k=rew_top_k,\n",
    "            temperature=temperature,\n",
    "            greedy=greedy,\n",
    "            device=device,\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        rollout_times.append(end_time - start_time)\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "        dones.append(done)\n",
    "\n",
    "    mean_rewards = np.mean(total_rewards)\n",
    "    std_rewards = np.std(total_rewards)\n",
    "\n",
    "    scores = [get_normalized_score(base_m_dataset, r) for r in total_rewards]\n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "\n",
    "    done_ratio = np.mean(dones)\n",
    "\n",
    "    mean_rollout_time = np.mean(rollout_times)\n",
    "    std_rollout_time = np.std(rollout_times)\n",
    "\n",
    "    model.train()\n",
    "    return mean_rewards, std_rewards, mean_score, std_score, done_ratio, mean_rollout_time, std_rollout_time\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    vocab_size: int,\n",
    "    n_epochs: int,\n",
    "    writer: SummaryWriter,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    "    eval_every: int = 10,\n",
    "    checkpoint_path: Optional[str] = None,\n",
    "):\n",
    "    model.train()\n",
    "    step = 0\n",
    "    for epoch in trange(n_epochs, desc=\"Training\"):\n",
    "        start_time = time.time()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(\n",
    "            dataloader, desc=f\"Epoch {epoch + 1}/{n_epochs}\", leave=False\n",
    "        ):\n",
    "            optimizer.zero_grad()\n",
    "            loss = calculate_loss(\n",
    "                model,\n",
    "                batch,\n",
    "                vocab_size,\n",
    "                device=device,\n",
    "                transition_dim=transition_dim,\n",
    "                observation_dim=observation_dim,\n",
    "                action_dim=action_dim,\n",
    "                reward_dim=reward_dim,\n",
    "                value_dim=value_dim,\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar(\"Loss/train\", loss.item(), step)\n",
    "            total_loss += loss.item()\n",
    "            step += 1\n",
    "        writer.add_scalar(\"Epoch\", epoch, epoch)\n",
    "        end_time = time.time()\n",
    "        writer.add_scalar(\"Time/train\", end_time - start_time, epoch)\n",
    "\n",
    "        if epoch % eval_every == 0:\n",
    "            start_time = time.time()\n",
    "            mean_rewards, std_rewards, mean_score, std_score, done_ratio, mean_rollout_time, std_rollout_time = eval(\n",
    "                env,\n",
    "                model,\n",
    "                dataset.discretizer,\n",
    "                num_episodes=10 if not local else 1,\n",
    "                beam_width=32 if not local else 2,\n",
    "                beam_steps=5 if not local else 3,\n",
    "                beam_context=5 if not local else 7,\n",
    "                sample_expansion=2 if not local else 1,\n",
    "                observation_dim=observation_dim,\n",
    "                action_dim=action_dim,\n",
    "                reward_dim=reward_dim,\n",
    "                value_dim=value_dim,\n",
    "                transition_dim=transition_dim,\n",
    "                max_steps=500 if not local else 4,\n",
    "                plan_every=2 if not local else 2,\n",
    "                obs_top_k=1,\n",
    "                act_top_k=20,\n",
    "                rew_top_k=None,\n",
    "                temperature=1.0,\n",
    "                greedy=False,\n",
    "                device=device,\n",
    "            )\n",
    "            writer.add_scalar(\"Reward/mean\", mean_rewards, epoch)\n",
    "            writer.add_scalar(\"Reward/std\", std_rewards, epoch)\n",
    "            writer.add_scalar(\"Score/mean\", mean_score, step)\n",
    "            writer.add_scalar(\"Score/std\", std_score, epoch)\n",
    "            writer.add_scalar(\"Done ratio\", done_ratio, epoch)\n",
    "            writer.add_scalar(\"Rollout time/mean\", mean_rollout_time, epoch)\n",
    "            writer.add_scalar(\"Rollout time/std\", std_rollout_time, epoch)\n",
    "\n",
    "            end_time = time.time()\n",
    "            writer.add_scalar(\"Time/eval\", end_time - start_time, epoch)\n",
    "\n",
    "        if checkpoint_path:\n",
    "            torch.save(model.state_dict(), checkpoint_path + \"model.pth\")\n",
    "            torch.save(optimizer.state_dict(), checkpoint_path + \"optimizer.pth\")\n",
    "\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "writer = SummaryWriter()\n",
    "dataset.discretizer.to(device)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=8 if not local else 0, pin_memory=True)\n",
    "model = TrajectoryTransformer(\n",
    "    seq_len, embedding_dim, n_heads, transition_dim, n_blocks, vocab_size\n",
    ").to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "if load_checkpoint and os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading model from {checkpoint_path}\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path + \"model.pth\", map_location=device))\n",
    "    optimizer.load_state_dict(torch.load(checkpoint_path + \"optimizer.pth\", map_location=device))\n",
    "else:\n",
    "    print(\"Training model from scratch\")\n",
    "    train(\n",
    "        model,\n",
    "        dataloader,\n",
    "        optimizer,\n",
    "        vocab_size,\n",
    "        n_epochs,\n",
    "        writer,\n",
    "        device=device,\n",
    "        eval_every=eval_every,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "    )\n",
    "    print(f\"Saved checkpoint to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9bb74ee04214e30b755b4193eb52d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rollout:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d7bf2a4fe64b06911846d672c5f09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fb7eb3b5754b688b19d0e4dc357b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede820b2734e48f08421d3a812dac1a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b578f3c3f7040e88d221e181bda3a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b298cf7bb454b66b1b9a73dc935dd1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92b7bf4d3064397a1bf6cdea5b94967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651353762d124f7899934f8436c0e76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0002f0c416604ff0a0a608b3820c84e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Beam Search:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m total_reward, trajectory, imgs, terminated \u001b[38;5;241m=\u001b[39m \u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscretizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscretizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlocal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlocal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeam_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlocal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_expansion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlocal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobservation_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreward_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransition_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransition_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplan_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobs_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mact_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrew_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerate_gif\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTerminated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mterminated\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 246\u001b[0m, in \u001b[0;36mrollout\u001b[0;34m(model, env, discretizer, beam_width, beam_steps, beam_context, sample_expansion, observation_dim, action_dim, reward_dim, value_dim, transition_dim, max_steps, plan_every, obs_top_k, act_top_k, rew_top_k, temperature, greedy, device, generate_gif)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m%\u001b[39m plan_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m# we need to plan a new trajectory, reset the context observation at t step\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     context_idx \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    244\u001b[0m         ((t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m transition_dim) \u001b[38;5;241m-\u001b[39m action_dim \u001b[38;5;241m-\u001b[39m reward_dim \u001b[38;5;241m-\u001b[39m value_dim\n\u001b[1;32m    245\u001b[0m     )\n\u001b[0;32m--> 246\u001b[0m     predicted_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mbeam_plan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiscretizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mcontext_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_width\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_expansion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreward_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransition_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobs_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobs_top_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mact_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mact_top_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrew_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrew_top_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgreedy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     predicted_tokens \u001b[38;5;241m=\u001b[39m predicted_tokens[transition_dim:]\n",
      "Cell \u001b[0;32mIn[9], line 176\u001b[0m, in \u001b[0;36mbeam_plan\u001b[0;34m(model, discretizer, context, beam_width, beam_steps, beam_context, sample_expansion, observation_dim, action_dim, reward_dim, value_dim, transition_dim, obs_top_k, act_top_k, rew_top_k, temperature, greedy)\u001b[0m\n\u001b[1;32m    171\u001b[0m     plan, rewards \u001b[38;5;241m=\u001b[39m plan[idx], rewards[idx]\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m<\u001b[39m beam_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;66;03m# sample observations only if we are not at the last step, why?\u001b[39;00m\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;66;03m# because beam plan has to end with a valid transition [...., obs, act, rew, val]\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m         plan, _ \u001b[38;5;241m=\u001b[39m \u001b[43msample_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobservation_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobs_top_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgreedy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m best_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(values)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# only return the best plan without the context\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 87\u001b[0m, in \u001b[0;36msample_tokens\u001b[0;34m(model, context, n_steps, temperature, greedy, top_k)\u001b[0m\n\u001b[1;32m     84\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(context)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Sample the next token\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# (batch_size, 1)\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[43msample_token_from_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgreedy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([context, token], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     93\u001b[0m raw_logits[:, i] \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[9], line 30\u001b[0m, in \u001b[0;36msample_token_from_logits\u001b[0;34m(logits, temperature, greedy, top_k)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# set all logits to -inf except the top-k indices\u001b[39;00m\n\u001b[1;32m     29\u001b[0m logits[logits \u001b[38;5;241m<\u001b[39m v[:, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Sample from the top-k indices\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# (batch_size, top_k) -> (batch_size, 1)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/blog-FROQ9Grm-py3.11/lib/python3.11/site-packages/torch/nn/functional.py:1855\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         ret \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1852\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[0;32m-> 1855\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msoftmax\u001b[39m(\u001b[38;5;28minput\u001b[39m: Tensor, dim: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, _stacklevel: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, dtype: Optional[DType] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Apply a softmax function.\u001b[39;00m\n\u001b[1;32m   1857\u001b[0m \n\u001b[1;32m   1858\u001b[0m \u001b[38;5;124;03m    Softmax is defined as:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1878\u001b[0m \n\u001b[1;32m   1879\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_reward, trajectory, imgs, terminated = rollout(\n",
    "    model=model,\n",
    "    env=env,\n",
    "    discretizer=dataset.discretizer,\n",
    "    beam_width=32 if not local else 4,\n",
    "    beam_steps=5 if not local else 3,\n",
    "    beam_context=5 if not local else 7,\n",
    "    sample_expansion=2 if not local else 1,\n",
    "    observation_dim=observation_dim,\n",
    "    action_dim=action_dim,\n",
    "    reward_dim=reward_dim,\n",
    "    value_dim=value_dim,\n",
    "    transition_dim=transition_dim,\n",
    "    max_steps=400,\n",
    "    plan_every=1,\n",
    "    obs_top_k=1,\n",
    "    act_top_k=20,\n",
    "    rew_top_k=None,\n",
    "    temperature=1.0,\n",
    "    greedy=False,\n",
    "    device=device,\n",
    "    generate_gif=True,\n",
    ")\n",
    "print(f\"Total reward: {total_reward}\")\n",
    "print(f\"Terminated: {terminated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from IPython.display import Image\n",
    "\n",
    "# save the images as a gif\n",
    "imageio.mimsave(f\"{base_dir}/trajectory.gif\", imgs, fps=30)\n",
    "# display the gif and repeat it forever\n",
    "Image(filename=f\"{base_dir}/trajectory.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog-FROQ9Grm-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
