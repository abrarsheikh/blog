[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Blog",
    "section": "",
    "text": "About Me\nMy name is Abrar. I am a software engineer. You can find me on LinkedIn.\n\n\nPosts\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAttention Mechanisms: Memory-Efficient Techniques and Implementations\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nJan 12, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/head_attention.html",
    "href": "content/head_attention.html",
    "title": "Attention Mechanisms: Memory-Efficient Techniques and Implementations",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport math\nimport time\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nimport pandas as pd\nimport seaborn as sns"
  },
  {
    "objectID": "content/head_attention.html#multi-head-attention",
    "href": "content/head_attention.html#multi-head-attention",
    "title": "Attention Mechanisms: Memory-Efficient Techniques and Implementations",
    "section": "Multi-Head Attention",
    "text": "Multi-Head Attention\nMulti-Head Attention is a core component of transformer models that allows the model to focus on different parts of the input sequence simultaneously. It involves multiple attention heads, each computing scaled dot-product attention independently, followed by a concatenation of the results and a final linear transformation.\n\n\n\nMHA\n\n\n\n1. Input Transformation:\nFor an input sequence \\(X \\in \\mathbb{R}^{T \\times d_{\\text{model}}}\\), where \\(T\\) is the sequence length and \\(d_{\\text{model}}\\) is the model’s hidden size, the inputs are transformed into query, key, and value matrices:\n\\[\nQ = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n\\]\nHere:\n\n\\(W_Q, W_K, W_V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}\\) are learnable projection matrices.\n\\(Q, K, V \\in \\mathbb{R}^{T \\times d_k}\\)\n\n\n\n2. Scaled Dot-Product Attention:\nFor a single attention head, attention scores are computed as:\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right)V\n\\]\nHere:\n\n\\(Q K^\\top \\in \\mathbb{R}^{T \\times T}\\) represents the similarity scores between queries and keys.\n\\(\\frac{1}{\\sqrt{d_k}}\\) is a scaling factor to prevent large values in the softmax.\n\\(\\text{softmax}(\\cdot)\\) normalizes the scores across the sequence.\n\n\n\n3. Multi-Head Attention:\nSplitting \\(d_{\\text{model}}\\) into multiple heads in Multi-Head Attention allows the model to focus on different parts of the input sequence simultaneously. Each attention head operates on a subset of the dimensions (\\(d_k = d_{\\text{model}} / h\\)), enabling the model to capture diverse patterns or relationships in the data, such as local and global dependencies.\nThis parallel processing improves the expressiveness of the attention mechanism, allowing it to learn richer representations compared to a single attention head. By combining the outputs of all heads, the model integrates these diverse perspectives into a more comprehensive understanding of the input.\nInstead of using a single attention head, multiple heads are used. Each head has its own projection matrices \\(W_Q^i, W_K^i, W_V^i\\) and computes attention independently:\n\\[\n\\text{head}_i = \\text{Attention}(Q W_Q^i, K W_K^i, V W_V^i)\n\\]\nThe outputs of all heads are concatenated and projected back to \\(d_{\\text{model}}\\):\n\\[\n\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W_O\n\\]\nHere:\n\n\\(W_O \\in \\mathbb{R}^{(h \\cdot d_k) \\times d_{\\text{model}}}\\) is a projection matrix for combining all heads.\n\\(h\\) is the number of attention heads.\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, dim: int, n_heads: int):\n        super().__init__()\n        # Total embedding dimension\n        self.dim = dim\n        # Number of attention heads\n        self.n_heads = n_heads\n        # Dimension of each attention head\n        self.head_dim = dim // n_heads\n        # Query projection matrix: maps input to query vectors\n        self.wq = nn.Linear(dim, dim)\n        # Key projection matrix: maps input to key vectors\n        self.wk = nn.Linear(dim, dim)\n        # Value projection matrix: maps input to value vectors\n        self.wv = nn.Linear(dim, dim)\n        # Output projection matrix: combines and maps attention outputs back to model dimension\n        self.wo = nn.Linear(dim, dim)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Input shape: (batch_size, seq_len, dim)\n        batch_size, seq_len, _ = x.shape\n\n        # Linear projections - output shapes: (batch_size, seq_len, dim)\n        q = self.wq(x)\n        k = self.wk(x)\n        v = self.wv(x)\n\n        # Reshape to separate heads and transpose to get shape:\n        # (batch_size, n_heads, seq_len, head_dim)\n        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n\n        # Compute attention scores\n        # q @ k.T shape: (batch_size, n_heads, seq_len, seq_len)\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n\n        # Create causal mask of shape (seq_len, seq_len)\n        mask = torch.tril(torch.ones(seq_len, seq_len)).to(x.device)\n        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n        scores = scores.softmax(dim=-1)  # Normalize scores\n\n        # Apply attention to values\n        # output shape: (batch_size, n_heads, seq_len, head_dim)\n        output = torch.matmul(scores, v)\n\n        # Reshape back to (batch_size, seq_len, dim)\n        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n        return self.wo(output)  # Final linear projection"
  },
  {
    "objectID": "content/head_attention.html#multi-head-attention-with-kv-cache",
    "href": "content/head_attention.html#multi-head-attention-with-kv-cache",
    "title": "Attention Mechanisms: Memory-Efficient Techniques and Implementations",
    "section": "Multi-Head Attention with KV Cache",
    "text": "Multi-Head Attention with KV Cache\nMulti-Head Attention with KV Cache is an optimization technique used in transformer models to improve efficiency during autoregressive tasks, such as language generation. Instead of recomputing the key (\\(K\\)) and value (\\(V\\)) matrices for all tokens in the sequence at each step, previously computed \\(K\\) and \\(V\\) are cached and reused.\n\n\n\nKV Cache\n\n\n\nKey Differences and Modifications\n\nCaching Keys and Values\nIn standard Multi-Head Attention, \\(K\\) and \\(V\\) are recomputed for the entire input sequence. With KV caching, only the new token’s query (\\(Q\\)) is processed, while \\(K\\) and \\(V\\) from previous steps are stored and concatenated:\n\\[\nK_{\\text{cached}} = \\text{Concat}(K_{\\text{prev}}, K_{\\text{new}}), \\quad V_{\\text{cached}} = \\text{Concat}(V_{\\text{prev}}, V_{\\text{new}})\n\\]\nHere: - \\(K_{\\text{prev}}\\) and \\(V_{\\text{prev}}\\) are the cached keys and values from earlier time steps. - \\(K_{\\text{new}}\\) and \\(V_{\\text{new}}\\) are the keys and values for the current token.\n\n\nComputing Attention\nThe attention mechanism remains the same but operates over the concatenated cached keys and values:\n\\[\n\\text{Attention}(Q, K_{\\text{cached}}, V_{\\text{cached}}) = \\text{softmax}\\left(\\frac{Q K_{\\text{cached}}^\\top}{\\sqrt{d_k}}\\right)V_{\\text{cached}}\n\\]\n\n\n\nEfficiency\n\nReduced Redundancy: By caching, recomputation of ( K ) and ( V ) for all tokens is avoided, significantly speeding up inference for long sequences.\nScalability: This makes the model more memory-efficient, especially for real-time or autoregressive tasks like text generation.\n\n\nclass MultiHeadAttentionKVCache(nn.Module):\n    def __init__(self, dim: int, n_heads: int):\n        super().__init__()\n        self.dim = dim\n        self.n_heads = n_heads\n        self.head_dim = dim // n_heads\n        self.wq = nn.Linear(dim, dim)\n        self.wk = nn.Linear(dim, dim)\n        self.wv = nn.Linear(dim, dim)\n        self.wo = nn.Linear(dim, dim)\n\n        self.register_buffer(\n            \"cache_k\",\n            torch.zeros(MAX_BATCH_SIZE, MAX_SEQ_LEN, self.n_heads, self.head_dim),\n        )\n        self.register_buffer(\n            \"cache_v\",\n            torch.zeros(MAX_BATCH_SIZE, MAX_SEQ_LEN, self.n_heads, self.head_dim),\n        )\n\n    def forward(self, x: torch.Tensor, start_pos: int) -&gt; torch.Tensor:\n        batch_size, seq_len, _ = x.shape\n        assert seq_len == 1, \"seq_len must be 1\"\n        q = self.wq(x)\n        k = self.wk(x)\n        v = self.wv(x)\n\n        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim)\n        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim)\n        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim)\n\n        # Store current k,v in cache at position start_pos\n        # k shape: (batch_size, 1, n_heads, head_dim)\n        # cache_k shape: (MAX_BATCH_SIZE, MAX_SEQ_LEN, n_heads, head_dim)\n        self.cache_k[:batch_size, start_pos : start_pos + seq_len, :, :] = k\n        self.cache_v[:batch_size, start_pos : start_pos + seq_len, :, :] = v\n\n        # Retrieve cached k,v up to current position for attention computation\n        # Retrieved k shape: (batch_size, start_pos+1, n_heads, head_dim)\n        # This allows attending to all previous tokens plus current token\n        k = self.cache_k[:batch_size, : start_pos + seq_len, :, :]\n        v = self.cache_v[:batch_size, : start_pos + seq_len, :, :]\n\n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        scores = scores.softmax(dim=-1)\n        output = torch.matmul(scores, v)\n        # Concatenate heads and apply output projection\n        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n        return self.wo(output)"
  },
  {
    "objectID": "content/head_attention.html#group-query-attention-with-kv-cache",
    "href": "content/head_attention.html#group-query-attention-with-kv-cache",
    "title": "Attention Mechanisms: Memory-Efficient Techniques and Implementations",
    "section": "Group Query Attention with KV Cache",
    "text": "Group Query Attention with KV Cache\nGroup Query Attention with KV Cache is an extension of multi-head attention where multiple query heads attend to the same cached key and value pairs. This allows the model to efficiently process key-value caches while reusing the same key-value heads across multiple queries, improving performance, especially in autoregressive tasks.\n\n\n\nGQA\n\n\n\nKey Differences and Modifications\n\nGrouping Query Heads\nInstead of having separate key-value heads for each query head, the model groups query heads that attend to the same cached key and value heads. The number of query heads per group is determined by dividing the total number of query heads by the number of key-value heads.\n\\[\n\\text{kv\\_per\\_head} = \\frac{n_{\\text{heads}}}{n_{\\text{kv\\_heads}}}\n\\]\n\n\nKey and Value Caching\nAs the attention mechanism processes input tokens, the keys (\\(K\\)) and values (\\(V\\)) for earlier tokens are cached to avoid recomputation in subsequent time steps. These cached keys and values are stored in buffers, which are updated at each new time step:\n\\[\n\\text{cache } K[:, \\text{start\\_pos}: \\text{start\\_pos}+1, :, :] = K\n\\] \\[\n\\text{cache } V[:, \\text{start\\_pos}: \\text{start\\_pos}+1, :, :] = V\n\\]\nThe cached keys and values are then used in the computation of attention scores, reducing the need for recalculating them for each new token.\n\n\nAttention Computation\nFor each query head in the group, attention is computed over the cached keys and values. The key and value matrices are repeated across multiple query heads (i.e., the \\(K\\) and \\(V\\) tensors are repeated \\(\\text{kv\\_per\\_head}\\) times) to account for multiple query heads attending to the same set of key-value heads:\n\\[\nK = K \\cdot \\text{repeat\\_interleave}(\\text{kv\\_per\\_head}, \\text{dim}=1)\n\\] \\[\nV = V \\cdot \\text{repeat\\_interleave}(\\text{kv\\_per\\_head}, \\text{dim}=1)\n\\]\n\n\nScaled Dot-Product Attention\nThe attention scores are computed as the dot product between the queries and the keys, followed by a scaling factor to normalize the scores:\n\\[\n\\text{scores} = \\frac{Q K^\\top}{\\sqrt{d_k}}\n\\]\nHere, \\(Q\\) is the query matrix, \\(K^\\top\\) is the transpose of the key matrix, and \\(d_k\\) is the dimensionality of the key vectors.\nThe attention scores are then normalized using softmax:\n\\[\n\\text{scores} = \\text{softmax}(\\text{scores}, \\text{dim}=-1)\n\\]\nThese attention scores are then used to weight the values:\n\\[\n\\text{output} = \\text{scores} \\cdot V\n\\]\n\nclass GroupQueryAttentionKVCache(nn.Module):\n    def __init__(self, dim: int, n_heads: int, n_kv_heads: int):\n        super().__init__()\n        self.dim = dim\n        self.n_heads = n_heads\n        self.n_kv_heads = n_kv_heads\n        self.kv_per_head = n_heads // n_kv_heads\n        assert (\n            self.kv_per_head * self.n_kv_heads == self.n_heads\n        ), \"n_kv_heads must be a divisor of n_heads\"\n        self.head_dim = dim // n_heads\n        assert self.head_dim * self.n_heads == dim, \"dim must be a multiple of n_heads\"\n        self.wq = nn.Linear(dim, dim)\n        self.wk = nn.Linear(dim, n_kv_heads * self.head_dim)\n        self.wv = nn.Linear(dim, n_kv_heads * self.head_dim)\n        self.wo = nn.Linear(dim, dim)\n\n        self.register_buffer(\n            \"cache_k\",\n            torch.zeros(MAX_BATCH_SIZE, MAX_SEQ_LEN, self.n_kv_heads, self.head_dim),\n        )\n        self.register_buffer(\n            \"cache_v\",\n            torch.zeros(MAX_BATCH_SIZE, MAX_SEQ_LEN, self.n_kv_heads, self.head_dim),\n        )\n\n    def forward(self, x: torch.Tensor, start_pos: int):\n        batch_size, seq_len, _ = x.shape\n        assert seq_len == 1, \"seq_len must be 1\"\n\n        q = self.wq(x)\n        k = self.wk(x)\n        v = self.wv(x)\n\n        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim)\n        k = k.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n        v = v.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n\n        self.cache_k[:batch_size, start_pos : start_pos + seq_len, :, :] = k\n        self.cache_v[:batch_size, start_pos : start_pos + seq_len, :, :] = v\n\n        k = self.cache_k[:batch_size, : start_pos + seq_len, :, :]\n        v = self.cache_v[:batch_size, : start_pos + seq_len, :, :]\n\n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n\n        # repeat each head of k, v self.kv_per_head times\n        k = k.repeat_interleave(self.kv_per_head, dim=1)\n        v = v.repeat_interleave(self.kv_per_head, dim=1)\n\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        scores = scores.softmax(dim=-1)\n        output = torch.matmul(scores, v)\n        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n        return self.wo(output)"
  },
  {
    "objectID": "content/head_attention.html#multi-head-latent-attention-mla",
    "href": "content/head_attention.html#multi-head-latent-attention-mla",
    "title": "Attention Mechanisms: Memory-Efficient Techniques and Implementations",
    "section": "Multi-Head Latent Attention (MLA)",
    "text": "Multi-Head Latent Attention (MLA)\nIn Multi-Head Latent Attention (MLA), a key technique is the low-rank joint compression of the key and value matrices to significantly reduce memory usage in the key-value (KV) cache. This technique is particularly useful for long sequences, where the KV cache size could become a bottleneck.\n\n\n\nMLA\n\n\n\n1. Low-Rank Key-Value Compression\nThe MLA approach starts by compressing the keys and values jointly into a smaller latent space, reducing the memory requirement for storing them. The key operations are:\n\nDown-Projection: The original key-value matrix \\(h_t\\) (size \\(d \\times n_{\\text{heads}}\\)) is projected down to a lower-dimensional latent space \\(c_{KV_t}\\) (size \\(d_c\\)) using the down-projection matrix \\(W_{D_{KV}}\\):\n\\[\nc_{KV_t} = W_{D_{KV}} h_t\n\\]\nWhere \\(W_{D_{KV}} \\in \\mathbb{R}^{d_c \\times d}\\) is the down-projection matrix, and \\(d_c\\) is the latent dimension, which is significantly smaller than the original dimensionality \\(d_{\\text{model}}\\).\nUp-Projection: After compression, the compressed keys and values are restored to their original dimensions for use in attention computation through the up-projection matrices \\(W_{U_K}\\) for keys and \\(W_{U_V}\\) for values:\n\\[\nk_{C_t} = W_{U_K} c_{KV_t}, \\quad v_{C_t} = W_{U_V} c_{KV_t}\n\\]\nHere, \\(W_{U_K} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_c}\\) and \\(W_{U_V} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_c}\\) are the up-projection matrices for keys and values, respectively.\n\nThe benefit of this low-rank joint compression is that it significantly reduces the memory footprint of the KV cache. Specifically, instead of storing the full-sized key and value matrices, we only store the compressed latent vectors \\(c_{KV_t}\\), which require much less space.\n\n\n2. Key-Value Cache Reduction\nDuring inference, the compressed vectors \\(c_{KV_t}\\) are cached, significantly reducing the memory required for storing keys and values across layers. The total size of the KV cache is proportional to \\(d_c\\) (the compressed latent dimension), which is much smaller than the original dimensionality.\nIn practical terms, the KV cache has only \\(d_c \\times l\\) elements (where \\(l\\) is the number of layers), instead of the full-size key-value matrices for each layer.\n\n\n3. Query Compression\nIn addition to compressing the keys and values, MLA also compresses the queries to further reduce the activation memory during training. This compression is done through similar down-projection and up-projection steps:\n\nDown-Projection of Queries: The queries \\(h_t\\) are projected down into a smaller latent vector \\(c_{Q_t}\\) using the down-projection matrix \\(W_{D_Q}\\):\n\\[\nc_{Q_t} = W_{D_Q} h_t\n\\]\nWhere \\(W_{D_Q} \\in \\mathbb{R}^{d'_c \\times d}\\) is the down-projection matrix, and \\(d'_c\\) is the query compression dimension.\nUp-Projection of Queries: The compressed query \\(c_{Q_t}\\) is restored to its original dimensions using the up-projection matrix \\(W_{U_Q}\\):\n\\[\nq_{C_t} = W_{U_Q} c_{Q_t}\n\\]\nWhere \\(W_{U_Q} \\in \\mathbb{R}^{d_{\\text{model}} \\times d'_c}\\) is the up-projection matrix for queries.\n\nThis query compression reduces the memory needed for the query representations, helping to alleviate memory pressure during training.\n\n\n4. Inference Optimization\nDuring inference, additional optimizations are possible. Specifically, the up-projection matrices for keys \\(W_{U_K}\\) and values \\(W_{U_V}\\) can be absorbed into the projection matrices \\(W_Q\\) and \\(W_O\\), respectively, for queries and outputs. This means that the keys and values do not need to be computed explicitly during inference, further reducing memory and computation costs.\n\n\nSummary of MLA’s Key Components:\n\nKey-Value Compression: Low-rank joint compression reduces memory usage for keys and values in the KV cache.\nQuery Compression: Compresses queries to reduce activation memory during training.\nInference Optimizations: Absorption of up-projection matrices into the query and output projections eliminates the need for recomputing keys and values.\nReduced KV Cache Size: Only the compressed latent vectors \\(c_{KV_t}\\) need to be cached, which significantly reduces memory usage.\n\n\n\nMathematical Notation Recap:\n\nCompressed Latent Vector for Keys and Values: \\(c_{KV_t} = W_{D_{KV}} h_t\\)\nUp-Projection of Keys and Values: \\(k_{C_t} = W_{U_K} c_{KV_t}, \\quad v_{C_t} = W_{U_V} c_{KV_t}\\)\nCompressed Latent Vector for Queries: \\(c_{Q_t} = W_{D_Q} h_t\\)\nUp-Projection of Queries: \\(q_{C_t} = W_{U_Q} c_{Q_t}\\)\n\nBy reducing the size of the KV cache and compressing the queries, MLA offers substantial memory savings while maintaining the performance of attention mechanisms.\nThere are two ways to implement MLA:\n\nDecompress KV: decompress the key and value matrices to the original dimension before storing them in the cache\nCompress KV: compress the key and value matrices to a lower dimension and store them in the cache\n\nWhy would we want to decompress the key and value matrices to the original dimension?\nAlthough decompressing the key and value matrices to the original dimension is memory inefficient, it’s generally faster to compute attention scores and apply them to the values. Vs in the compressed version we would have to recompute all the past key and value matrices from the compressed latent vectors before computing the attention scores.\n\nclass MultiHeadLatentAttentionDecompressKV(nn.Module):\n    def __init__(\n        self, dim: int, n_heads: int, q_compression_dim: int, kv_compression_dim: int\n    ):\n        super().__init__()\n        assert dim % n_heads == 0, \"dim must be divisible by n_heads\"\n        self.dim = dim\n        self.n_heads = n_heads\n        self.head_dim = dim // n_heads\n        self.kv_compression_dim = kv_compression_dim\n\n        # Query compression weights\n        # w_dq: Projects query from full dim to compressed dim (down projection)\n        self.w_dq = nn.Linear(dim, q_compression_dim)\n        # w_uq: Projects query back from compressed to full dim (up projection)\n        self.w_uq = nn.Linear(q_compression_dim, dim)\n\n        # Key-Value compression weights\n        # w_dkv: Projects both key and value to shared compressed dim\n        self.w_dkv = nn.Linear(dim, kv_compression_dim)\n        # w_uk, w_uv: Separate projections from compressed dim back to full dim\n        self.w_uk = nn.Linear(kv_compression_dim, dim)\n        self.w_uv = nn.Linear(kv_compression_dim, dim)\n\n        # Final output projection\n        self.w_o = nn.Linear(dim, dim, bias=False)\n\n        # KV Cache buffers to store previous key/value pairs\n        self.register_buffer(\n            \"cache_k\",\n            torch.zeros(MAX_BATCH_SIZE, MAX_SEQ_LEN, self.n_heads, self.head_dim),\n        )\n        self.register_buffer(\n            \"cache_v\",\n            torch.zeros(MAX_BATCH_SIZE, MAX_SEQ_LEN, self.n_heads, self.head_dim),\n        )\n\n    def forward(self, x: torch.Tensor, start_pos: int):\n        # Input shape: (batch_size, seq_len, dim)\n        batch_size, seq_len, _ = x.shape\n        assert (\n            seq_len == 1\n        ), \"Only single-step inference (seq_len=1) is supported for MLA.\"\n\n        # Project to compressed q dimension: (batch_size, seq_len, q_compression_dim)\n        c_q = self.w_dq(x)\n        # Project back to full dimension: (batch_size, seq_len, dim)\n        q = self.w_uq(c_q)\n\n        # Project to compressed kv dimension: (batch_size, seq_len, kv_compression_dim)\n        c_kv = self.w_dkv(x)\n\n        # Project back to full dimension: (batch_size, seq_len, dim)\n        k = self.w_uk(c_kv)\n        v = self.w_uv(c_kv)\n\n        # Reshape to split dim into n_heads and head_dim\n        # q shape: (batch_size, seq_len, n_heads, head_dim)\n        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim)\n        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim)\n        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim)\n\n        # Cache k,v for current position\n        self.cache_k[:batch_size, start_pos : start_pos + seq_len, :, :] = k\n        self.cache_v[:batch_size, start_pos : start_pos + seq_len, :, :] = v\n\n        # Get cached k,v up to current position\n        k = self.cache_k[:batch_size, : start_pos + seq_len, :, :]\n        v = self.cache_v[:batch_size, : start_pos + seq_len, :, :]\n\n        # Transpose to get shape (batch_size, n_heads, seq_len, head_dim)\n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n\n        # Compute attention scores: (batch_size, n_heads, seq_len, seq_len)\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        attention = F.softmax(scores, dim=-1)\n\n        # Apply attention: (batch_size, n_heads, seq_len, head_dim)\n        context = torch.matmul(attention, v)\n\n        # Reshape back to (batch_size, seq_len, dim)\n        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n        output = self.w_o(context)\n\n        return output\n\n\nclass MultiHeadLatentAttentionCompressKV(nn.Module):\n    def __init__(\n        self, dim: int, n_heads: int, q_compression_dim: int, kv_compression_dim: int\n    ):\n        super().__init__()\n        assert dim % n_heads == 0, \"dim must be divisible by n_heads\"\n        self.dim = dim\n        self.n_heads = n_heads\n        self.head_dim = dim // n_heads\n        self.kv_compression_dim = kv_compression_dim\n\n        # Down-project query from dim to compressed dimension\n        self.w_dq = nn.Linear(dim, q_compression_dim)\n        # Up-project query back to original dimension\n        self.w_uq = nn.Linear(q_compression_dim, dim)\n\n        # Down-project key-value to shared compressed dimension\n        self.w_dkv = nn.Linear(dim, kv_compression_dim)\n        # Separate up-projections for key and value back to original dimension\n        self.w_uk = nn.Linear(kv_compression_dim, dim)\n        self.w_uv = nn.Linear(kv_compression_dim, dim)\n\n        # Final output projection\n        self.w_o = nn.Linear(dim, dim, bias=False)\n\n        # Cache for storing compressed key-value pairs during generation\n        self.register_buffer(\n            \"cache_kv\", torch.zeros(MAX_BATCH_SIZE, MAX_SEQ_LEN, kv_compression_dim)\n        )\n\n    def forward(self, x: torch.Tensor, start_pos: int):\n        batch_size, seq_len, _ = x.shape\n        assert (\n            seq_len == 1\n        ), \"Only single-step inference (seq_len=1) is supported for MLA.\"\n\n        # Project to compressed q dimension: (batch_size, seq_len, q_compression_dim)\n        c_q = self.w_dq(x)\n        # Project back to full dimension: (batch_size, seq_len, dim)\n        q = self.w_uq(c_q)\n\n        # Project to compressed kv dimension: (batch_size, seq_len, kv_compression_dim)\n        c_kv = self.w_dkv(x)\n\n        # Cache compressed kv for current position\n        self.cache_kv[:batch_size, start_pos : start_pos + seq_len, :] = c_kv\n\n        # Retrieve cached kv up to current position\n        cached_kv = self.cache_kv[:batch_size, : start_pos + seq_len, :]\n\n        # Project back to full dimension: (batch_size, seq_len, dim)\n        k = self.w_uk(cached_kv)\n        v = self.w_uv(cached_kv)\n\n        # Reshape to get shape (batch_size, n_heads, seq_len, head_dim)\n        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n        # seq_len is not the same as the seq_len of the input which is 1, it is the seq_len of the cached kv\n        # hence we need to view it with -1 to get the correct shape\n        k = k.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n        v = v.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        attention = F.softmax(scores, dim=-1)\n\n        context = torch.matmul(attention, v)\n\n        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n        output = self.w_o(context)\n\n        return output"
  },
  {
    "objectID": "content/head_attention.html#memory-usage-comparison",
    "href": "content/head_attention.html#memory-usage-comparison",
    "title": "Attention Mechanisms: Memory-Efficient Techniques and Implementations",
    "section": "Memory Usage Comparison",
    "text": "Memory Usage Comparison\n\n1. Memory Usage in Multi-Head Attention\nIn Multi-Head Attention (MHA), for each query, separate key-value heads are used. This means for each query head, there is a corresponding set of keys and values. The size of the key and value matrices increases with the number of heads, leading to higher memory requirements.\nFor an input sequence of length \\(T\\), model dimension \\(d_{\\text{model}}\\), and number of attention heads \\(n_{\\text{heads}}\\), the memory required for storing keys and values is proportional to:\n\\[\n\\text{Memory}_{\\text{keys/values}} = T \\times d_k \\times n_{\\text{heads}}\n\\]\nwhere \\(d_k = \\frac{d_{\\text{model}}}{n_{\\text{heads}}}\\) is the dimension of each head.\nKey and Value Memory:\nIn MHA, each query head has its own unique key and value projections, so the memory for storing keys and values grows with \\(n_{\\text{heads}}\\), the number of heads.\n\n\n2. Memory Usage in Group Query Attention\nIn Group Query Attention, multiple query heads share the same key and value heads, significantly reducing the number of unique key-value matrices stored in memory. This grouping reduces the total memory needed to store the keys and values, as multiple query heads attend to the same key and value matrices.\nThe key-value heads are shared across query heads, and the number of key-value heads is determined by \\(n_{\\text{kv\\_heads}}\\). The memory for storing keys and values is now:\n\\[\n\\text{Memory}_{\\text{keys/values}} = T \\times d_k \\times n_{\\text{kv\\_heads}}\n\\]\nHere, \\(d_k\\) is the dimensionality of each key-value pair, and the memory required depends only on \\(n_{\\text{kv\\_heads}}\\), which is usually much smaller than the total number of heads in MHA.\nKey and Value Memory Reduction:\nSince the keys and values are repeated across multiple query heads, this leads to a reduction in memory usage. Specifically, the number of key-value heads \\(n_{\\text{kv\\_heads}}\\) is much smaller than the total number of heads \\(n_{\\text{heads}}\\) in MHA, leading to lower memory usage.\n\n\n3. Memory Usage in Multi-Query Attention (a Variation of Group Query Attention)\nMulti-Query Attention is a variation of Group Query Attention where all query heads attend to the same set of key-value heads. Instead of having multiple key-value heads, all query heads in Multi-Query Attention use the same set of keys and values. This leads to the most significant memory reduction, as there is only a single key-value matrix shared across all query heads.\nIn Multi-Query Attention, the number of key-value heads \\(n_{\\text{kv\\_heads}}\\) is equal to 1, as all query heads attend to the same key and value:\n\\[\n\\text{Memory}_{\\text{keys/values}} = T \\times d_k \\times 1\n\\]\nIn this case, there is only one key-value matrix to store, which drastically reduces memory usage compared to both MHA and Group Query Attention.\n\n\n4. Memory Usage in Multi-Head Latent Attention (MLA)\nMulti-Head Latent Attention (MLA) introduces an additional technique to further reduce memory usage by compressing the key and value matrices into a lower-dimensional latent space. This low-rank joint compression of keys and values helps in reducing the size of the key-value cache while maintaining the ability to compute attention effectively.\nIn MLA, the key-value matrices are compressed into a latent space of dimension \\(d_c\\), where \\(d_c\\) is much smaller than the original dimension \\(d_{\\text{model}}\\). The memory for storing compressed key-value matrices is proportional to:\n\\[\n\\text{Memory}_{\\text{keys/values}} = T \\times d_c\n\\]\n\n\nMemory Comparison\n\n\n\n\n\n\n\nAttention Mechanism\nMemory Usage for Keys/Values\n\n\n\n\nMulti-Head Attention\n\\(2 \\times T \\times d_k \\times n_{\\text{heads}}\\)\n\n\nGroup Query Attention\n\\(2 \\times T \\times d_k \\times n_{\\text{kv\\_heads}}\\)\n\n\nMulti-Query Attention\n\\(2 \\times T \\times d_k \\times 1\\)\n\n\nMulti-Head Latent Attention\n\\(2 \\times T \\times d_c\\)"
  },
  {
    "objectID": "content/head_attention.html#experiments",
    "href": "content/head_attention.html#experiments",
    "title": "Attention Mechanisms: Memory-Efficient Techniques and Implementations",
    "section": "Experiments",
    "text": "Experiments\nIn the following experiments, we will evaluate the performance and memory efficiency of different attention modules in an autoregressive generation setup. Using the profile_model function, we will measure key metrics such as memory usage, generation speed, and the number of trainable parameters. Each module will be tested with KV Cache enabled to optimize performance and reduce redundant computations.\nFor consistency and fairness, all models will share the same configuration: batch size, sequence length, token dimensions, and number of attention heads. The goal is not to assess the correctness or quality of the models but rather to validate whether their performance and memory usage align with theoretical expectations.\nBelow is the code for the generation loop and profiling. Through this analysis, we aim to understand the trade-offs each attention mechanism presents in terms of efficiency and scalability.\n\ndef generate(\n    model: nn.Module,\n    input_tokens: torch.Tensor,\n    max_length: int = 100,\n    use_kv_cache: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Generate a sequence of tokens using an autoregressive model.\n\n    This function performs autoregressive generation by repeatedly feeding the model's output\n    back as input to generate a sequence of tokens up to max_length.\n\n    Args:\n        model (nn.Module): The neural network model used for generation. Should accept input\n            tokens and return logits for next token prediction.\n        input_tokens (torch.Tensor): Initial input sequence of shape (batch_size, seq_len, dim).\n            This serves as the prompt/context for generation.\n        max_length (int, optional): Maximum number of new tokens to generate. Defaults to 100.\n        use_kv_cache (bool, optional): Whether to use key-value caching during generation.\n            If True, expects model to accept start_pos parameter and maintain its own cache.\n            If False, recomputes attention over full sequence each step. Defaults to False.\n\n    Returns:\n        torch.Tensor: Generated sequence of shape (batch_size, seq_len + max_length, dim),\n            containing the input tokens followed by max_length generated tokens.\n\n    Note:\n        - When use_kv_cache=True, the model should implement caching of key-value pairs\n          to avoid recomputing attention over the entire sequence each generation step.\n        - The model should output logits of shape (batch_size, seq_len, dim) where dim\n          matches the input token dimension.\n\n    This function is not a correct implementation of autoregressive generation,\n    it is just a simple implementation to test the attention mechanisms\n    \"\"\"\n    generated = input_tokens\n    for i in range(max_length):\n        with torch.no_grad():\n            if use_kv_cache:\n                next_token_logits = model(generated[:, i : i + 1], start_pos=i)\n            else:\n                next_token_logits = model(generated)\n        next_token = next_token_logits[:, -1:, :]\n        generated = torch.cat([generated, next_token], dim=1)\n\n    return generated\n\n\n# Function to profile a model\ndef profile_model(\n    model: nn.Module,\n    input_seq: torch.Tensor,\n    n_runs: int,\n    max_length: int = 64,\n    use_kv_cache: bool = False,\n    device: torch.device = torch.device(\"cpu\"),\n    **kwargs\n):\n    \"\"\"\n    Profile a model's performance and memory usage.\n    \"\"\"\n\n    total_time = 0\n    total_tokens = 0\n\n    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n    initial_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else get_cpu_memory_usage()\n\n\n    m_obj = model(**kwargs).to(device)\n    for _ in range(n_runs):\n        start_time = time.time()\n        _ = generate(\n            m_obj, input_seq, max_length=max_length, use_kv_cache=use_kv_cache\n        )\n        end_time = time.time()\n\n        total_time += end_time - start_time  # Accumulate time in seconds\n        total_tokens += max_length  # Accumulate tokens generated\n\n    # Measure memory usage after inference\n    final_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else get_cpu_memory_usage()\n    memory_usage = (final_memory - initial_memory) / 1024 / 1024  # Convert to MB\n    parameters = sum(p.numel() for p in m_obj.parameters())\n    # Compute tokens/sec\n    tokens_per_sec = total_tokens / total_time if total_time &gt; 0 else 0\n\n    return (\n        total_time,\n        memory_usage,\n        tokens_per_sec,\n        parameters,\n    )\n\n# get number of parameters\ndef get_num_params(model: nn.Module) -&gt; int:\n    return sum(p.numel() for p in model.parameters())\n\nimport psutil\nimport os\n\ndef get_cpu_memory_usage():\n    process = psutil.Process(os.getpid())\n    memory_usage = process.memory_info().rss  # Resident Set Size (RSS) in bytes\n    return memory_usage\n\n\n# Main evaluation loop\nbatch_size = MAX_BATCH_SIZE // 2\nseq_len = MAX_SEQ_LEN // 4\nmax_length = MAX_SEQ_LEN // 2\ndim = 2048\nn_heads = 16\ndevice = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda\")\n\ninput_seq = torch.randn(batch_size, seq_len, dim).to(device)\nn_runs = 5\n\nmodels = {\n    # \"MHA\": (MultiHeadAttention, False, {\"dim\": dim, \"n_heads\": n_heads}),\n    \"MHA KV Cache\": (MultiHeadAttentionKVCache, True, {\"dim\": dim, \"n_heads\": n_heads}),\n    \"GQA KV Cache\": (\n        GroupQueryAttentionKVCache,\n        True,\n        {\"dim\": dim, \"n_heads\": n_heads, \"n_kv_heads\": n_heads // 4},\n    ),\n    \"MQA KV Cache\": (\n        GroupQueryAttentionKVCache,\n        True,\n        {\"dim\": dim, \"n_heads\": n_heads, \"n_kv_heads\": 1},\n    ),\n    \"MLA KV Cache Decompress\": (\n        MultiHeadLatentAttentionDecompressKV,\n        True,\n        {\n            \"dim\": dim,\n            \"n_heads\": n_heads,\n            \"q_compression_dim\": dim // 32,\n            \"kv_compression_dim\": dim // 32,\n        },\n    ),\n    \"MLA KV Cache Compress\": (\n        MultiHeadLatentAttentionCompressKV,\n        True,\n        {\n            \"dim\": dim,\n            \"n_heads\": n_heads,\n            \"q_compression_dim\": dim // 32,\n            \"kv_compression_dim\": dim // 32,\n        },\n    ),\n}\n\nresults = {}\n\nfor model_name, (model, use_kv_cache, kwargs) in models.items():\n    print(f\"Profiling {model_name}...\")\n    total_time, memory_usage, tokens_per_sec, parameters = (\n        profile_model(\n            model,\n            input_seq,\n            n_runs,\n            max_length=max_length,\n            use_kv_cache=use_kv_cache,\n            device=device,\n            **kwargs,\n        )\n    )\n    results[model_name] = {\n        \"total_time\": total_time,\n        \"memory_usage\": memory_usage,\n        \"tokens_per_sec\": tokens_per_sec,\n        \"parameters\": parameters,\n    }\n    print(\n        f\"Model: {model_name}, Total Time: {total_time:.2f}s, Memory Usage: {memory_usage:.2f}MB, Parameters: {parameters}\"\n    )\n\n# Plot results\nresults_df = pd.DataFrame(\n    {\n        \"Model\": list(models.keys()),\n        \"Total Time (s)\": [results[model][\"total_time\"] for model in models],\n        \"Memory Usage (MB)\": [results[model][\"memory_usage\"] for model in models],\n        \"Parameters\": [results[model][\"parameters\"] for model in models],\n        \"Tokens/s\": [results[model][\"tokens_per_sec\"] for model in models],\n    }\n)\n\nProfiling MHA KV Cache...\nModel: MHA KV Cache, Total Time: 20.87s, Memory Usage: 1216.03MB, Parameters: 16785408\nProfiling GQA KV Cache...\nModel: GQA KV Cache, Total Time: 15.43s, Memory Usage: 488.02MB, Parameters: 10490880\nProfiling MQA KV Cache...\nModel: MQA KV Cache, Total Time: 14.10s, Memory Usage: 290.02MB, Parameters: 8917248\nProfiling MLA KV Cache Decompress...\nModel: MLA KV Cache Decompress, Total Time: 18.53s, Memory Usage: 1234.52MB, Parameters: 4855936\nProfiling MLA KV Cache Compress...\nModel: MLA KV Cache Compress, Total Time: 30.92s, Memory Usage: 226.52MB, Parameters: 4855936"
  },
  {
    "objectID": "content/head_attention.html#observations",
    "href": "content/head_attention.html#observations",
    "title": "Attention Mechanisms: Memory-Efficient Techniques and Implementations",
    "section": "Observations",
    "text": "Observations\n\n1. Number of Parameters\nThe number of parameters follows the order MHA &gt; GQA &gt; MQA &gt; MLA. This is because MHA maintains independent key-value projections for all heads, whereas MLA uses low-rank approximations.\n\n\n2. Memory Usage\n\nMHA: The highest memory usage, as the key-value cache stores projections for all heads independently.\n\nMLA with Decompressed KV: Comparable to MHA in memory usage because it also maintains decompressed key-value matrices.\n\nMLA with Compressed KV: The lowest memory usage because the key-value cache stores compressed latent vectors instead of full matrices.\n\nGQA and MQA:\n\nGQA uses less memory than MHA because multiple query heads share the same key-value heads.\n\nMQA further reduces memory usage by having all query heads share a single set of key-value projections.\n\n\n\n\n3. Throughput (Tokens/s)\n\nMLA with Decompressed KV achieves higher throughput than MLA with Compressed KV.\n\nThis happens because in MLA with Compressed KV, key-value matrices need to be recomputed from latent vectors for every token during attention computation, adding overhead.\n\nDespite this overhead, MLA remains efficient in memory and parameters, with the decompressed variant being the better choice for practical implementations, as demonstrated in the DeepSeek paper.\n\n\n\n4. MLA’s Advantages Over MHA\n\nMLA achieves significantly fewer parameters by using low-rank approximations like LoRA (Low-Rank Adaptation) for key and value projection matrices.\n\nWhile MLA reduces parameters, the DeepSeek paper claims its prediction quality is on par with or better than MHA. This makes MLA attractive for memory-constrained environments without sacrificing quality.\n\n\n\n5. Trade-offs Between Models\n\nMHA: Best prediction quality due to the highest expressiveness, enabled by the largest number of parameters.\n\nGQA and MQA:\n\nAs parameters decrease, throughput improves significantly, making these models ideal for scenarios where speed and resource efficiency are critical.\n\nHowever, a slight reduction in prediction quality might be expected in theory.\n\n\nMLA:\n\nStrikes a balance by offering reduced memory usage and parameters.\n\nCompressed MLA is memory-efficient, while decompressed MLA is more throughput-friendly.\n\n\n\n\n\nAdditional Consideration\nEach model aligns with specific use cases: - MHA: Suited for scenarios where prediction quality is paramount, regardless of resource cost.\n- MQA/GQA: Ideal for real-time systems with stringent latency requirements.\n- MLA: Best for memory-constrained environments, such as edge devices or applications with limited hardware resources.\n\nresults_df\n\n\n  \n    \n\n\n\n\n\n\nModel\nTotal Time (s)\nMemory Usage (MB)\nParameters\nTokens/s\n\n\n\n\n0\nMHA KV Cache\n20.869432\n1216.031250\n16785408\n245.334903\n\n\n1\nGQA KV Cache\n15.433480\n488.019531\n10490880\n331.746317\n\n\n2\nMQA KV Cache\n14.097093\n290.016602\n8917248\n363.195457\n\n\n3\nMLA KV Cache Decompress\n18.534862\n1234.524414\n4855936\n276.236215\n\n\n4\nMLA KV Cache Compress\n30.923788\n226.524414\n4855936\n165.568332\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Create a FacetGrid with 3 rows and 2 columns\nfig = plt.figure(figsize=(12, 12))\n\n# Define metrics to plot\nmetrics = [\n    \"Total Time (s)\",\n    \"Memory Usage (MB)\",\n    \"Parameters\",\n    \"Tokens/s\",\n]\n\n# Create facet grid for multiple plots\ng = sns.FacetGrid(\n    pd.melt(results_df, id_vars=[\"Model\"], value_vars=metrics),\n    col_wrap=2,\n    height=4,\n    aspect=1.5,\n    row=None,\n    col=\"variable\",\n    sharex=False,\n    sharey=False,\n)\n\n# Plot bars for each metric\ng.map_dataframe(\n    sns.barplot,\n    x=\"Model\",\n    y=\"value\",\n    hue=\"Model\",\n    palette=\"viridis\",\n)\n\n# Rotate x-axis labels for better readability\nfor ax in g.axes:\n    ax.tick_params(axis=\"x\", rotation=45)\n\n# Adjust layout to prevent overlapping\nplt.tight_layout()\nplt.show()\n\n&lt;Figure size 1200x1200 with 0 Axes&gt;"
  },
  {
    "objectID": "content/head_attention.html#references",
    "href": "content/head_attention.html#references",
    "title": "Attention Mechanisms: Memory-Efficient Techniques and Implementations",
    "section": "References",
    "text": "References\n\nhttps://arxiv.org/pdf/1706.03762 Transformer\nhttps://arxiv.org/pdf/2405.04434 MLA\nhttps://arxiv.org/pdf/2305.13245v3 GQA"
  }
]