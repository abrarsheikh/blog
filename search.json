[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Blog",
    "section": "",
    "text": "1 About Me\nMy name is Abrar. I am a software engineer. You can find me on LinkedIn.\n\n\n2 Posts\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nTrajectory Transformer (TT) from code\n\n\n\n\n\n\nMachine Learning\n\n\nReinforcement Learning\n\n\n\n\n\n\n\n\n\nFeb 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAttention Mechanisms: Memory-Efficient Techniques and Implementations\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\nJan 12, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/head_attention.html",
    "href": "content/head_attention.html",
    "title": "Attention Mechanisms: Memory-Efficient Techniques and Implementations",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport math\nimport time\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nimport pandas as pd\nimport seaborn as sns"
  },
  {
    "objectID": "content/head_attention.html#multi-head-attention",
    "href": "content/head_attention.html#multi-head-attention",
    "title": "Attention Mechanisms: Memory-Efficient Techniques and Implementations",
    "section": "1.1 Multi-Head Attention",
    "text": "1.1 Multi-Head Attention\nMulti-Head Attention is a core component of transformer models that allows the model to focus on different parts of the input sequence simultaneously. It involves multiple attention heads, each computing scaled dot-product attention independently, followed by a concatenation of the results and a final linear transformation.\n\n\n\nMHA\n\n\n\n1.1.0.1 1. Input Transformation:\nFor an input sequence \\(X \\in \\mathbb{R}^{T \\times d_{\\text{model}}}\\), where \\(T\\) is the sequence length and \\(d_{\\text{model}}\\) is the model’s hidden size, the inputs are transformed into query, key, and value matrices:\n\\[\nQ = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n\\]\nHere:\n\n\\(W_Q, W_K, W_V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}\\) are learnable projection matrices.\n\\(Q, K, V \\in \\mathbb{R}^{T \\times d_k}\\)\n\n\n\n1.1.0.2 2. Scaled Dot-Product Attention:\nFor a single attention head, attention scores are computed as:\n\\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right)V\n\\]\nHere:\n\n\\(Q K^\\top \\in \\mathbb{R}^{T \\times T}\\) represents the similarity scores between queries and keys.\n\\(\\frac{1}{\\sqrt{d_k}}\\) is a scaling factor to prevent large values in the softmax.\n\\(\\text{softmax}(\\cdot)\\) normalizes the scores across the sequence.\n\n\n\n1.1.0.3 3. Multi-Head Attention:\nSplitting \\(d_{\\text{model}}\\) into multiple heads in Multi-Head Attention allows the model to focus on different parts of the input sequence simultaneously. Each attention head operates on a subset of the dimensions (\\(d_k = d_{\\text{model}} / h\\)), enabling the model to capture diverse patterns or relationships in the data, such as local and global dependencies.\nThis parallel processing improves the expressiveness of the attention mechanism, allowing it to learn richer representations compared to a single attention head. By combining the outputs of all heads, the model integrates these diverse perspectives into a more comprehensive understanding of the input.\nInstead of using a single attention head, multiple heads are used. Each head has its own projection matrices \\(W_Q^i, W_K^i, W_V^i\\) and computes attention independently:\n\\[\n\\text{head}_i = \\text{Attention}(Q W_Q^i, K W_K^i, V W_V^i)\n\\]\nThe outputs of all heads are concatenated and projected back to \\(d_{\\text{model}}\\):\n\\[\n\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W_O\n\\]\nHere:\n\n\\(W_O \\in \\mathbb{R}^{(h \\cdot d_k) \\times d_{\\text{model}}}\\) is a projection matrix for combining all heads.\n\\(h\\) is the number of attention heads.\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, dim: int, n_heads: int):\n        super().__init__()\n        # Total embedding dimension\n        self.dim = dim\n        # Number of attention heads\n        self.n_heads = n_heads\n        # Dimension of each attention head\n        self.head_dim = dim // n_heads\n        # Query projection matrix: maps input to query vectors\n        self.wq = nn.Linear(dim, dim)\n        # Key projection matrix: maps input to key vectors\n        self.wk = nn.Linear(dim, dim)\n        # Value projection matrix: maps input to value vectors\n        self.wv = nn.Linear(dim, dim)\n        # Output projection matrix: combines and maps attention outputs back to model dimension\n        self.wo = nn.Linear(dim, dim)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        # Input shape: (batch_size, seq_len, dim)\n        batch_size, seq_len, _ = x.shape\n\n        # Linear projections - output shapes: (batch_size, seq_len, dim)\n        q = self.wq(x)\n        k = self.wk(x)\n        v = self.wv(x)\n\n        # Reshape to separate heads and transpose to get shape:\n        # (batch_size, n_heads, seq_len, head_dim)\n        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n\n        # Compute attention scores\n        # q @ k.T shape: (batch_size, n_heads, seq_len, seq_len)\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n\n        # Create causal mask of shape (seq_len, seq_len)\n        mask = torch.tril(torch.ones(seq_len, seq_len)).to(x.device)\n        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n        scores = scores.softmax(dim=-1)  # Normalize scores\n\n        # Apply attention to values\n        # output shape: (batch_size, n_heads, seq_len, head_dim)\n        output = torch.matmul(scores, v)\n\n        # Reshape back to (batch_size, seq_len, dim)\n        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n        return self.wo(output)  # Final linear projection"
  },
  {
    "objectID": "content/head_attention.html#multi-head-attention-with-kv-cache",
    "href": "content/head_attention.html#multi-head-attention-with-kv-cache",
    "title": "Attention Mechanisms: Memory-Efficient Techniques and Implementations",
    "section": "1.2 Multi-Head Attention with KV Cache",
    "text": "1.2 Multi-Head Attention with KV Cache\nMulti-Head Attention with KV Cache is an optimization technique used in transformer models to improve efficiency during autoregressive tasks, such as language generation. Instead of recomputing the key (\\(K\\)) and value (\\(V\\)) matrices for all tokens in the sequence at each step, previously computed \\(K\\) and \\(V\\) are cached and reused.\n\n\n\nKV Cache\n\n\n\n1.2.1 Key Differences and Modifications\n\n1.2.1.1 Caching Keys and Values\nIn standard Multi-Head Attention, \\(K\\) and \\(V\\) are recomputed for the entire input sequence. With KV caching, only the new token’s query (\\(Q\\)) is processed, while \\(K\\) and \\(V\\) from previous steps are stored and concatenated:\n\\[\nK_{\\text{cached}} = \\text{Concat}(K_{\\text{prev}}, K_{\\text{new}}), \\quad V_{\\text{cached}} = \\text{Concat}(V_{\\text{prev}}, V_{\\text{new}})\n\\]\nHere: - \\(K_{\\text{prev}}\\) and \\(V_{\\text{prev}}\\) are the cached keys and values from earlier time steps. - \\(K_{\\text{new}}\\) and \\(V_{\\text{new}}\\) are the keys and values for the current token.\n\n\n1.2.1.2 Computing Attention\nThe attention mechanism remains the same but operates over the concatenated cached keys and values:\n\\[\n\\text{Attention}(Q, K_{\\text{cached}}, V_{\\text{cached}}) = \\text{softmax}\\left(\\frac{Q K_{\\text{cached}}^\\top}{\\sqrt{d_k}}\\right)V_{\\text{cached}}\n\\]\n\n\n\n1.2.2 Efficiency\n\nReduced Redundancy: By caching, recomputation of ( K ) and ( V ) for all tokens is avoided, significantly speeding up inference for long sequences.\nScalability: This makes the model more memory-efficient, especially for real-time or autoregressive tasks like text generation.\n\n\nclass MultiHeadAttentionKVCache(nn.Module):\n    def __init__(self, dim: int, n_heads: int):\n        super().__init__()\n        self.dim = dim\n        self.n_heads = n_heads\n        self.head_dim = dim // n_heads\n        self.wq = nn.Linear(dim, dim)\n        self.wk = nn.Linear(dim, dim)\n        self.wv = nn.Linear(dim, dim)\n        self.wo = nn.Linear(dim, dim)\n\n        self.register_buffer(\n            \"cache_k\",\n            torch.zeros(MAX_BATCH_SIZE, MAX_SEQ_LEN, self.n_heads, self.head_dim),\n        )\n        self.register_buffer(\n            \"cache_v\",\n            torch.zeros(MAX_BATCH_SIZE, MAX_SEQ_LEN, self.n_heads, self.head_dim),\n        )\n\n    def forward(self, x: torch.Tensor, start_pos: int) -&gt; torch.Tensor:\n        batch_size, seq_len, _ = x.shape\n        assert seq_len == 1, \"seq_len must be 1\"\n        q = self.wq(x)\n        k = self.wk(x)\n        v = self.wv(x)\n\n        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim)\n        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim)\n        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim)\n\n        # Store current k,v in cache at position start_pos\n        # k shape: (batch_size, 1, n_heads, head_dim)\n        # cache_k shape: (MAX_BATCH_SIZE, MAX_SEQ_LEN, n_heads, head_dim)\n        self.cache_k[:batch_size, start_pos : start_pos + seq_len, :, :] = k\n        self.cache_v[:batch_size, start_pos : start_pos + seq_len, :, :] = v\n\n        # Retrieve cached k,v up to current position for attention computation\n        # Retrieved k shape: (batch_size, start_pos+1, n_heads, head_dim)\n        # This allows attending to all previous tokens plus current token\n        k = self.cache_k[:batch_size, : start_pos + seq_len, :, :]\n        v = self.cache_v[:batch_size, : start_pos + seq_len, :, :]\n\n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        scores = scores.softmax(dim=-1)\n        output = torch.matmul(scores, v)\n        # Concatenate heads and apply output projection\n        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n        return self.wo(output)"
  },
  {
    "objectID": "content/head_attention.html#group-query-attention-with-kv-cache",
    "href": "content/head_attention.html#group-query-attention-with-kv-cache",
    "title": "Attention Mechanisms: Memory-Efficient Techniques and Implementations",
    "section": "1.3 Group Query Attention with KV Cache",
    "text": "1.3 Group Query Attention with KV Cache\nGroup Query Attention with KV Cache is an extension of multi-head attention where multiple query heads attend to the same cached key and value pairs. This allows the model to efficiently process key-value caches while reusing the same key-value heads across multiple queries, improving performance, especially in autoregressive tasks.\n\n\n\nGQA\n\n\n\n1.3.1 Key Differences and Modifications\n\n1.3.1.1 Grouping Query Heads\nInstead of having separate key-value heads for each query head, the model groups query heads that attend to the same cached key and value heads. The number of query heads per group is determined by dividing the total number of query heads by the number of key-value heads.\n\\[\n\\text{kv\\_per\\_head} = \\frac{n_{\\text{heads}}}{n_{\\text{kv\\_heads}}}\n\\]\n\n\n1.3.1.2 Key and Value Caching\nAs the attention mechanism processes input tokens, the keys (\\(K\\)) and values (\\(V\\)) for earlier tokens are cached to avoid recomputation in subsequent time steps. These cached keys and values are stored in buffers, which are updated at each new time step:\n\\[\n\\text{cache } K[:, \\text{start\\_pos}: \\text{start\\_pos}+1, :, :] = K\n\\] \\[\n\\text{cache } V[:, \\text{start\\_pos}: \\text{start\\_pos}+1, :, :] = V\n\\]\nThe cached keys and values are then used in the computation of attention scores, reducing the need for recalculating them for each new token.\n\n\n1.3.1.3 Attention Computation\nFor each query head in the group, attention is computed over the cached keys and values. The key and value matrices are repeated across multiple query heads (i.e., the \\(K\\) and \\(V\\) tensors are repeated \\(\\text{kv\\_per\\_head}\\) times) to account for multiple query heads attending to the same set of key-value heads:\n\\[\nK = K \\cdot \\text{repeat\\_interleave}(\\text{kv\\_per\\_head}, \\text{dim}=1)\n\\] \\[\nV = V \\cdot \\text{repeat\\_interleave}(\\text{kv\\_per\\_head}, \\text{dim}=1)\n\\]\n\n\n1.3.1.4 Scaled Dot-Product Attention\nThe attention scores are computed as the dot product between the queries and the keys, followed by a scaling factor to normalize the scores:\n\\[\n\\text{scores} = \\frac{Q K^\\top}{\\sqrt{d_k}}\n\\]\nHere, \\(Q\\) is the query matrix, \\(K^\\top\\) is the transpose of the key matrix, and \\(d_k\\) is the dimensionality of the key vectors.\nThe attention scores are then normalized using softmax:\n\\[\n\\text{scores} = \\text{softmax}(\\text{scores}, \\text{dim}=-1)\n\\]\nThese attention scores are then used to weight the values:\n\\[\n\\text{output} = \\text{scores} \\cdot V\n\\]\n\nclass GroupQueryAttentionKVCache(nn.Module):\n    def __init__(self, dim: int, n_heads: int, n_kv_heads: int):\n        super().__init__()\n        self.dim = dim\n        self.n_heads = n_heads\n        self.n_kv_heads = n_kv_heads\n        self.kv_per_head = n_heads // n_kv_heads\n        assert (\n            self.kv_per_head * self.n_kv_heads == self.n_heads\n        ), \"n_kv_heads must be a divisor of n_heads\"\n        self.head_dim = dim // n_heads\n        assert self.head_dim * self.n_heads == dim, \"dim must be a multiple of n_heads\"\n        self.wq = nn.Linear(dim, dim)\n        self.wk = nn.Linear(dim, n_kv_heads * self.head_dim)\n        self.wv = nn.Linear(dim, n_kv_heads * self.head_dim)\n        self.wo = nn.Linear(dim, dim)\n\n        self.register_buffer(\n            \"cache_k\",\n            torch.zeros(MAX_BATCH_SIZE, MAX_SEQ_LEN, self.n_kv_heads, self.head_dim),\n        )\n        self.register_buffer(\n            \"cache_v\",\n            torch.zeros(MAX_BATCH_SIZE, MAX_SEQ_LEN, self.n_kv_heads, self.head_dim),\n        )\n\n    def forward(self, x: torch.Tensor, start_pos: int):\n        batch_size, seq_len, _ = x.shape\n        assert seq_len == 1, \"seq_len must be 1\"\n\n        q = self.wq(x)\n        k = self.wk(x)\n        v = self.wv(x)\n\n        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim)\n        k = k.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n        v = v.view(batch_size, seq_len, self.n_kv_heads, self.head_dim)\n\n        self.cache_k[:batch_size, start_pos : start_pos + seq_len, :, :] = k\n        self.cache_v[:batch_size, start_pos : start_pos + seq_len, :, :] = v\n\n        k = self.cache_k[:batch_size, : start_pos + seq_len, :, :]\n        v = self.cache_v[:batch_size, : start_pos + seq_len, :, :]\n\n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n\n        # repeat each head of k, v self.kv_per_head times\n        k = k.repeat_interleave(self.kv_per_head, dim=1)\n        v = v.repeat_interleave(self.kv_per_head, dim=1)\n\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        scores = scores.softmax(dim=-1)\n        output = torch.matmul(scores, v)\n        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n        return self.wo(output)"
  },
  {
    "objectID": "content/head_attention.html#multi-head-latent-attention-mla",
    "href": "content/head_attention.html#multi-head-latent-attention-mla",
    "title": "Attention Mechanisms: Memory-Efficient Techniques and Implementations",
    "section": "1.4 Multi-Head Latent Attention (MLA)",
    "text": "1.4 Multi-Head Latent Attention (MLA)\nIn Multi-Head Latent Attention (MLA), a key technique is the low-rank joint compression of the key and value matrices to significantly reduce memory usage in the key-value (KV) cache. This technique is particularly useful for long sequences, where the KV cache size could become a bottleneck.\n\n\n\nMLA\n\n\n\n1.4.0.1 1. Low-Rank Key-Value Compression\nThe MLA approach starts by compressing the keys and values jointly into a smaller latent space, reducing the memory requirement for storing them. The key operations are:\n\nDown-Projection: The original key-value matrix \\(h_t\\) (size \\(d \\times n_{\\text{heads}}\\)) is projected down to a lower-dimensional latent space \\(c_{KV_t}\\) (size \\(d_c\\)) using the down-projection matrix \\(W_{D_{KV}}\\):\n\\[\nc_{KV_t} = W_{D_{KV}} h_t\n\\]\nWhere \\(W_{D_{KV}} \\in \\mathbb{R}^{d_c \\times d}\\) is the down-projection matrix, and \\(d_c\\) is the latent dimension, which is significantly smaller than the original dimensionality \\(d_{\\text{model}}\\).\nUp-Projection: After compression, the compressed keys and values are restored to their original dimensions for use in attention computation through the up-projection matrices \\(W_{U_K}\\) for keys and \\(W_{U_V}\\) for values:\n\\[\nk_{C_t} = W_{U_K} c_{KV_t}, \\quad v_{C_t} = W_{U_V} c_{KV_t}\n\\]\nHere, \\(W_{U_K} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_c}\\) and \\(W_{U_V} \\in \\mathbb{R}^{d_{\\text{model}} \\times d_c}\\) are the up-projection matrices for keys and values, respectively.\n\nThe benefit of this low-rank joint compression is that it significantly reduces the memory footprint of the KV cache. Specifically, instead of storing the full-sized key and value matrices, we only store the compressed latent vectors \\(c_{KV_t}\\), which require much less space.\n\n\n1.4.0.2 2. Key-Value Cache Reduction\nDuring inference, the compressed vectors \\(c_{KV_t}\\) are cached, significantly reducing the memory required for storing keys and values across layers. The total size of the KV cache is proportional to \\(d_c\\) (the compressed latent dimension), which is much smaller than the original dimensionality.\nIn practical terms, the KV cache has only \\(d_c \\times l\\) elements (where \\(l\\) is the number of layers), instead of the full-size key-value matrices for each layer.\n\n\n1.4.0.3 3. Query Compression\nIn addition to compressing the keys and values, MLA also compresses the queries to further reduce the activation memory during training. This compression is done through similar down-projection and up-projection steps:\n\nDown-Projection of Queries: The queries \\(h_t\\) are projected down into a smaller latent vector \\(c_{Q_t}\\) using the down-projection matrix \\(W_{D_Q}\\):\n\\[\nc_{Q_t} = W_{D_Q} h_t\n\\]\nWhere \\(W_{D_Q} \\in \\mathbb{R}^{d'_c \\times d}\\) is the down-projection matrix, and \\(d'_c\\) is the query compression dimension.\nUp-Projection of Queries: The compressed query \\(c_{Q_t}\\) is restored to its original dimensions using the up-projection matrix \\(W_{U_Q}\\):\n\\[\nq_{C_t} = W_{U_Q} c_{Q_t}\n\\]\nWhere \\(W_{U_Q} \\in \\mathbb{R}^{d_{\\text{model}} \\times d'_c}\\) is the up-projection matrix for queries.\n\nThis query compression reduces the memory needed for the query representations, helping to alleviate memory pressure during training.\n\n\n1.4.0.4 4. Inference Optimization\nDuring inference, additional optimizations are possible. Specifically, the up-projection matrices for keys \\(W_{U_K}\\) and values \\(W_{U_V}\\) can be absorbed into the projection matrices \\(W_Q\\) and \\(W_O\\), respectively, for queries and outputs. This means that the keys and values do not need to be computed explicitly during inference, further reducing memory and computation costs.\n\n\n1.4.0.5 Summary of MLA’s Key Components:\n\nKey-Value Compression: Low-rank joint compression reduces memory usage for keys and values in the KV cache.\nQuery Compression: Compresses queries to reduce activation memory during training.\nInference Optimizations: Absorption of up-projection matrices into the query and output projections eliminates the need for recomputing keys and values.\nReduced KV Cache Size: Only the compressed latent vectors \\(c_{KV_t}\\) need to be cached, which significantly reduces memory usage.\n\n\n\n1.4.0.6 Mathematical Notation Recap:\n\nCompressed Latent Vector for Keys and Values: \\(c_{KV_t} = W_{D_{KV}} h_t\\)\nUp-Projection of Keys and Values: \\(k_{C_t} = W_{U_K} c_{KV_t}, \\quad v_{C_t} = W_{U_V} c_{KV_t}\\)\nCompressed Latent Vector for Queries: \\(c_{Q_t} = W_{D_Q} h_t\\)\nUp-Projection of Queries: \\(q_{C_t} = W_{U_Q} c_{Q_t}\\)\n\nBy reducing the size of the KV cache and compressing the queries, MLA offers substantial memory savings while maintaining the performance of attention mechanisms.\nThere are two ways to implement MLA:\n\nDecompress KV: decompress the key and value matrices to the original dimension before storing them in the cache\nCompress KV: compress the key and value matrices to a lower dimension and store them in the cache\n\nWhy would we want to decompress the key and value matrices to the original dimension?\nAlthough decompressing the key and value matrices to the original dimension is memory inefficient, it’s generally faster to compute attention scores and apply them to the values. Vs in the compressed version we would have to recompute all the past key and value matrices from the compressed latent vectors before computing the attention scores.\n\nclass MultiHeadLatentAttentionDecompressKV(nn.Module):\n    def __init__(\n        self, dim: int, n_heads: int, q_compression_dim: int, kv_compression_dim: int\n    ):\n        super().__init__()\n        assert dim % n_heads == 0, \"dim must be divisible by n_heads\"\n        self.dim = dim\n        self.n_heads = n_heads\n        self.head_dim = dim // n_heads\n        self.kv_compression_dim = kv_compression_dim\n\n        # Query compression weights\n        # w_dq: Projects query from full dim to compressed dim (down projection)\n        self.w_dq = nn.Linear(dim, q_compression_dim)\n        # w_uq: Projects query back from compressed to full dim (up projection)\n        self.w_uq = nn.Linear(q_compression_dim, dim)\n\n        # Key-Value compression weights\n        # w_dkv: Projects both key and value to shared compressed dim\n        self.w_dkv = nn.Linear(dim, kv_compression_dim)\n        # w_uk, w_uv: Separate projections from compressed dim back to full dim\n        self.w_uk = nn.Linear(kv_compression_dim, dim)\n        self.w_uv = nn.Linear(kv_compression_dim, dim)\n\n        # Final output projection\n        self.w_o = nn.Linear(dim, dim, bias=False)\n\n        # KV Cache buffers to store previous key/value pairs\n        self.register_buffer(\n            \"cache_k\",\n            torch.zeros(MAX_BATCH_SIZE, MAX_SEQ_LEN, self.n_heads, self.head_dim),\n        )\n        self.register_buffer(\n            \"cache_v\",\n            torch.zeros(MAX_BATCH_SIZE, MAX_SEQ_LEN, self.n_heads, self.head_dim),\n        )\n\n    def forward(self, x: torch.Tensor, start_pos: int):\n        # Input shape: (batch_size, seq_len, dim)\n        batch_size, seq_len, _ = x.shape\n        assert (\n            seq_len == 1\n        ), \"Only single-step inference (seq_len=1) is supported for MLA.\"\n\n        # Project to compressed q dimension: (batch_size, seq_len, q_compression_dim)\n        c_q = self.w_dq(x)\n        # Project back to full dimension: (batch_size, seq_len, dim)\n        q = self.w_uq(c_q)\n\n        # Project to compressed kv dimension: (batch_size, seq_len, kv_compression_dim)\n        c_kv = self.w_dkv(x)\n\n        # Project back to full dimension: (batch_size, seq_len, dim)\n        k = self.w_uk(c_kv)\n        v = self.w_uv(c_kv)\n\n        # Reshape to split dim into n_heads and head_dim\n        # q shape: (batch_size, seq_len, n_heads, head_dim)\n        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim)\n        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim)\n        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim)\n\n        # Cache k,v for current position\n        self.cache_k[:batch_size, start_pos : start_pos + seq_len, :, :] = k\n        self.cache_v[:batch_size, start_pos : start_pos + seq_len, :, :] = v\n\n        # Get cached k,v up to current position\n        k = self.cache_k[:batch_size, : start_pos + seq_len, :, :]\n        v = self.cache_v[:batch_size, : start_pos + seq_len, :, :]\n\n        # Transpose to get shape (batch_size, n_heads, seq_len, head_dim)\n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n\n        # Compute attention scores: (batch_size, n_heads, seq_len, seq_len)\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        attention = F.softmax(scores, dim=-1)\n\n        # Apply attention: (batch_size, n_heads, seq_len, head_dim)\n        context = torch.matmul(attention, v)\n\n        # Reshape back to (batch_size, seq_len, dim)\n        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n        output = self.w_o(context)\n\n        return output\n\n\nclass MultiHeadLatentAttentionCompressKV(nn.Module):\n    def __init__(\n        self, dim: int, n_heads: int, q_compression_dim: int, kv_compression_dim: int\n    ):\n        super().__init__()\n        assert dim % n_heads == 0, \"dim must be divisible by n_heads\"\n        self.dim = dim\n        self.n_heads = n_heads\n        self.head_dim = dim // n_heads\n        self.kv_compression_dim = kv_compression_dim\n\n        # Down-project query from dim to compressed dimension\n        self.w_dq = nn.Linear(dim, q_compression_dim)\n        # Up-project query back to original dimension\n        self.w_uq = nn.Linear(q_compression_dim, dim)\n\n        # Down-project key-value to shared compressed dimension\n        self.w_dkv = nn.Linear(dim, kv_compression_dim)\n        # Separate up-projections for key and value back to original dimension\n        self.w_uk = nn.Linear(kv_compression_dim, dim)\n        self.w_uv = nn.Linear(kv_compression_dim, dim)\n\n        # Final output projection\n        self.w_o = nn.Linear(dim, dim, bias=False)\n\n        # Cache for storing compressed key-value pairs during generation\n        self.register_buffer(\n            \"cache_kv\", torch.zeros(MAX_BATCH_SIZE, MAX_SEQ_LEN, kv_compression_dim)\n        )\n\n    def forward(self, x: torch.Tensor, start_pos: int):\n        batch_size, seq_len, _ = x.shape\n        assert (\n            seq_len == 1\n        ), \"Only single-step inference (seq_len=1) is supported for MLA.\"\n\n        # Project to compressed q dimension: (batch_size, seq_len, q_compression_dim)\n        c_q = self.w_dq(x)\n        # Project back to full dimension: (batch_size, seq_len, dim)\n        q = self.w_uq(c_q)\n\n        # Project to compressed kv dimension: (batch_size, seq_len, kv_compression_dim)\n        c_kv = self.w_dkv(x)\n\n        # Cache compressed kv for current position\n        self.cache_kv[:batch_size, start_pos : start_pos + seq_len, :] = c_kv\n\n        # Retrieve cached kv up to current position\n        cached_kv = self.cache_kv[:batch_size, : start_pos + seq_len, :]\n\n        # Project back to full dimension: (batch_size, seq_len, dim)\n        k = self.w_uk(cached_kv)\n        v = self.w_uv(cached_kv)\n\n        # Reshape to get shape (batch_size, n_heads, seq_len, head_dim)\n        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim).transpose(1, 2)\n        # seq_len is not the same as the seq_len of the input which is 1, it is the seq_len of the cached kv\n        # hence we need to view it with -1 to get the correct shape\n        k = k.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n        v = v.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        attention = F.softmax(scores, dim=-1)\n\n        context = torch.matmul(attention, v)\n\n        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n        output = self.w_o(context)\n\n        return output"
  },
  {
    "objectID": "content/head_attention.html#memory-usage-comparison",
    "href": "content/head_attention.html#memory-usage-comparison",
    "title": "Attention Mechanisms: Memory-Efficient Techniques and Implementations",
    "section": "1.5 Memory Usage Comparison",
    "text": "1.5 Memory Usage Comparison\n\n1.5.0.1 1. Memory Usage in Multi-Head Attention\nIn Multi-Head Attention (MHA), for each query, separate key-value heads are used. This means for each query head, there is a corresponding set of keys and values. The size of the key and value matrices increases with the number of heads, leading to higher memory requirements.\nFor an input sequence of length \\(T\\), model dimension \\(d_{\\text{model}}\\), and number of attention heads \\(n_{\\text{heads}}\\), the memory required for storing keys and values is proportional to:\n\\[\n\\text{Memory}_{\\text{keys/values}} = T \\times d_k \\times n_{\\text{heads}}\n\\]\nwhere \\(d_k = \\frac{d_{\\text{model}}}{n_{\\text{heads}}}\\) is the dimension of each head.\nKey and Value Memory:\nIn MHA, each query head has its own unique key and value projections, so the memory for storing keys and values grows with \\(n_{\\text{heads}}\\), the number of heads.\n\n\n1.5.0.2 2. Memory Usage in Group Query Attention\nIn Group Query Attention, multiple query heads share the same key and value heads, significantly reducing the number of unique key-value matrices stored in memory. This grouping reduces the total memory needed to store the keys and values, as multiple query heads attend to the same key and value matrices.\nThe key-value heads are shared across query heads, and the number of key-value heads is determined by \\(n_{\\text{kv\\_heads}}\\). The memory for storing keys and values is now:\n\\[\n\\text{Memory}_{\\text{keys/values}} = T \\times d_k \\times n_{\\text{kv\\_heads}}\n\\]\nHere, \\(d_k\\) is the dimensionality of each key-value pair, and the memory required depends only on \\(n_{\\text{kv\\_heads}}\\), which is usually much smaller than the total number of heads in MHA.\nKey and Value Memory Reduction:\nSince the keys and values are repeated across multiple query heads, this leads to a reduction in memory usage. Specifically, the number of key-value heads \\(n_{\\text{kv\\_heads}}\\) is much smaller than the total number of heads \\(n_{\\text{heads}}\\) in MHA, leading to lower memory usage.\n\n\n1.5.0.3 3. Memory Usage in Multi-Query Attention (a Variation of Group Query Attention)\nMulti-Query Attention is a variation of Group Query Attention where all query heads attend to the same set of key-value heads. Instead of having multiple key-value heads, all query heads in Multi-Query Attention use the same set of keys and values. This leads to the most significant memory reduction, as there is only a single key-value matrix shared across all query heads.\nIn Multi-Query Attention, the number of key-value heads \\(n_{\\text{kv\\_heads}}\\) is equal to 1, as all query heads attend to the same key and value:\n\\[\n\\text{Memory}_{\\text{keys/values}} = T \\times d_k \\times 1\n\\]\nIn this case, there is only one key-value matrix to store, which drastically reduces memory usage compared to both MHA and Group Query Attention.\n\n\n1.5.0.4 4. Memory Usage in Multi-Head Latent Attention (MLA)\nMulti-Head Latent Attention (MLA) introduces an additional technique to further reduce memory usage by compressing the key and value matrices into a lower-dimensional latent space. This low-rank joint compression of keys and values helps in reducing the size of the key-value cache while maintaining the ability to compute attention effectively.\nIn MLA, the key-value matrices are compressed into a latent space of dimension \\(d_c\\), where \\(d_c\\) is much smaller than the original dimension \\(d_{\\text{model}}\\). The memory for storing compressed key-value matrices is proportional to:\n\\[\n\\text{Memory}_{\\text{keys/values}} = T \\times d_c\n\\]\n\n\n1.5.1 Memory Comparison\n\n\n\n\n\n\n\nAttention Mechanism\nMemory Usage for Keys/Values\n\n\n\n\nMulti-Head Attention\n\\(2 \\times T \\times d_k \\times n_{\\text{heads}}\\)\n\n\nGroup Query Attention\n\\(2 \\times T \\times d_k \\times n_{\\text{kv\\_heads}}\\)\n\n\nMulti-Query Attention\n\\(2 \\times T \\times d_k \\times 1\\)\n\n\nMulti-Head Latent Attention\n\\(2 \\times T \\times d_c\\)"
  },
  {
    "objectID": "content/head_attention.html#experiments",
    "href": "content/head_attention.html#experiments",
    "title": "Attention Mechanisms: Memory-Efficient Techniques and Implementations",
    "section": "1.6 Experiments",
    "text": "1.6 Experiments\nIn the following experiments, we will evaluate the performance and memory efficiency of different attention modules in an autoregressive generation setup. Using the profile_model function, we will measure key metrics such as memory usage, generation speed, and the number of trainable parameters. Each module will be tested with KV Cache enabled to optimize performance and reduce redundant computations.\nFor consistency and fairness, all models will share the same configuration: batch size, sequence length, token dimensions, and number of attention heads. The goal is not to assess the correctness or quality of the models but rather to validate whether their performance and memory usage align with theoretical expectations.\nBelow is the code for the generation loop and profiling. Through this analysis, we aim to understand the trade-offs each attention mechanism presents in terms of efficiency and scalability.\n\ndef generate(\n    model: nn.Module,\n    input_tokens: torch.Tensor,\n    max_length: int = 100,\n    use_kv_cache: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"Generate a sequence of tokens using an autoregressive model.\n\n    This function performs autoregressive generation by repeatedly feeding the model's output\n    back as input to generate a sequence of tokens up to max_length.\n\n    Args:\n        model (nn.Module): The neural network model used for generation. Should accept input\n            tokens and return logits for next token prediction.\n        input_tokens (torch.Tensor): Initial input sequence of shape (batch_size, seq_len, dim).\n            This serves as the prompt/context for generation.\n        max_length (int, optional): Maximum number of new tokens to generate. Defaults to 100.\n        use_kv_cache (bool, optional): Whether to use key-value caching during generation.\n            If True, expects model to accept start_pos parameter and maintain its own cache.\n            If False, recomputes attention over full sequence each step. Defaults to False.\n\n    Returns:\n        torch.Tensor: Generated sequence of shape (batch_size, seq_len + max_length, dim),\n            containing the input tokens followed by max_length generated tokens.\n\n    Note:\n        - When use_kv_cache=True, the model should implement caching of key-value pairs\n          to avoid recomputing attention over the entire sequence each generation step.\n        - The model should output logits of shape (batch_size, seq_len, dim) where dim\n          matches the input token dimension.\n\n    This function is not a correct implementation of autoregressive generation,\n    it is just a simple implementation to test the attention mechanisms\n    \"\"\"\n    generated = input_tokens\n    for i in range(max_length):\n        with torch.no_grad():\n            if use_kv_cache:\n                next_token_logits = model(generated[:, i : i + 1], start_pos=i)\n            else:\n                next_token_logits = model(generated)\n        next_token = next_token_logits[:, -1:, :]\n        generated = torch.cat([generated, next_token], dim=1)\n\n    return generated\n\n\n# Function to profile a model\ndef profile_model(\n    model: nn.Module,\n    input_seq: torch.Tensor,\n    n_runs: int,\n    max_length: int = 64,\n    use_kv_cache: bool = False,\n    device: torch.device = torch.device(\"cpu\"),\n    **kwargs\n):\n    \"\"\"\n    Profile a model's performance and memory usage.\n    \"\"\"\n\n    total_time = 0\n    total_tokens = 0\n\n    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n    initial_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else get_cpu_memory_usage()\n\n\n    m_obj = model(**kwargs).to(device)\n    for _ in range(n_runs):\n        start_time = time.time()\n        _ = generate(\n            m_obj, input_seq, max_length=max_length, use_kv_cache=use_kv_cache\n        )\n        end_time = time.time()\n\n        total_time += end_time - start_time  # Accumulate time in seconds\n        total_tokens += max_length  # Accumulate tokens generated\n\n    # Measure memory usage after inference\n    final_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else get_cpu_memory_usage()\n    memory_usage = (final_memory - initial_memory) / 1024 / 1024  # Convert to MB\n    parameters = sum(p.numel() for p in m_obj.parameters())\n    # Compute tokens/sec\n    tokens_per_sec = total_tokens / total_time if total_time &gt; 0 else 0\n\n    return (\n        total_time,\n        memory_usage,\n        tokens_per_sec,\n        parameters,\n    )\n\n# get number of parameters\ndef get_num_params(model: nn.Module) -&gt; int:\n    return sum(p.numel() for p in model.parameters())\n\nimport psutil\nimport os\n\ndef get_cpu_memory_usage():\n    process = psutil.Process(os.getpid())\n    memory_usage = process.memory_info().rss  # Resident Set Size (RSS) in bytes\n    return memory_usage\n\n\n# Main evaluation loop\nbatch_size = MAX_BATCH_SIZE // 2\nseq_len = MAX_SEQ_LEN // 4\nmax_length = MAX_SEQ_LEN // 2\ndim = 2048\nn_heads = 16\ndevice = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda\")\n\ninput_seq = torch.randn(batch_size, seq_len, dim).to(device)\nn_runs = 5\n\nmodels = {\n    # \"MHA\": (MultiHeadAttention, False, {\"dim\": dim, \"n_heads\": n_heads}),\n    \"MHA KV Cache\": (MultiHeadAttentionKVCache, True, {\"dim\": dim, \"n_heads\": n_heads}),\n    \"GQA KV Cache\": (\n        GroupQueryAttentionKVCache,\n        True,\n        {\"dim\": dim, \"n_heads\": n_heads, \"n_kv_heads\": n_heads // 4},\n    ),\n    \"MQA KV Cache\": (\n        GroupQueryAttentionKVCache,\n        True,\n        {\"dim\": dim, \"n_heads\": n_heads, \"n_kv_heads\": 1},\n    ),\n    \"MLA KV Cache Decompress\": (\n        MultiHeadLatentAttentionDecompressKV,\n        True,\n        {\n            \"dim\": dim,\n            \"n_heads\": n_heads,\n            \"q_compression_dim\": dim // 32,\n            \"kv_compression_dim\": dim // 32,\n        },\n    ),\n    \"MLA KV Cache Compress\": (\n        MultiHeadLatentAttentionCompressKV,\n        True,\n        {\n            \"dim\": dim,\n            \"n_heads\": n_heads,\n            \"q_compression_dim\": dim // 32,\n            \"kv_compression_dim\": dim // 32,\n        },\n    ),\n}\n\nresults = {}\n\nfor model_name, (model, use_kv_cache, kwargs) in models.items():\n    print(f\"Profiling {model_name}...\")\n    total_time, memory_usage, tokens_per_sec, parameters = (\n        profile_model(\n            model,\n            input_seq,\n            n_runs,\n            max_length=max_length,\n            use_kv_cache=use_kv_cache,\n            device=device,\n            **kwargs,\n        )\n    )\n    results[model_name] = {\n        \"total_time\": total_time,\n        \"memory_usage\": memory_usage,\n        \"tokens_per_sec\": tokens_per_sec,\n        \"parameters\": parameters,\n    }\n    print(\n        f\"Model: {model_name}, Total Time: {total_time:.2f}s, Memory Usage: {memory_usage:.2f}MB, Parameters: {parameters}\"\n    )\n\n# Plot results\nresults_df = pd.DataFrame(\n    {\n        \"Model\": list(models.keys()),\n        \"Total Time (s)\": [results[model][\"total_time\"] for model in models],\n        \"Memory Usage (MB)\": [results[model][\"memory_usage\"] for model in models],\n        \"Parameters\": [results[model][\"parameters\"] for model in models],\n        \"Tokens/s\": [results[model][\"tokens_per_sec\"] for model in models],\n    }\n)\n\nProfiling MHA KV Cache...\nModel: MHA KV Cache, Total Time: 20.87s, Memory Usage: 1216.03MB, Parameters: 16785408\nProfiling GQA KV Cache...\nModel: GQA KV Cache, Total Time: 15.43s, Memory Usage: 488.02MB, Parameters: 10490880\nProfiling MQA KV Cache...\nModel: MQA KV Cache, Total Time: 14.10s, Memory Usage: 290.02MB, Parameters: 8917248\nProfiling MLA KV Cache Decompress...\nModel: MLA KV Cache Decompress, Total Time: 18.53s, Memory Usage: 1234.52MB, Parameters: 4855936\nProfiling MLA KV Cache Compress...\nModel: MLA KV Cache Compress, Total Time: 30.92s, Memory Usage: 226.52MB, Parameters: 4855936"
  },
  {
    "objectID": "content/head_attention.html#observations",
    "href": "content/head_attention.html#observations",
    "title": "Attention Mechanisms: Memory-Efficient Techniques and Implementations",
    "section": "1.7 Observations",
    "text": "1.7 Observations\n\n1.7.1 1. Number of Parameters\nThe number of parameters follows the order MHA &gt; GQA &gt; MQA &gt; MLA. This is because MHA maintains independent key-value projections for all heads, whereas MLA uses low-rank approximations.\n\n\n1.7.2 2. Memory Usage\n\nMHA: The highest memory usage, as the key-value cache stores projections for all heads independently.\n\nMLA with Decompressed KV: Comparable to MHA in memory usage because it also maintains decompressed key-value matrices.\n\nMLA with Compressed KV: The lowest memory usage because the key-value cache stores compressed latent vectors instead of full matrices.\n\nGQA and MQA:\n\nGQA uses less memory than MHA because multiple query heads share the same key-value heads.\n\nMQA further reduces memory usage by having all query heads share a single set of key-value projections.\n\n\n\n\n1.7.3 3. Throughput (Tokens/s)\n\nMLA with Decompressed KV achieves higher throughput than MLA with Compressed KV.\n\nThis happens because in MLA with Compressed KV, key-value matrices need to be recomputed from latent vectors for every token during attention computation, adding overhead.\n\nDespite this overhead, MLA remains efficient in memory and parameters, with the decompressed variant being the better choice for practical implementations, as demonstrated in the DeepSeek paper.\n\n\n\n1.7.4 4. MLA’s Advantages Over MHA\n\nMLA achieves significantly fewer parameters by using low-rank approximations like LoRA (Low-Rank Adaptation) for key and value projection matrices.\n\nWhile MLA reduces parameters, the DeepSeek paper claims its prediction quality is on par with or better than MHA. This makes MLA attractive for memory-constrained environments without sacrificing quality.\n\n\n\n1.7.5 5. Trade-offs Between Models\n\nMHA: Best prediction quality due to the highest expressiveness, enabled by the largest number of parameters.\n\nGQA and MQA:\n\nAs parameters decrease, throughput improves significantly, making these models ideal for scenarios where speed and resource efficiency are critical.\n\nHowever, a slight reduction in prediction quality might be expected in theory.\n\n\nMLA:\n\nStrikes a balance by offering reduced memory usage and parameters.\n\nCompressed MLA is memory-efficient, while decompressed MLA is more throughput-friendly.\n\n\n\n\n\n1.7.6 Additional Consideration\nEach model aligns with specific use cases: - MHA: Suited for scenarios where prediction quality is paramount, regardless of resource cost.\n- MQA/GQA: Ideal for real-time systems with stringent latency requirements.\n- MLA: Best for memory-constrained environments, such as edge devices or applications with limited hardware resources.\n\nresults_df\n\n\n  \n    \n\n\n\n\n\n\nModel\nTotal Time (s)\nMemory Usage (MB)\nParameters\nTokens/s\n\n\n\n\n0\nMHA KV Cache\n20.869432\n1216.031250\n16785408\n245.334903\n\n\n1\nGQA KV Cache\n15.433480\n488.019531\n10490880\n331.746317\n\n\n2\nMQA KV Cache\n14.097093\n290.016602\n8917248\n363.195457\n\n\n3\nMLA KV Cache Decompress\n18.534862\n1234.524414\n4855936\n276.236215\n\n\n4\nMLA KV Cache Compress\n30.923788\n226.524414\n4855936\n165.568332\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\n\n# Create a FacetGrid with 3 rows and 2 columns\nfig = plt.figure(figsize=(12, 12))\n\n# Define metrics to plot\nmetrics = [\n    \"Total Time (s)\",\n    \"Memory Usage (MB)\",\n    \"Parameters\",\n    \"Tokens/s\",\n]\n\n# Create facet grid for multiple plots\ng = sns.FacetGrid(\n    pd.melt(results_df, id_vars=[\"Model\"], value_vars=metrics),\n    col_wrap=2,\n    height=4,\n    aspect=1.5,\n    row=None,\n    col=\"variable\",\n    sharex=False,\n    sharey=False,\n)\n\n# Plot bars for each metric\ng.map_dataframe(\n    sns.barplot,\n    x=\"Model\",\n    y=\"value\",\n    hue=\"Model\",\n    palette=\"viridis\",\n)\n\n# Rotate x-axis labels for better readability\nfor ax in g.axes:\n    ax.tick_params(axis=\"x\", rotation=45)\n\n# Adjust layout to prevent overlapping\nplt.tight_layout()\nplt.show()\n\n&lt;Figure size 1200x1200 with 0 Axes&gt;"
  },
  {
    "objectID": "content/head_attention.html#references",
    "href": "content/head_attention.html#references",
    "title": "Attention Mechanisms: Memory-Efficient Techniques and Implementations",
    "section": "1.8 References",
    "text": "1.8 References\n\nAttention Is All You Need\nDeepSeek-V2\nGQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\nMQA\nMeta-Llama code\nDeepSeek-V3 code"
  },
  {
    "objectID": "content/offline_rl.html",
    "href": "content/offline_rl.html",
    "title": "Trajectory Transformer (TT) from code",
    "section": "",
    "text": "In this notebook, we explore the Trajectory Transformer (TT), a sequence modeling approach to reinforcement learning (RL) introduced in the paper Offline RL as One Big Sequence Modeling Problem by Janner et al. TT reframes RL as a sequence modeling task, leveraging Transformer architectures to model trajectories of states, actions, and rewards. By doing so, it unifies different aspects of RL—policy learning, value estimation, and model-based planning—under a single framework.\n\n\n\nTT\n\n\nThis notebook provides a detailed walkthrough of TT, an offline RL model that formulates trajectory generation as a sequence modeling problem. The goal is to help readers understand TT’s inner workings, from data processing and model training to beam search-based rollouts and evaluation.\nWhat to Expect in This Notebook?\n\nOffline RL & TT Basics – Overview of discretization, tokenization, and trajectory modeling.\nTraining TT – Cross-entropy loss, weighted action importance, and optimization techniques (LR scheduling, weight decay, gradient clipping).\nRollouts & Beam Search – How TT predicts future trajectories, explores high-reward sequences, and enables long-term planning.\nEvaluation – Rollout-based metrics (mean reward, variance, done ratio) to assess trajectory quality, stability, and efficiency.\n\n\n\n\n\n\n\nWhy This Notebook?\n\n\n\nMuch of the code in this notebook is inspired by the original TT implementation, with significant updates and refinements. Additionally:\n\nThe KV cache and vectorized rollouts are adapted from the faster-trajectory-transformer repository.\nBoth the original TT repo and faster-trajectory-transformer are non-functional due to:\n\nD4RL being deprecated.\nUse of outdated Gym versions.\nReliance on older MuJoCo versions that no longer run.\n\n\nThis notebook modernizes the implementation, ensuring it runs with current libraries, while providing a clearer, more accessible explanation of TT’s mechanics.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThis notebook does not introduce new concepts or bespoke code. Instead, it presents existing concepts and code in a more accessible manner for new machine learning engineers and scientists in this field.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nSome of the code cells in this notebook are collapsed by default for brevity.\n\n\n\n\n\n\n\n\nTips for Following This Notebook\n\n\n\n\nTake Your Time: This notebook is lengthy and cannot be completed in one sitting. Approach it slowly, and take breaks as needed to fully understand each section.\nRun the Notebook: To get the most out of this notebook, run it to train a model and reproduce the results. Try using different Gym environments to see what you observe and how the TT performs in various settings.\nSupplementary Resource: This document is not a replacement for the original paper. While it provides a detailed walkthrough of TT’s implementation and usage, it does not cover all the theoretical foundations and experimental results presented in the paper. Readers should refer to the original paper for a comprehensive understanding of the underlying concepts and the broader context of TT’s development.\nUnderstand the Code: Take the time to read and understand the code snippets provided. Each section builds on the previous one, so a solid understanding of the code will help you follow along more easily.\nExperiment and Modify: Don’t hesitate to experiment with the code. Modify parameters, try different environments, and observe how these changes affect the results. This hands-on approach will deepen your understanding of TT.\nUse Visualizations: Pay attention to the visualizations provided in the notebook. They can help you better understand the behavior of the model and the results of your experiments.\n\n\n\n\n\nRL traditionally operates in an online setting, where an agent actively interacts with an environment, collecting data to learn an optimal policy. This approach, while effective, has several limitations:\n\nData inefficiency: Many RL algorithms require millions of interactions to learn good policies.\nSafety concerns: Direct exploration can be costly or dangerous in real-world applications like robotics, healthcare, and autonomous driving.\nExpensive data collection: Gathering high-quality data in real-world systems is often impractical.\n\nOffline RL (also known as batch RL) addresses these challenges by learning solely from a pre-collected dataset of past interactions, without any further environment interaction. The key differences between traditional RL and offline RL are:\n\n\n\n\n\n\n\n\nFeature\nTraditional (Online) RL\nOffline RL\n\n\n\n\nData Collection\nAgent interacts with the environment continuously\nFixed dataset with no new data collection\n\n\nExploration\nActive, the agent learns by trial and error\nNo exploration, learning is constrained by dataset\n\n\nSafety\nCan lead to unsafe exploration\nNo risk, as learning is purely from past data\n\n\nPracticality\nHard to apply in real-world settings\nMore feasible for real-world applications\n\n\n\n\n\nOffline RL is crucial for scenarios where active exploration is either unsafe or expensive. Some examples include:\n“Imagine trying to train a self-driving car without ever letting it touch the road. Instead of real-world driving, you have to learn everything from past recorded trips—navigating intersections, avoiding pedestrians, and handling bad weather—all from historical data. Sounds impossible? This is exactly the challenge that Offline RL aims to solve.”\nTT takes this a step further, leveraging the same Transformer architecture that powers language models like GPT to model sequences of actions, states, and rewards—treating RL as a prediction problem rather than a trial-and-error game.\nDespite its benefits, offline RL faces unique challenges:\n\nOut-of-Distribution (OOD) Actions: Since the agent cannot explore, it might learn to make decisions outside the dataset distribution, leading to extrapolation errors.\nLimited Coverage: If the dataset does not contain enough diverse experiences, the learned policy may not generalize well.\n\n\n\n\nThe TT offers a novel approach to offline RL by treating RL as a sequence modeling problem, inspired by successes in natural language processing (NLP). Instead of relying on traditional RL components like Q-learning or policy optimization, TT models full trajectories as sequences and generates optimal action sequences through beam search planning.\n\n\n\n\nHandles long-horizon dependencies: Unlike traditional RL models that rely on stepwise decisions, TT captures entire sequences of states, actions, and rewards.\nAvoids OOD actions: Since TT directly models trajectories from data, it naturally generates in-distribution actions, reducing extrapolation errors.\nUnifies multiple RL approaches: TT can be used for imitation learning, goal-conditioned RL, and offline RL under a single framework.\n\nBy leveraging sequence modeling, TT sidesteps many challenges of conventional offline RL, offering a more scalable and flexible alternative to existing methods.\nUnlike traditional RL models that interact with the environment, TT reframes RL as a sequence modeling problem, treating trajectories as text-like sequences—similar to how GPT-4 generates coherent text.\n\n\nImports for the project\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom typing import Any\nimport numpy as np\nimport gymnasium as gym\nfrom gymnasium import spaces\nfrom typing import List\nimport torch.nn.functional as F\nfrom typing import Tuple, Any\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DataLoader, Dataset, Subset, SubsetRandomSampler\nfrom typing import Optional\nfrom tqdm.auto import tqdm, trange\nfrom minari import EpisodeData, MinariDataset\nimport minari\nfrom gymnasium import Env\nimport os\nimport time\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nimport math\nimport warnings\nimport pickle\nimport random\nimport glob\n\n\nWarning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you are running this notebook on a CUDA-enabled machine, it will use the entire dataset for training and regular parameters. On non-CUDA machines, the notebook heavily subsamples the dataset and adjusts hyperparameters to ensure it can run on local development machines with lower performance. This behavior is controlled using the local variable, which you can override as needed.\n\n\n\n# local variable is used to run the code faster on local machine\nlocal = not torch.cuda.is_available()\n\n\n\nSeed everything for reproducibility\nseed = 42\nos.environ[\"PYTHONHASHSEED\"] = str(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\ntorch.manual_seed(seed)\n\ndevice = torch.device(\n    \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n)"
  },
  {
    "objectID": "content/offline_rl.html#introduction",
    "href": "content/offline_rl.html#introduction",
    "title": "Trajectory Transformer (TT) from code",
    "section": "",
    "text": "In this notebook, we explore the Trajectory Transformer (TT), a sequence modeling approach to reinforcement learning (RL) introduced in the paper Offline RL as One Big Sequence Modeling Problem by Janner et al. TT reframes RL as a sequence modeling task, leveraging Transformer architectures to model trajectories of states, actions, and rewards. By doing so, it unifies different aspects of RL—policy learning, value estimation, and model-based planning—under a single framework.\n\n\n\nTT\n\n\nThis notebook provides a detailed walkthrough of TT, an offline RL model that formulates trajectory generation as a sequence modeling problem. The goal is to help readers understand TT’s inner workings, from data processing and model training to beam search-based rollouts and evaluation.\nWhat to Expect in This Notebook?\n\nOffline RL & TT Basics – Overview of discretization, tokenization, and trajectory modeling.\nTraining TT – Cross-entropy loss, weighted action importance, and optimization techniques (LR scheduling, weight decay, gradient clipping).\nRollouts & Beam Search – How TT predicts future trajectories, explores high-reward sequences, and enables long-term planning.\nEvaluation – Rollout-based metrics (mean reward, variance, done ratio) to assess trajectory quality, stability, and efficiency.\n\n\n\n\n\n\n\nWhy This Notebook?\n\n\n\nMuch of the code in this notebook is inspired by the original TT implementation, with significant updates and refinements. Additionally:\n\nThe KV cache and vectorized rollouts are adapted from the faster-trajectory-transformer repository.\nBoth the original TT repo and faster-trajectory-transformer are non-functional due to:\n\nD4RL being deprecated.\nUse of outdated Gym versions.\nReliance on older MuJoCo versions that no longer run.\n\n\nThis notebook modernizes the implementation, ensuring it runs with current libraries, while providing a clearer, more accessible explanation of TT’s mechanics.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThis notebook does not introduce new concepts or bespoke code. Instead, it presents existing concepts and code in a more accessible manner for new machine learning engineers and scientists in this field.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nSome of the code cells in this notebook are collapsed by default for brevity.\n\n\n\n\n\n\n\n\nTips for Following This Notebook\n\n\n\n\nTake Your Time: This notebook is lengthy and cannot be completed in one sitting. Approach it slowly, and take breaks as needed to fully understand each section.\nRun the Notebook: To get the most out of this notebook, run it to train a model and reproduce the results. Try using different Gym environments to see what you observe and how the TT performs in various settings.\nSupplementary Resource: This document is not a replacement for the original paper. While it provides a detailed walkthrough of TT’s implementation and usage, it does not cover all the theoretical foundations and experimental results presented in the paper. Readers should refer to the original paper for a comprehensive understanding of the underlying concepts and the broader context of TT’s development.\nUnderstand the Code: Take the time to read and understand the code snippets provided. Each section builds on the previous one, so a solid understanding of the code will help you follow along more easily.\nExperiment and Modify: Don’t hesitate to experiment with the code. Modify parameters, try different environments, and observe how these changes affect the results. This hands-on approach will deepen your understanding of TT.\nUse Visualizations: Pay attention to the visualizations provided in the notebook. They can help you better understand the behavior of the model and the results of your experiments.\n\n\n\n\n\nRL traditionally operates in an online setting, where an agent actively interacts with an environment, collecting data to learn an optimal policy. This approach, while effective, has several limitations:\n\nData inefficiency: Many RL algorithms require millions of interactions to learn good policies.\nSafety concerns: Direct exploration can be costly or dangerous in real-world applications like robotics, healthcare, and autonomous driving.\nExpensive data collection: Gathering high-quality data in real-world systems is often impractical.\n\nOffline RL (also known as batch RL) addresses these challenges by learning solely from a pre-collected dataset of past interactions, without any further environment interaction. The key differences between traditional RL and offline RL are:\n\n\n\n\n\n\n\n\nFeature\nTraditional (Online) RL\nOffline RL\n\n\n\n\nData Collection\nAgent interacts with the environment continuously\nFixed dataset with no new data collection\n\n\nExploration\nActive, the agent learns by trial and error\nNo exploration, learning is constrained by dataset\n\n\nSafety\nCan lead to unsafe exploration\nNo risk, as learning is purely from past data\n\n\nPracticality\nHard to apply in real-world settings\nMore feasible for real-world applications\n\n\n\n\n\nOffline RL is crucial for scenarios where active exploration is either unsafe or expensive. Some examples include:\n“Imagine trying to train a self-driving car without ever letting it touch the road. Instead of real-world driving, you have to learn everything from past recorded trips—navigating intersections, avoiding pedestrians, and handling bad weather—all from historical data. Sounds impossible? This is exactly the challenge that Offline RL aims to solve.”\nTT takes this a step further, leveraging the same Transformer architecture that powers language models like GPT to model sequences of actions, states, and rewards—treating RL as a prediction problem rather than a trial-and-error game.\nDespite its benefits, offline RL faces unique challenges:\n\nOut-of-Distribution (OOD) Actions: Since the agent cannot explore, it might learn to make decisions outside the dataset distribution, leading to extrapolation errors.\nLimited Coverage: If the dataset does not contain enough diverse experiences, the learned policy may not generalize well.\n\n\n\n\nThe TT offers a novel approach to offline RL by treating RL as a sequence modeling problem, inspired by successes in natural language processing (NLP). Instead of relying on traditional RL components like Q-learning or policy optimization, TT models full trajectories as sequences and generates optimal action sequences through beam search planning.\n\n\n\n\nHandles long-horizon dependencies: Unlike traditional RL models that rely on stepwise decisions, TT captures entire sequences of states, actions, and rewards.\nAvoids OOD actions: Since TT directly models trajectories from data, it naturally generates in-distribution actions, reducing extrapolation errors.\nUnifies multiple RL approaches: TT can be used for imitation learning, goal-conditioned RL, and offline RL under a single framework.\n\nBy leveraging sequence modeling, TT sidesteps many challenges of conventional offline RL, offering a more scalable and flexible alternative to existing methods.\nUnlike traditional RL models that interact with the environment, TT reframes RL as a sequence modeling problem, treating trajectories as text-like sequences—similar to how GPT-4 generates coherent text.\n\n\nImports for the project\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom typing import Any\nimport numpy as np\nimport gymnasium as gym\nfrom gymnasium import spaces\nfrom typing import List\nimport torch.nn.functional as F\nfrom typing import Tuple, Any\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DataLoader, Dataset, Subset, SubsetRandomSampler\nfrom typing import Optional\nfrom tqdm.auto import tqdm, trange\nfrom minari import EpisodeData, MinariDataset\nimport minari\nfrom gymnasium import Env\nimport os\nimport time\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nimport math\nimport warnings\nimport pickle\nimport random\nimport glob\n\n\nWarning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you are running this notebook on a CUDA-enabled machine, it will use the entire dataset for training and regular parameters. On non-CUDA machines, the notebook heavily subsamples the dataset and adjusts hyperparameters to ensure it can run on local development machines with lower performance. This behavior is controlled using the local variable, which you can override as needed.\n\n\n\n# local variable is used to run the code faster on local machine\nlocal = not torch.cuda.is_available()\n\n\n\nSeed everything for reproducibility\nseed = 42\nos.environ[\"PYTHONHASHSEED\"] = str(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\ntorch.manual_seed(seed)\n\ndevice = torch.device(\n    \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n)"
  },
  {
    "objectID": "content/offline_rl.html#introduction-to-gymnasium-and-minari",
    "href": "content/offline_rl.html#introduction-to-gymnasium-and-minari",
    "title": "Trajectory Transformer (TT) from code",
    "section": "2 Introduction to Gymnasium and Minari",
    "text": "2 Introduction to Gymnasium and Minari\nIn this project, we use Gymnasium for environments and Minari for datasets, two essential libraries in RL. If you’re new to RL, understanding these libraries will help you get started with training and evaluating RL models efficiently. If you are already familiar with these libraries feel free to skip this section.\n\n2.1 Gymnasium: A Standard for RL Environments\n\n2.1.1 What is Gymnasium?\nGymnasium (formerly OpenAI Gym) is a widely used library for simulating RL environments. It provides a standardized interface to interact with different RL tasks, making it easy to test and compare different RL algorithms.\n\n\n2.1.2 Key Concepts in Gymnasium\nEnvironments:\n\nAn environment represents a problem setting where an agent interacts and learns.\nExamples include robotic control (Mujoco), video games (Atari), and navigation tasks (GridWorld).\n\nAgent-Environment Loop:\n\nRL involves an agent taking actions in an environment, receiving rewards, and transitioning to new states.\nGymnasium provides the standard step() function to execute this interaction.\n\nGymnasium API:\n\nenv = gymnasium.make(\"CartPole-v1\") → Creates an environment.\nobs, reward, done, truncated, info = env.step(action) → Takes an action and returns the next observation, reward, and status flags.\nenv.reset() → Resets the environment to its initial state.\n\nObservation and Action Spaces:\n\nenv.observation_space: Defines what states look like (e.g., position, velocity).\nenv.action_space: Defines valid actions an agent can take.\n\nRendering:\nenv.render() can visualize the environment (useful for debugging and understanding agent behavior).\nGymnasium helps us simulate real-world scenarios, allowing RL models to learn efficiently in controlled settings.\n\n\n\n2.2 Minari: Standardized Datasets for Offline RL\n\n2.2.1 What is Minari?\nMinari is a dataset library designed for offline RL, where agents learn solely from pre-collected experience datasets instead of interacting with the environment in real time. It provides high-quality, standardized datasets for different RL tasks.\n\n\n2.2.2 Why Use Minari?\nIn offline RL, training directly in Gymnasium is not feasible because the agent cannot collect new data. Instead, Minari offers:\n\nCurated datasets with state-action-reward trajectories from past interactions.\nReproducibility, ensuring fair comparisons across different RL methods.\nSeamless integration with Gymnasium, so datasets match Gym environments.\n\n\n\n2.2.3 Key Concepts in Minari\nDatasets in Offline RL:\n\nEach dataset contains trajectories (sequences of (state, action, reward, next_state)).\nThese are generated using pre-trained policies (expert, medium, or random behavior).\n\nLoading a Minari Dataset:\nimport minari\n\ndataset = minari.load_dataset(\"halfcheetah-expert-v2\")\nUsing Minari for Training:\nInstead of interacting with the environment (env.step()), the agent learns by sampling from the dataset. This is similar to supervised learning, where the agent trains on past experiences rather than live exploration.\n\n\n\n2.3 Bringing It All Together: Gymnasium + Minari in Offline RL\n\nGymnasium provides a standard interface for RL environments (but in offline RL, the agent doesn’t interact with it).\n\nIn out project we will use Gymnasium interface to interact with the environment during beam search.\n\nMinari provides pre-recorded experience datasets, allowing the agent to learn without exploration.\nOur project trains a TT model using Minari datasets to understand and optimize decision-making without direct environment interaction."
  },
  {
    "objectID": "content/offline_rl.html#loading-data",
    "href": "content/offline_rl.html#loading-data",
    "title": "Trajectory Transformer (TT) from code",
    "section": "3 Loading Data",
    "text": "3 Loading Data\nIn this implementation, I initialize the environment and fetch the dataset from Minari. We’ll use the HalfCheetah Gym environment and its corresponding datasets from Minari to demonstrate TT. Some Gym environments use dictionaries to represent observation and action spaces, but neural networks work with ndarrays. To address this, I’ve implemented helper functions to convert observations and actions between dictionaries and flat arrays. The notebook is designed to work with any Gym environment, not just HalfCheetah, so the implementation is more flexible and a bit more complex as a result.\n\n\n\nhalf_cheetah\n\n\n\n\nDefine environment related utilities\ndef get_space_dim(space):\n    if isinstance(space, spaces.Discrete):\n        return 1\n    elif isinstance(space, spaces.Box):\n        return space.shape[0]\n    elif isinstance(space, spaces.Dict):\n        return sum([get_space_dim(v) for v in space.values()])\n    else:\n        raise ValueError(\"Unsupported observation space\")\n\n\ndef flatten_space(s_dict: Any, space: spaces.Space) -&gt; np.ndarray:\n    if isinstance(space, spaces.Discrete):\n        return s_dict\n    elif isinstance(space, spaces.Box):\n        return s_dict\n    elif isinstance(space, spaces.Dict):\n        return np.concatenate(\n            [flatten_space(s_dict[k], space.spaces[k]) for k in space.spaces.keys()],\n            axis=-1,\n        )\n    else:\n        raise ValueError(\"Unsupported observation space\")\n\n\ndef unflatten_space(s_flat: np.ndarray, space: spaces.Space) -&gt; dict:\n    if isinstance(space, spaces.Discrete):\n        return s_flat\n    elif isinstance(space, spaces.Box):\n        return s_flat\n    elif isinstance(space, spaces.Dict):\n        s_dict = {}\n        start = 0\n        for k, v in space.spaces.items():\n            end = start + get_space_dim(v)\n            s_dict[k] = unflatten_space(s_flat[:, start:end], v)\n            start = end\n        return s_dict\n    else:\n        raise ValueError(\"Unsupported observation space\")\n\n\n# Test the flatten_space_dict and unflatten_space_dict functions\ntest_dict = {\"obs\": np.array([[1, 2, 3], [4, 5, 6]]), \"act\": np.array([[0], [1]])}\ntest_space = spaces.Dict(\n    {\"obs\": spaces.Box(low=0, high=10, shape=(3,)), \"act\": spaces.Discrete(2)}\n)\ntest_flat = flatten_space(test_dict, test_space)\ntest_unflat = unflatten_space(test_flat, test_space)\n\nassert np.isclose(\n    test_flat, np.array([[0, 1, 2, 3], [1, 4, 5, 6]])\n).all(), f\"Flattened array {test_flat} is not as expected.\"\nassert np.isclose(\n    test_unflat[\"obs\"], test_dict[\"obs\"]\n).all(), f\"Unflattened observation {test_unflat['obs']} is not as expected.\"\nassert np.isclose(\n    test_unflat[\"act\"], test_dict[\"act\"]\n).all(), f\"Unflattened action {test_unflat['act']} is not as expected.\"\n\n# test discrete space\ntest_dict = np.array([[0], [1]])\ntest_space = spaces.Discrete(2)\ntest_flat = flatten_space(test_dict, test_space)\ntest_unflat = unflatten_space(test_flat, test_space)\n\nassert np.isclose(\n    test_flat, test_dict\n).all(), f\"Flattened array {test_flat} is not as expected.\"\nassert np.isclose(\n    test_unflat, test_dict\n).all(), f\"Unflattened array {test_unflat} is not as expected.\"\n\n# test box space\ntest_dict = np.array([[1, 2, 3], [4, 5, 6]])\ntest_space = spaces.Box(low=0, high=10, shape=(3,))\ntest_flat = flatten_space(test_dict, test_space)\ntest_unflat = unflatten_space(test_flat, test_space)\n\nassert np.isclose(\n    test_flat, test_dict\n).all(), f\"Flattened array {test_flat} is not as expected.\"\nassert np.isclose(\n    test_unflat, test_dict\n).all(), f\"Unflattened array {test_unflat} is not as expected.\"\n\nprint(\"All tests passed successfully.\")\n\n\nAll tests passed successfully.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nTo control which environment to run set dataset_ref and env_name from Minari.\n\n\n\ndataset_ref = \"mujoco/halfcheetah/expert-v0\"\nenv_name = \"HalfCheetah\"\n\n\n\n\n\n\n\nImportant\n\n\n\nBy default, when local=True, the notebook will train on all episodes in the dataset. If you would like to train on a subsample, you can control this behavior using the n_episodes parameter.\n\n\n\n# other parameters\nn_episodes: Optional[int] = None if not local else 10\n\n\n\nLoad the dataset and environment\nbase_m_dataset = minari.load_dataset(dataset_ref, download=True)\nwrapped_env = base_m_dataset.recover_environment(render_mode=\"rgb_array\")\nenv = wrapped_env.unwrapped\nenv.name = env_name\n\nif n_episodes:\n    m_dataset = base_m_dataset.sample_episodes(n_episodes)\nelse:\n    m_dataset = base_m_dataset\n\nprint(f\"Number of episodes: {len(m_dataset)}\")\n\n# Environment parameters\nobservation_dim = get_space_dim(env.observation_space)\naction_dim = get_space_dim(env.action_space)\nreward_dim = 1\nvalue_dim = 1\ntransition_dim = observation_dim + action_dim + reward_dim + value_dim\n\nprint(f\"Observation dim: {observation_dim}, Action dim: {action_dim}\")\nprint(f\"Reward dim: {reward_dim}, Value dim: {value_dim}\")\nprint(f\"Transition dim: {transition_dim}\")\nprint(f\"One episode from the dataset: {m_dataset[0]}\")\n\n\n/Users/abrar/Library/Caches/pypoetry/virtualenvs/blog-FROQ9Grm-py3.11/lib/python3.11/site-packages/minari/dataset/minari_dataset.py:204: UserWarning: Installed mujoco version 3.1.6 does not meet the requirement ==3.2.3.\nWe recommend to install the required version with `pip install \"mujoco==3.2.3\"`\n  warnings.warn(\n\n\nNumber of episodes: 10\nObservation dim: 17, Action dim: 6\nReward dim: 1, Value dim: 1\nTransition dim: 25\nOne episode from the dataset: EpisodeData(id=502, total_steps=1000, observations=ndarray of shape (1001, 17) and dtype float64, actions=ndarray of shape (1000, 6) and dtype float32, rewards=ndarray of 1000 floats, terminations=ndarray of 1000 bools, truncations=ndarray of 1000 bools, infos=dict with the following keys: [])"
  },
  {
    "objectID": "content/offline_rl.html#key-terminology",
    "href": "content/offline_rl.html#key-terminology",
    "title": "Trajectory Transformer (TT) from code",
    "section": "4 Key Terminology",
    "text": "4 Key Terminology\n\nState/Observation: Represents the current condition of the environment. In TT, states are discretized into tokens and modeled as part of a sequence. A state is a vector [s1, s2, s3, ...], where the number of state variables is referred to as observation_dim. For example, in HalfCheetah, observation_dim = 17.\nAction: A decision taken by the agent that affects the environment. Actions are also vectorized, e.g., [a1, a2, ...], and modeled alongside states in TT. The number of action variables is called action_dim. For HalfCheetah, action_dim = 6.\nReward: A scalar signal r that quantifies the immediate outcome of an action. TT uses rewards to guide trajectory optimization. Typically, reward_dim = 1, but this is not always the case.\nValue: The expected cumulative future reward from a given state. In TT, values (v) are used during beam search for trajectory optimization. Like rewards, values are scalars, so value_dim = 1.\nTransition: A single step in the environment, represented as a tuple (state, action, reward, value), e.g., [s1, s2, s3, ..., a1, a2, ..., r, v]. The total number of variables in a transition is transition_dim = observation_dim + action_dim + reward_dim + value_dim. The number of consecutive transitions used for training is referred to as n_transitions.\nSequence/Trajectory: A sequence of multiple transitions that serve as input to the model, e.g., [s1, s2, s3, a1, a2, r, v, s1, s2, s3, a1, a2, r, v, ...]. In TT, trajectories are modeled as discrete token sequences.\nEpisode: A complete run from an initial state to termination. A sequence is a subset of an episode, but TT trains on multiple episodes to learn generalizable trajectory patterns.\nRollout: The process of generating a trajectory by autoregressively sampling tokens (states, actions, rewards) using TT. Rollouts can be greedy (deterministic), stochastic (using temperature or top-k sampling), or beam-searched (optimizing reward-to-go). TT rollouts allow policy evaluation, trajectory forecasting, and planning in offline RL."
  },
  {
    "objectID": "content/offline_rl.html#hyperparameters",
    "href": "content/offline_rl.html#hyperparameters",
    "title": "Trajectory Transformer (TT) from code",
    "section": "5 Hyperparameters",
    "text": "5 Hyperparameters\n\n# Model parameters\n# n_transitions is the number of transitions in a sequence, one transition is (s, a, r, v)\nn_transitions = 10\n# seq_len is the length of the sequence as seen by the model\nseq_len = n_transitions * transition_dim\n# vocab_size is the number of bins used for discretization\n# it also represents the size of the vocabulary for the embedding\nvocab_size = 100\nmax_bins = vocab_size\ndiscount_factor = 0.99\nembedding_dim = 128 if not local else 32\n# number of heads in the multihead attention\nn_heads = 4 if not local else 4\n# number of blocks in the transformer\nn_blocks = 4 if not local else 4\nn_epochs = 70 if not local else 5\nbatch_size = 256 if not local else 128\n# whether to use separate heads for each transition dimension\nuse_sep_heads = True\nlr = 0.0006\nweight_decay = 0.1\nbetas = (0.9, 0.95)\nclip_grad = 1.0\n# how often to evaluate the model. During evaluation, the model is used during rollouts\neval_every = 5 if not local else 5\nstrategy = \"uniform\"  # \"quantile\" or \"uniform\" for discritization\n\n\n\nSetup data directories\n# create a directory to save the model\nbase_dir = f\"data/{dataset_ref}\"\ncheckpoint_path = f\"{base_dir}/\"\nload_checkpoint = (\n    False  # set to False if you want to train from scratch even if a checkpoint exists\n)\n\n# if load_checkpoint is False and a checkpoint exists, delete it\nif load_checkpoint is False and os.path.exists(checkpoint_path):\n    # remove only model checkpoints\n    for f in os.listdir(checkpoint_path):\n        if f.startswith(\"model\"):\n            os.remove(os.path.join(checkpoint_path, f))"
  },
  {
    "objectID": "content/offline_rl.html#discretizer",
    "href": "content/offline_rl.html#discretizer",
    "title": "Trajectory Transformer (TT) from code",
    "section": "6 Discretizer",
    "text": "6 Discretizer\n\n6.1 What is Discretization?\nDiscretization is the process of converting continuous variables (e.g., states, actions, rewards) into discrete tokens that can be processed by a model designed for categorical data, such as a Transformer. Instead of representing values as continuous numbers, they are quantized into a fixed set of bins, allowing TT to model RL problems as a sequence modeling task, similar to how language models process words.\n\n\n\nhttps://fritz.ai/hands-on-with-feature-engineering-techniques-variable-discretization/\n\n\n\n\n6.2 Why is Discretization Needed in TT?\nThe TT applies autoregressive sequence modeling, which typically operates over discrete token sequences (like words in NLP). Since RL states and actions are usually continuous, discretization is necessary to make them compatible with Transformer architectures.\n\n\n\n\n\n\nNote\n\n\n\nAutoregressive sequence modeling is a method where each element in a sequence is predicted based on the previous elements, one step at a time, allowing the model to generate or forecast sequences by building on its own prior outputs.\n\n\n\n\n6.3 Key reasons for discretization in TT:\n\nEnables Direct Application of Transformers: Transformers require discrete inputs, and discretization allows TT to treat RL data like a language modeling problem.\nImproves Long-Horizon Prediction: Standard RL models often struggle with compounding errors in continuous spaces. Discretization reduces error accumulation and improves stability in trajectory prediction.\nAvoids Gaussian Assumptions: Traditional model-based RL methods assume Gaussian-distributed transitions, which can limit expressivity. TT, using discretization, models more complex distributions without restrictive assumptions.\nUnifies State, Action, and Reward Modeling: Discretizing all components enables TT to model joint distributions over states, actions, and rewards, leading to better trajectory optimization.\n\n\n\n6.4 How Does Discretization Work in TT?\nEach continuous state and action dimension is divided into a fixed number of bins (tokens). TT then models trajectories as sequences of these discrete tokens. The paper explores two discretization methods:\n\nUniform Discretization:\n\nDivides the range of each variable into equally spaced intervals.\nPreserves Euclidean distance but can be sensitive to outliers.\n\nQuantile Discretization:\n\nDivides data so that each bin contains an equal number of data points.\nEnsures all tokens are well-represented in the dataset, improving learning stability.\n\n\n\n\n6.5 How TT Uses Discretized Inputs\n\nA trajectory (state, action, reward, value, etc.) is broken into discrete tokens.\nThe Transformer learns sequence patterns over these tokens, predicting the most probable future trajectory.\nDuring inference, TT generates the next token step-by-step, similar to how language models predict the next word.\n\n\n\n6.6 Bin Size trade offs\n\nChoosing a large bin size reduces reconstruction loss, preserving more information from continuous inputs. However, this increases the number of discrete tokens, requiring a larger vocabulary size for the embedding map. While this gives the model more expressive capacity, it comes at the cost of higher memory usage and longer training time.\nConversely, choosing a small bin size results in fewer tokens, leading to faster training and lower memory requirements. However, this comes at the cost of higher reconstruction loss, causing a loss of precision in trajectory representation and potentially degrading model performance.\nThe trade-off lies in balancing model expressiveness and training efficiency while minimizing information loss.\n\nBy discretizing continuous RL data, TT effectively applies Transformer-based sequence modeling to RL, achieving strong long-horizon planning and offline RL performance.\n\nclass KBinsDiscretizer:\n    \"\"\"\n    This class is responsible for encoding and decoding continuous values into discrete bins.\n    Number of bins are fixed for all the features.\n    \"\"\"\n\n    def __init__(self, dataset: np.ndarray, n_bins: int, strategy: str = \"ordinal\"):\n        self.n_bins = n_bins\n        self.strategy = strategy\n\n        # bin_edges shape: (n_features, n_bins + 1)\n        self.bin_edges = self._find_bin_edges(dataset)\n        # bin_centers shape: (n_features, n_bins)\n        self.bin_centers = (self.bin_edges[:, :-1] + self.bin_edges[:, 1:]) * 0.5\n        self.bin_centers_torch = torch.from_numpy(self.bin_centers).float()\n\n    def _find_bin_edges(self, dataset: np.ndarray):\n        # dataset shape: (n_samples, n_features)\n        bin_edges = []\n        if self.strategy == \"uniform\":\n            # min and max values for each feature, shpae: (n_features,)\n            mins, maxs = np.min(dataset, axis=0), np.max(dataset, axis=0)\n            # bin_edges shape: (n_features, n_bins + 1)\n            bin_edges = np.linspace(mins, maxs, self.n_bins + 1).T\n        elif self.strategy == \"quantile\":\n            quantiles = np.linspace(0, 100, self.n_bins + 1)\n            # bin_edges shape: (n_features, n_bins + 1)\n            bin_edges = np.percentile(dataset, quantiles, axis=0).T\n        else:\n            raise ValueError(f\"Unknown strategy: {self.strategy}\")\n        return bin_edges\n\n    def encode(\n        self, X: np.ndarray, subslice: Optional[Tuple[int, int]] = None\n    ) -&gt; np.ndarray:\n        # use subslice to encode only a part of the features in the X\n        if X.ndim == 1:\n            # this is to handle the case where we have a single feature\n            X = X[None]\n        # data shape: (n_samples, n_features)\n        edges = self.bin_edges\n        if subslice is not None:\n            start, end = subslice\n            edges = edges[start:end]\n\n        # Xt represents discretized data, shape: (n_samples, n_features)\n        Xt = np.zeros(X.shape, dtype=np.long)\n\n        # See documentation of numpy.isclose for an explanation of ``rtol`` and ``atol``.\n        rtol = 1.0e-5\n        atol = 1.0e-8\n\n        for jj in range(X.shape[1]):\n            # Values which are close to a bin edge are susceptible to numeric\n            # instability. Add eps to X so these values are binned correctly\n            # with respect to their decimal truncation.\n            eps = atol + rtol * np.abs(X[:, jj])\n            # why [1:]? bins = edges - 1, but its unclear why we leave out the first element and not the last\n            Xt[:, jj] = np.digitize(X[:, jj] + eps, edges[jj][1:])\n\n        # clip the values to be within the range [0, n_bins - 1]\n        np.clip(Xt, 0, self.n_bins - 1, out=Xt)\n\n        return Xt\n\n    def decode(\n        self, Xt: np.ndarray, subslice: Optional[Tuple[int, int]] = None\n    ) -&gt; np.ndarray:\n        # use subslice to decode only a part of the features in the Xt\n        if Xt.ndim == 1:\n            # this is to handle the case where we have a single feature\n            Xt = Xt[None]\n        # data shape: (n_samples, n_features)\n        centers = self.bin_centers\n        if subslice is not None:\n            start, end = subslice\n            centers = centers[start:end]\n\n        X = np.zeros(Xt.shape, dtype=np.float64)\n        for jj in range(Xt.shape[1]):\n            X[:, jj] = centers[jj, np.int_(Xt[:, jj])]\n\n        return X\n\n    def expectation(\n        self, probs: np.ndarray, subslice: Optional[Tuple[int, int]] = None\n    ) -&gt; np.ndarray:\n        # given the probabilities of each bin, calculate the expectation of the feature values\n        # perticularly useful when we have a distribution over the bins, maybe from a model after softmax\n        # from logits.\n        # probs shape: (n_samples, n_features, n_bins)\n        if probs.ndim == 1:\n            # this is to handle the case where we have a single feature\n            probs = probs[None]\n        # probs shape: (batch_size, n_features, n_bins)\n        # bin_centers shape: (n_features, n_bins) -&gt; (1, n_features, n_bins)\n        if torch.is_tensor(probs):\n            bin_centers = self.bin_centers_torch.unsqueeze(0)\n        else:\n            # bin_centers shape: (n_features, n_bins) -&gt; (1, n_features, n_bins)\n            bin_centers = np.expand_dims(self.bin_centers, axis=0)\n\n        if subslice is not None:\n            start, end = subslice\n            bin_centers = bin_centers[:, start:end]\n\n        # use formula E[X] = sum(p(x) * x) for all x\n        # (batch_size, n_features, n_bins) * (1, n_features, n_bins) -&gt; sum (batch_size, n_features, n_bins) -&gt; (batch_size, n_features)\n        X = (probs * bin_centers).sum(axis=-1)\n        return X\n\n    def to(self, device):\n        self.bin_centers_torch = self.bin_centers_torch.to(device)\n\n\n\nTest the KBinsDiscretizer class\n# Test array\ntest_arr = np.array([[1, 2], [3, 4], [5, 6]])\n\n# Initialize the discretizer\ndiscretizer = KBinsDiscretizer(test_arr, 1000, strategy=\"uniform\")\n\n# Encode and decode the test array\nencoded = discretizer.encode(test_arr)\ndecoded = discretizer.decode(encoded)\n\n# Check if the decoded array is close to the original array\nassert np.isclose(\n    decoded, test_arr, atol=1e-2\n).all(), f\"Decoded array {decoded} is not close to the original array {test_arr}\"\n\n# Generate random probabilities\nprobs = F.softmax(torch.from_numpy(np.random.rand(3, 2, 1000)), dim=-1).numpy()\n\n# Calculate the expectation\nexpectation = discretizer.expectation(probs)\n\n# Check if the expectation is close to the mean of the test array\nexpected_mean = np.tile(np.mean(test_arr, axis=0), (3, 1))\nassert np.isclose(\n    expectation, expected_mean, atol=1e-1\n).all(), f\"Expectation {expectation} is not close to the expected mean {expected_mean}\"\n\nprint(\"All tests passed successfully.\")\n\n\nAll tests passed successfully."
  },
  {
    "objectID": "content/offline_rl.html#pytorch-dataset",
    "href": "content/offline_rl.html#pytorch-dataset",
    "title": "Trajectory Transformer (TT) from code",
    "section": "7 Pytorch Dataset",
    "text": "7 Pytorch Dataset\nMinari datasets store offline RL trajectories, nore specifically it contain list of episodes, but they need to be structured into a PyTorch-compatible format for training of the TT. This conversion is essential for:\n\nBatch Processing: PyTorch’s Dataset and DataLoader enable efficient mini-batch training, improving speed and scalability.\nDiscretization & Sequence Formatting: TT requires discretized trajectory sequences, which involves converting continuous states, actions, and rewards into discrete tokens that can be processed like a language model.\nLoss Masking & Padding: Not all trajectories are of the same length. The conversion ensures proper padding and masking, preventing short sequences from corrupting training.\nEfficient Sampling: Instead of loading full trajectories, PyTorch datasets allow sampling smaller transition sequences, making training more memory-efficient.\n\n\n7.1 Rewards-to-Go (RTG)\nRewards-to-Go (RTG) is the discounted cumulative sum of future rewards from a given time step until the end of the trajectory. It represents the expected return from a state, assuming the agent follows the observed trajectory.\n\n7.1.1 Formula:\n\\[\nR_t = r_{t} + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\ldots + \\gamma^{T-t} r_T\n\\]\nwhere \\((\\gamma)\\) (discount factor) determines how much future rewards influence current decisions.\n\n\n7.1.2 Why is RTG Important in TT?\nRTG is only relevant for reward-maximizing beam search. In contrast, it is not used during imitation learning or goal-conditioned RL, where the focus is on replicating behavior or reaching a target state rather than optimizing for rewards.\n\nEncodes Long-Term Future Returns: Helps the model learn which actions lead to high rewards over time.\nNo Need for Value Functions: Unlike traditional RL, TT doesn’t learn separate value functions; RTG is used directly as input.\nImproves Planning: During inference, TT biases beam search towards high RTG sequences, optimizing decision-making.\n\nBy including RTG as a token in TT, the model can predict high-reward trajectories more effectively, making it powerful for offline RL and long-horizon planning.\n\ndef join_trajectory(env: Env, episode: EpisodeData, discount: float = 0.99):\n    # Convert the object of type EpisodeData to a numpy array. EpisodeData\n    # contains the following fields: observations, actions, rewards, other\n    # and each of these fields is a numpy array. We need to concatenate\n    # these arrays along the last axis to get a single array for each time.\n\n    success = episode.terminations\n    # end of the trajectory is the first success or the end of the episode\n    success_indices = np.where(success)[0]\n    if len(success_indices) &gt; 0:\n        last_success_idx = success_indices[0]\n        trajectory_len = last_success_idx + 1\n    else:\n        last_success_idx = -1\n        trajectory_len = len(episode.rewards)\n    # shape (trajectory_len, observation_dim)\n    observations = episode.observations\n    # shape (trajectory_len, action_dim)\n    actions = episode.actions\n    # shape (trajectory_len, action_dim)\n    rewards = episode.rewards[:trajectory_len]\n\n    # use values to store the rewards to go\n    # for a given time step, the value is the sum of rewards from that time step\n    # to the end of the trajectory, discounted by discount factor at each time step\n    values = np.zeros_like(rewards, dtype=np.float32)\n    # calculate discounts for each time step\n    discounts = discount ** np.arange(trajectory_len)\n    # calculate rewards to go with discount\n    for t in range(trajectory_len):\n        values[t] = (rewards[t + 1 :].T * discounts[: -t - 1]).sum()\n\n    # drop the last state because we don't have a reward for it\n    states = flatten_space(observations, env.observation_space)\n    states = states[:trajectory_len]\n    actions = flatten_space(actions, env.action_space)\n    actions = actions[:trajectory_len]\n    rewards = rewards[:, None]\n    values = values[:, None]\n\n    # shape (trajectory_len, observation_dim + action_dim + reward_dim + value_dim)\n    joined = np.concatenate([states, actions, rewards, values], axis=-1)\n\n    return joined\n\n\nclass DiscretizeDataset(Dataset):\n    # Each input into the sequence model needs to be (batch_size, tokens)\n    # output should be in groups of transitions\n    def __init__(\n        self,\n        env: Env,\n        m_dataset: MinariDataset,\n        n_transitions: int,\n        discount: float = 0.99,\n        max_bins: int = 1000,\n        strategy: str = \"quantile\",\n        cache_path: Optional[str] = None,\n        load_checkpoint: bool = True,\n    ):\n        self.m_dataset = m_dataset\n        self.n_transitions = n_transitions\n\n        ds_len = len(self.m_dataset)\n\n        self.cache_name = (\n            f\"joined_trajectories_{n_transitions}_{max_bins}_{strategy}_{ds_len}.pkl\"\n        )\n        cache_path = os.path.join(cache_path, self.cache_name) if cache_path else None\n\n        if load_checkpoint and cache_path is not None and os.path.exists(cache_path):\n            print(f\"Loading cached dataset from {cache_path}\")\n            with open(cache_path, \"rb\") as f:\n                self.joined_trajectories = pickle.load(f)\n        else:\n            # this list will contain the joined trajectories, each item in the list\n            # is a trajectory of shape (trajectory_len, observation_dim + action_dim + reward_dim + value_dim)\n            # and that trajectory is one episodedata from the m_dataset\n            self.joined_trajectories = []\n            for episode in m_dataset:\n                self.joined_trajectories.append(join_trajectory(env, episode, discount))\n\n            print(f\"Caching dataset to {cache_path}\")\n            with open(cache_path, \"wb\") as f:\n                pickle.dump(self.joined_trajectories, f)\n\n        self.discretizer = KBinsDiscretizer(\n            n_bins=max_bins,\n            strategy=strategy,\n            # concatenate all the trajectories\n            # shape (n_samples * trajectory_len, observation_dim + action_dim + reward_dim + value_dim)\n            dataset=np.concatenate(self.joined_trajectories, axis=0),\n        )\n\n        # we need a dataset for training sequence model\n        # given that we need a sequence of n_transitions, we need to generate\n        # indices such that we can get n_transitions from each trajectory\n        indices = []\n        for traj_idx, joined_trajectory in enumerate(self.joined_trajectories):\n            traj_len = joined_trajectory.shape[0]\n            end = traj_len - 1\n            for i in range(end):\n                indices.append((traj_idx, i, i + n_transitions))\n\n        self.indices = np.array(indices)\n\n    def __len__(self):\n        return len(self.indices)\n\n    def __getitem__(self, idx):\n        traj_idx, start, end = self.indices[idx]\n        # sample a sequence of n_transitions from trajectory at traj_idx\n        joined = self.joined_trajectories[traj_idx][start:end]\n        loss_pad_mask = np.ones((self.n_transitions, joined.shape[-1]), dtype=np.long)\n        # some sequences may be shorter than n_transitions, pad them with zeros\n        # and set the mask to zero for the padded part, this mask will be used\n        # to mask the loss when calculating the loss\n        if joined.shape[0] &lt; self.n_transitions:\n            # pad along dimension 0, zero padding at the beginning\n            # and (self.n_transitions - joined.shape[0]) padding at the end\n            joined = np.pad(\n                joined,\n                ((0, self.n_transitions - joined.shape[0]), (0, 0)),\n                mode=\"constant\",\n                constant_values=0,\n            )\n            loss_pad_mask[joined.shape[0] :] = 0\n\n        # since transformer model expects discrete values, we need to encode the\n        # continuous values into discrete bins\n        # shape (n_transitions, transition_dim) -&gt; (n_transitions, transition_dim)\n        joined_discretized = self.discretizer.encode(joined)\n        # shape (n_transitions, transition_dim) -&gt; (n_transitions * transition_dim)\n        # i'e [s1, a1, r1, v1, s2, a2, r2, v2, ...]\n        joined_discretized = joined_discretized.reshape(-1).astype(np.long)\n        loss_pad_mask = loss_pad_mask.reshape(-1)\n        # return input, target, and mask\n        # since sequence model predicts the next token, target is the next token in the sequence\n        return joined_discretized[:-1], joined_discretized[1:], loss_pad_mask[:-1]\n\n\ndataset = DiscretizeDataset(\n    env=env,\n    m_dataset=m_dataset,\n    n_transitions=n_transitions,\n    discount=discount_factor,\n    max_bins=max_bins,\n    strategy=strategy,\n    cache_path=checkpoint_path,\n    load_checkpoint=load_checkpoint,\n)\n\nprint(f\"Length of dataset: {len(dataset)}\")\nprint(f\"Shape of input: {dataset[0][0].shape}\")\nprint(f\"Shape of target: {dataset[0][1].shape}\")\nprint(f\"Shape of mask: {dataset[0][2].shape}\")\n\nCaching dataset to data/mujoco/halfcheetah/expert-v0/joined_trajectories_10_100_uniform_10.pkl\nLength of dataset: 9990\nShape of input: (249,)\nShape of target: (249,)\nShape of mask: (249,)\n\n\nLet’s inspect one input, target, and mask. The call to dataset[0] fetches the first record from the dataset. For brevity, we will show only the first 10 elements. Notice how the target is simply the input shifted by 1. This approach ensures compatibility with sequence modeling, similar to how it is done in NLP. Mask value of 1 signifies that the token should be part of loss calculation.\n\n\nTest the DiscretizeDataset class\n# show input, target, and mask for the first item in the dataset\nprint(f\"Input: {dataset[0][0][:10]}\")\nprint(f\"Target: {dataset[0][1][:10]}\")\nprint(f\"Mask: {dataset[0][2][:10]}\")\n\n\nInput: [38 34 42 55 40 56 43 48  5 41]\nTarget: [34 42 55 40 56 43 48  5 41 38]\nMask: [1 1 1 1 1 1 1 1 1 1]\n\n\n\n\nutility functions\ndef round_to_multiple(number, multiple):\n    \"\"\"\n    Rounds a given number up to the nearest multiple of a specified value.\n\n    Args:\n        number (int or float): The number to be rounded.\n        multiple (int or float): The multiple to which the number should be rounded.\n\n    Returns:\n        int or float: The number rounded up to the nearest multiple of the specified value.\n    \"\"\"\n    pad = (multiple - number % multiple) % multiple\n    return number + pad\n\n\n# Test the round_to_multiple function\nassert round_to_multiple(5, 3) == 6\nassert round_to_multiple(6, 3) == 6\nassert round_to_multiple(7, 3) == 9\n\n\n\n\nSchedule the learning rate - linear warmup and cosine decay karpathy/minGPT\ndef weight_decay_groups(\n    model, whitelist_modules, blacklist_modules, blacklist_named=None\n):\n    # from https://github.com/karpathy/minGPT\n    decay, no_decay = set(), set()\n\n    for mn, m in model.named_modules():\n        for pn, p in m.named_parameters():\n            fpn = \"%s.%s\" % (mn, pn) if mn else pn  # full param name\n\n            # starts with for rnn's, endswith other\n            if pn.startswith(\"bias\") or pn.endswith(\"bias\"):\n                # all biases will not be decayed\n                no_decay.add(fpn)\n            elif (pn.startswith(\"weight\") or pn.endswith(\"weight\")) and isinstance(\n                m, blacklist_modules\n            ):\n                # weights of blacklist modules will NOT be weight decayed\n                no_decay.add(fpn)\n            elif (pn.startswith(\"weight\") or pn.endswith(\"weight\")) and isinstance(\n                m, whitelist_modules\n            ):\n                # weights of whitelist modules will be weight decayed\n                decay.add(fpn)\n\n    if blacklist_named is not None:\n        for name in blacklist_named:\n            no_decay.add(name)  # also no decay\n\n    # validate that we considered every parameter\n    param_dict = {pn: p for pn, p in model.named_parameters()}\n    inter_params = decay & no_decay\n    union_params = decay | no_decay\n    if len(inter_params) != 0:\n        warnings.warn(\n            f\"parameters {str(inter_params)} made it into both decay/no_decay sets! They will be added to only no_decay by default.\"\n        )\n        decay = decay - no_decay\n\n    inter_params = decay & no_decay\n    union_params = decay | no_decay\n    if len(param_dict.keys() - union_params) != 0:\n        warnings.warn(\n            f\"parameters {str(param_dict.keys() - union_params)} were not separated into either decay/no_decay set! They will be added to decay by default.\"\n        )\n        decay = decay | (param_dict.keys() - union_params)\n\n    optim_groups = {\n        \"decay\": [param_dict[pn] for pn in sorted(list(decay))],\n        \"nodecay\": [param_dict[pn] for pn in sorted(list(no_decay))],\n    }\n    return optim_groups\n\n\nclass GPTScheduler:\n    \"\"\"\n    Linear warmup to optimizer inital_lr for #warmup_tokens,\n    then cosine decay to inital_lr * final_lr_ratio for the rest #final_tokens\n    source: https://github.com/karpathy/minGPT\n    \"\"\"\n\n    def __init__(\n        self, optimizer, warmup_tokens, final_tokens, final_lr_ratio=0.1, decay=True\n    ):\n        self.optimizer = optimizer\n        # assuming that lr same for all group\n        self.init_lr = optimizer.param_groups[0][\"lr\"]\n\n        self.warmup_tokens = warmup_tokens\n        self.final_tokens = final_tokens\n        self.final_lr_ratio = final_lr_ratio\n        self.decay = decay\n\n        self.tokens_count = 0.0\n\n    def step(self, batch_size):\n        lr_mult = self.__get_lr_multiplier(batch_size)\n\n        for group in self.optimizer.param_groups:\n            group[\"lr\"] = self.init_lr * lr_mult\n\n    def get_current_lr(self):\n        lr_mult = self.__get_lr_multiplier(0.0)\n        return self.init_lr * lr_mult\n\n    def __get_lr_multiplier(self, batch_size):\n        self.tokens_count += batch_size\n\n        assert (\n            self.tokens_count &lt;= self.final_tokens\n        ), f\"number of tokens {self.tokens_count} already bigger than number of tokens for one cycle\"\n\n        if self.tokens_count &lt; self.warmup_tokens:\n            lr_mult = float(self.tokens_count) / float(max(1, self.warmup_tokens))\n        elif self.tokens_count &gt;= self.warmup_tokens and self.decay:\n            tokens_passed = self.tokens_count - self.warmup_tokens\n            tokens_left = self.final_tokens - self.warmup_tokens\n\n            progress = float(tokens_passed) / float(max(1, tokens_left))\n            lr_mult = max(\n                self.final_lr_ratio, 0.5 * (1.0 + math.cos(math.pi * progress))\n            )\n        else:\n            lr_mult = 1.0\n\n        return lr_mult\n\n    def state_dict(self):\n        # just for checkpoint callback\n        pass\n\n\ndef get_optimizer(model, weight_decay, learning_rate, betas):\n    param_groups = weight_decay_groups(\n        model=model,\n        whitelist_modules=(torch.nn.Linear, torch.nn.MultiheadAttention, EinLinear),\n        blacklist_modules=(torch.nn.LayerNorm, torch.nn.Embedding),\n        blacklist_named=(\"positional_embedding\",),\n    )\n    optim_groups = [\n        {\"params\": param_groups[\"decay\"], \"weight_decay\": weight_decay},\n        {\"params\": param_groups[\"nodecay\"], \"weight_decay\": 0.0},\n    ]\n    optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas)\n\n    return optimizer\n\n\ndef get_scheduler(optimizer, warmup_tokens, final_tokens):\n    scheduler = GPTScheduler(\n        optimizer,\n        warmup_tokens=warmup_tokens,\n        final_tokens=final_tokens,\n        decay=True,\n    )\n    return scheduler"
  },
  {
    "objectID": "content/offline_rl.html#model",
    "href": "content/offline_rl.html#model",
    "title": "Trajectory Transformer (TT) from code",
    "section": "8 Model",
    "text": "8 Model\n\n\n\nTT Model\n\n\n\n8.1 Tokenization in TT: Why and How Offsetting is Done\nIn traditional NLP Transformers, each token comes from a single vocabulary (e.g., words or subwords). However, in TT, we deal with multiple types of tokens—states, actions, rewards, and values—each with a shared vocabulary size (determined by the number of discretization bins max_bins).\nSince we have multiple token types, we need a way to differentiate them. This is done using offsetting, where each token type is assigned a different section of the vocabulary. Kinda of like local vocab and global vocab.\n\n8.1.1 How Offsetting Works\nFor a given vocab_size, the tokens are offset by multiples of it:\n\\[\n\\text{offsets} = [0, \\text{vocab\\_size}, 2 \\times \\text{vocab\\_size}, \\dots]\n\\]\nFor example, if vocab_size = 10 and we have 4 token types (state, action, reward, value), the token indices are adjusted as follows:\n\nState tokens: [0-9]\nAction tokens: [10-19]\nReward tokens: [20-29]\nValue tokens: [30-39]\n\nThis ensures that each token type is uniquely represented, preventing mix-ups between different types of information.\n\n\n\n8.2 EinLinear: Einstein Notation as a Shorthand for MLP\nEinstein summation notation (einsum) is a compact and efficient way of writing tensor operations. In EinLinear, we use torch.einsum() instead of the standard torch.matmul(), making the computation more explicit and flexible.\n\n8.2.1 How EinLinear Works\nA regular MLP applies a linear transformation:\n\\[\nY = XW^T + b\n\\]\nFor multiple models (e.g., if we use separate linear layers for different token types), EinLinear performs batched linear transformations efficiently:\noutput = torch.einsum(\"eoi,bei-&gt;beo\", self.weight, x)\nwhere:\n\neoi represents the weight tensor (n_models, out_features, in_features).\nbei represents the input tensor (batch_size, n_models, in_features).\n\nThe result is (batch_size, n_models, out_features).\nThis is just a compact way of writing matrix multiplication across multiple models, saving computation time and improving clarity.\n\n\n\n8.3 Causal Masking: Why and How?\nTransformers process entire sequences at once, but in RL, future information must not be leaked to the model during training. This is handled via causal masking.\n\n8.3.1 Types of Masking in TT\n\n8.3.1.1 Self-Causal Masking\n\nEnsures each token only attends to past tokens.\nImplemented using a lower triangular mask (future tokens are masked out).\nPrevents information leakage during training.\n\n\n\n8.3.1.2 Value Masking\n\nIn TT, reward-to-go (RTG) is included as a token.\nRTG contains future information, so it must not be attended to by other tokens.\nThe mask explicitly blocks RTG tokens during self-attention.\n\n\n\n\n\n8.4 KV Cache: Why and How It Works\nIn inference, processing one token at a time (autoregressive decoding) is inefficient if we recompute self-attention for all previous tokens at every step. Key-Value (KV) Caching solves this by storing past computations, reducing redundant work.\n\n8.4.1 How KV Cache Works\n\nDuring inference, previously computed key-value (K-V) pairs are stored.\nWhen a new token arrives, we append its K-V pairs instead of recomputing for all tokens.\nThis makes generation much faster.\n\n\nclass Block(nn.Module):\n    # Transformer block\n    def __init__(\n        self,\n        seq_len,\n        embedding_dim: int,\n        transition_dim: int,\n        n_heads: int,\n        attention_dropout: float,\n        residual_dropout: float,\n    ):\n        super().__init__()\n        self.attn = nn.MultiheadAttention(\n            embedding_dim, n_heads, batch_first=True, dropout=attention_dropout\n        )\n        self.attn_norm = nn.LayerNorm(embedding_dim)\n\n        self.fc_norm = nn.LayerNorm(embedding_dim)\n\n        self.drop = nn.Dropout(residual_dropout)\n\n        self.mlp = nn.Sequential(\n            nn.Linear(embedding_dim, embedding_dim * 4),\n            nn.GELU(),\n            nn.Linear(embedding_dim * 4, embedding_dim),\n            nn.Dropout(residual_dropout),\n        )\n        self.seq_len = seq_len\n\n        # mask value of true means that the value is not allowed to be attended to\n        mask = ~torch.tril(torch.ones(seq_len, seq_len)).bool()\n        # transition_dim - 1 stores rewards to go, we don't want to attend to them because they contain future information\n        mask[:, transition_dim - 1 :: transition_dim] = True\n        self.register_buffer(\"mask\", mask)\n\n    def forward(\n        self, x: torch.Tensor, kv_cache: Optional[torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        # x shape (batch_size, n_tokens, embedding_dim) in prefill mode else (batch_size, 1, embedding_dim)\n        # kv_cache shape (batch_size, n_tokens, embedding_dim) in inference mode else None\n        _, n_tokens, _ = x.shape\n\n        # normalize the input before passing it to the attention layer\n        x_norm = self.attn_norm(x)\n\n        if kv_cache is None:\n            # when kv_cache is None, we are in prefill mode\n\n            # attn_mask shape (seq_len, seq_len), but incoming shape is (batch_size, n_tokens, embedding_dim)\n            # so filter the mask to the correct size (n_tokens, n_tokens)\n            attn_mask = self.mask[:n_tokens, :n_tokens]\n            q, k, v = x_norm, x_norm, x_norm\n        else:\n            assert n_tokens == 1, \"kv_cache can only be None with a single token\"\n            # +1 because we are adding a new token\n            assert kv_cache.shape[1] + 1 &lt;= self.seq_len, \"kv_cache is too large\"\n\n            # attn_mask is None because we are running in inference mode, processing one token at a time\n            # and this token is not allowed to attend to future tokens\n            attn_mask = None\n            q, k, v = (\n                x_norm,\n                # shape (batch_size, n_tokens + 1, embedding_dim)\n                torch.cat([kv_cache, x_norm], dim=1),\n                torch.cat([kv_cache, x_norm], dim=1),\n            )\n\n        new_kv_cache = k\n\n        # x shape (batch_size, n_tokens, embedding_dim) in prefill mode else (batch_size, 1, embedding_dim)\n        x = x + self.drop(\n            self.attn(q, k, v, attn_mask=attn_mask, need_weights=False)[0]\n        )\n\n        x = x + self.mlp(self.fc_norm(x))\n\n        return x, new_kv_cache\n\n\nclass EinLinear(nn.Module):\n    def __init__(\n        self, n_models: int, in_features: int, out_features: int, bias: bool = True\n    ):\n        super().__init__()\n        self.n_models = n_models\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.Tensor(n_models, out_features, in_features))\n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(n_models, out_features))\n        else:\n            self.register_parameter(\"bias\", None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for i in range(self.n_models):\n            nn.init.kaiming_uniform_(self.weight[i], a=math.sqrt(5))\n            if self.bias is not None:\n                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight[i])\n                bound = 1 / math.sqrt(fan_in)\n                nn.init.uniform_(self.bias[i], -bound, bound)\n\n    def forward(self, x: torch.Tensor, model_idx: Optional[int] = None) -&gt; torch.Tensor:\n        if model_idx is None:\n            # when model_idx is None, we are in prefill mode\n            # (n_models, out_features, in_features) * (batch_size, n_models, in_features) -&gt; (batch_size, n_models, out_features)\n            output = torch.einsum(\"eoi,bei-&gt;beo\", self.weight, x)\n        else:\n            # when model_idx is not None, we are in inference mode\n            # shape (batch_size, in_features) * (out_features, in_features).T -&gt; (batch_size, out_features)\n            output = x @ self.weight[model_idx].T\n\n        if self.bias is not None:\n            raise RuntimeError()\n\n        return output\n\n\nclass TrajectoryTransformer(nn.Module):\n    def __init__(\n        self,\n        seq_len: int,\n        embedding_dim: int,\n        n_heads: int,\n        transition_dim: int,\n        n_blocks: int,\n        vocab_size: int,\n        dropout_embedding: float = 0.1,\n        attention_dropout: float = 0.1,\n        residual_dropout: float = 0.1,\n        use_sep_heads: bool = False,\n    ):\n        super().__init__()\n        self.seq_len = seq_len\n        self.embedding_dim = embedding_dim\n        self.n_heads = n_heads\n        self.transition_dim = transition_dim\n        self.n_blocks = n_blocks\n        self.vocab_size = vocab_size\n\n        # our input contains transition_dim types of tokens and each token is from a vocab of size vocab_size\n        # so the total number of tokens is transition_dim * vocab_size\n        self.token_embedding = nn.Embedding(\n            vocab_size * transition_dim, self.embedding_dim\n        )\n        # learnable positional embedding\n        self.positional_embedding = nn.Parameter(\n            torch.zeros(1, seq_len, self.embedding_dim)\n        )\n\n        self.dropout_embedding = nn.Dropout(dropout_embedding)\n\n        # create n_blocks of transformer blocks\n        self.blocks = nn.ModuleList(\n            [\n                Block(\n                    self.seq_len,\n                    self.embedding_dim,\n                    self.transition_dim,\n                    self.n_heads,\n                    attention_dropout,\n                    residual_dropout,\n                )\n                for _ in range(self.n_blocks)\n            ]\n        )\n\n        self.norm = nn.LayerNorm(self.embedding_dim)\n\n        self.use_sep_heads = use_sep_heads\n\n        if not self.use_sep_heads:\n            # project the output of the transformer to the vocab size\n            # since each token type is from a vocab of size vocab_size\n            # we can do this. But for instance if every token type used different\n            # number of bins, then we would have handled this differently. But dont worry\n            # that is not the case here.\n            self.fc = nn.Linear(self.embedding_dim, vocab_size)\n        else:\n            self.fc = EinLinear(\n                self.transition_dim, self.embedding_dim, vocab_size, bias=False\n            )\n\n        # self.apply is a crazy function that applies the given function recursively to every submodule\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        # standard practice in transformer models\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            torch.nn.init.xavier_uniform_(module.weight)\n            if isinstance(module, (nn.Linear)) and module.bias is not None:\n                torch.nn.init.constant_(module.bias, 0.0)\n        elif isinstance(module, nn.LayerNorm):\n            torch.nn.init.constant_(module.bias, 0.0)\n            torch.nn.init.constant_(module.weight, 1.0)\n        elif isinstance(module, TrajectoryTransformer):\n            torch.nn.init.normal_(module.positional_embedding, mean=0.0, std=0.02)\n\n    def get_seq_len(self):\n        return self.seq_len\n\n    def _pad_to_full_transition(self, tokens: torch.Tensor) -&gt; torch.Tensor:\n        # pad the tokens to full transition_dim\n        batch_size, n_tokens, _ = tokens.shape\n        n_pad = round_to_multiple(n_tokens, self.transition_dim) - n_tokens\n        padding = torch.zeros(\n            batch_size, n_pad, self.embedding_dim, device=tokens.device\n        )\n        x_pad = torch.cat([tokens, padding], dim=1)\n        return x_pad, n_pad\n\n    def _offset_tokens(\n        self, tokens: torch.Tensor, kv_caches: Optional[List] = None\n    ) -&gt; torch.Tensor:\n        # for beginners, this function may be a bit confusing. So let me explain\n\n        # our input consists of transition_dim types of tokens\n        # and each token is from a vocab of size vocab_size. So total\n        # there are transition_dim * vocab_size unique tokens. In contrast\n        # to NLP where we have just one token type(the word) and each word\n        # is from a vocab of size vocab_size (50k in llama).\n        # So to bridge this gap, we need to project each token's local vocab\n        # into the global vocab space. And the way we do this is by offsetting\n        # each token type by a factor of vocab_size.\n        # eg. if we have 3 token types and vocab_size is 10, then the tokens\n        # will be offset by [0, 10, 20] respectively.\n        # given input [2, 6, 3, 1, 2, 5] and vocab_size 10,\n        # the output will be [2, 16, 23, 11, 12, 25]\n\n        n_tokens = tokens.shape[1] if kv_caches is None else kv_caches[0].shape[1] + 1\n        # calculate the number of transitions in the input\n        n_transition = int(np.ceil(n_tokens / self.transition_dim))\n\n        # if transition_dim is 4, and vocab_size is 10, then the offsets will be\n        # [0, 10, 20, 30]\n        # shape (transition_dim,)\n        offsets = (\n            torch.arange(self.transition_dim, device=tokens.device) * self.vocab_size\n        )\n        # repeat the offset n_transition times\n        # shape (n_transition * transition_dim,)\n        offsets = offsets.repeat(n_transition)\n        if kv_caches is not None:\n            # in inference mode, we need to offset the last token only\n            offset_idx = offsets[:n_tokens][-1] + tokens\n        else:\n            # add the offsets to the tokens, and truncate the tokens to n_tokens\n            offset_idx = offsets[:n_tokens] + tokens\n        return offset_idx\n\n    def forward(\n        self, tokens: torch.Tensor, kv_caches: Optional[List] = None\n    ) -&gt; torch.Tensor:\n        # tokens shape (batch_size, n_tokens) in prefill mode else (batch_size, 1)\n        batch_size, n_tokens = tokens.shape\n        assert (\n            n_tokens &lt;= self.seq_len\n        ), f\"n_tokens {n_tokens} is greater than seq_len {self.seq_len}\"\n\n        if kv_caches is not None:\n            assert n_tokens == 1, \"kv_caches can only be used with a single token\"\n\n        # project each token into their vocab space, this is similar to tokenization\n        # in NLP where we project each word into their vocab space\n        # (batch_size, n_tokens)\n        offset_idx = self._offset_tokens(tokens, kv_caches)\n\n        # (batch_size, n_tokens) -&gt; (batch_size, n_tokens, embedding_dim)\n        tokens = self.token_embedding(offset_idx)\n\n        if kv_caches is not None:\n            # in inference mode\n            idx = kv_caches[0].shape[1]\n            # (1, 1, embedding_dim)\n            positional_embedding = self.positional_embedding[:, idx : idx + 1]\n        else:\n            # in prefill mode\n            # initialize kv_caches to None\n            kv_caches = [None for _ in range(self.n_blocks)]\n            # (1, n_tokens, embedding_dim)\n            positional_embedding = self.positional_embedding[:, :n_tokens]\n\n        # (batch_size, n_tokens, embedding_dim) -&gt; (batch_size, n_tokens, embedding_dim)\n        tokens = self.dropout_embedding(tokens + positional_embedding)\n\n        new_kv_caches = []\n        for block, kv_cache in zip(self.blocks, kv_caches):\n            tokens, new_kv_cache = block(tokens, kv_cache)\n            new_kv_caches.append(new_kv_cache)\n\n        # (batch_size, n_tokens, embedding_dim) -&gt; (batch_size, n_tokens, embedding_dim)\n        tokens = self.norm(tokens)\n\n        if self.use_sep_heads:\n            # by using separate heads, we can route each token type to a different module\n            # this can be useful when each token type uses different number of bins or\n            # we want to give more capacity for the model to learn.\n            if kv_caches[0] is None:\n                # in prefill mode, we need to calculate the logits for each token type\n                # (batch_size, n_tokens, embedding_dim) -&gt; (batch_size, n_tokens + n_pad, embedding_dim)\n                x_pad, n_pad = self._pad_to_full_transition(tokens)\n                # (batch_size, n_tokens + n_pad, vocab_size) -&gt; (batch_size * n_transitions, transition_dim, embedding_dim)\n                x_pad = x_pad.view(-1, self.transition_dim, self.embedding_dim)\n                # (batch_size * n_transitions, transition_dim, embedding_dim) -&gt; (batch_size * n_transitions, transition_dim, vocab_size)\n                logits = self.fc(x_pad, model_idx=None)\n                # (batch_size * n_transitions, transition_dim, vocab_size) -&gt; (batch_size, n_tokens + n_pad, vocab_size)\n                logits = logits.reshape(batch_size, n_tokens + n_pad, self.vocab_size)\n                # truncate the logits to n_tokens\n                logits = logits[:, :n_tokens, :]\n            else:\n                # in inference mode, we need to calculate the logits for the last token type\n                # infer the model index to route the token to the correct model.\n                cache_size = kv_cache[0].shape[1]\n                model_idx = cache_size % self.transition_dim\n                # (batch_size, 1, embedding_dim) -&gt; (batch_size, embedding_dim) -&gt; (batch_size, vocab_size) -&gt; (batch_size, 1, vocab_size)\n                logits = self.fc(tokens.squeeze(1), model_idx).unsqueeze(1)\n        else:\n            # (batch_size, n_tokens, embedding_dim) -&gt; (batch_size, n_tokens, vocab_size)\n            logits = self.fc(tokens)\n        return logits, new_kv_caches"
  },
  {
    "objectID": "content/offline_rl.html#sampling-techniques",
    "href": "content/offline_rl.html#sampling-techniques",
    "title": "Trajectory Transformer (TT) from code",
    "section": "9 Sampling Techniques",
    "text": "9 Sampling Techniques\nWhen generating sequences in TT, selecting the next token is crucial for controlling exploration vs. exploitation in trajectory rollouts. The code implements three sampling strategies:\n\n9.1 Greedy Sampling\n\n\n\nGreedy - huggingface\n\n\n\n9.1.1 How It Works:\n\nAlways picks the token with the highest probability (argmax).\nNo randomness; always chooses the most likely action.\n\n\n\n9.1.2 Pros:\n\nDeterministic (same input → same output).\nExploits the model’s knowledge effectively.\n\n\n\n9.1.3 Cons:\n\nCan lead to suboptimal decisions (local optima).\nLacks diversity—may get stuck in repetitive or unnatural trajectories.\n\n\n\n9.1.4 Use Case in TT:\n\nBest suited for deterministic policy rollouts where maximizing likelihood is preferred.\nNot ideal for exploration-driven RL tasks.\n\n\n\n\n9.2 Top-K Sampling\n\n9.2.1 How It Works:\n\nFilters logits by keeping only the top-K most probable tokens and sets the rest to -inf.\nSampling is then performed only among these top-K tokens.\n\n\n\n9.2.2 Pros:\n\nReduces randomness while still allowing diversity.\nPrevents sampling from extremely unlikely tokens.\n\n\n\n9.2.3 Cons:\n\nLimits exploration to the top-K tokens.\nRequires tuning K—too low may restrict diversity, too high may introduce poor samples.\n\n\n\n9.2.4 Use Case in TT:\n\nHelps balance exploration and exploitation, useful in stochastic policy rollouts.\nIdeal for trajectory prediction where diverse outcomes are needed.\n\n\n\n\n9.3 Temperature Scaling\n\n9.3.1 How It Works:\n\nAdjusts the sharpness of the probability distribution by dividing logits by a temperature factor (T) before applying softmax.\n\nHigh T (T &gt; 1.0): More uniform distribution (increases exploration).\nLow T (T &lt; 1.0): Peaked distribution (increases exploitation).\n\n\nBy combining these techniques, TT can generate diverse, high-quality trajectories while maintaining stability and performance.\n\ndef sample_token_from_logits(\n    logits: torch.Tensor,\n    temperature: float = 1.0,\n    greedy: bool = False,\n    top_k: Optional[int] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    This function return exactly one token from the logits.\n    We have options to sample from the logits using\n    1. Greedy sampling\n    2. Top-k sampling\n    3. Temperature scaling\n\n    \"\"\"\n    # logits shape (batch_size, vocab_size) representing the logits of the next token\n\n    # Apply temperature scaling, the higher the temperature, the more uniform the distribution\n    # the lower the temperature, the more peaked the distribution\n    if temperature != 1.0:\n        logits = logits / temperature\n\n    if top_k is not None:\n        # Apply top-k sampling\n        # (batch_size, vocab_size) -&gt; (batch_size, top_k)\n        v, indices = torch.topk(logits, top_k, dim=-1)\n\n        # Next instruction is a bit tricky, but it simply selects the top-k tokens\n        # set all logits to -inf except the top-k indices\n        # v[:, [-1]] might be a bit confusing, but it simply selects the last element\n        # along dim=1, and the result is a tensor of shape (batch_size, 1)\n        logits[logits &lt; v[:, [-1]]] = -float(\"Inf\")\n    # Calculate the probabilities from the logits\n    probs = F.softmax(logits, dim=-1)\n    if not greedy:\n        # Sample from the top-k indices\n        # (batch_size, top_k) -&gt; (batch_size, 1)\n        idx = torch.multinomial(probs, num_samples=1)\n    else:\n        # Greedy sampling\n        _, idx = torch.max(probs, dim=-1)\n    return idx\n\n\ndef sample_tokens(\n    model: nn.Module,\n    context: nn.Module,\n    kv_caches: Optional[List],\n    n_steps: int,\n    temperature: float = 1.0,\n    greedy: bool = False,\n    top_k: Optional[int] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Sample a sequence of tokens from the model.\n\n    Args:\n        model (nn.Module): The model to sample from.\n        context (nn.Module): The context to condition the sampling on.\n            shape (batch_size, n_tokens).\n        n_steps (int): The number of steps to sample.\n        temperature (float): The temperature scaling factor.\n        greedy (bool): Whether to sample greedily.\n        top_k (Optional[int]): The top-k sampling parameter.\n\n    Returns:\n        torch.Tensor: The sampled tokens.\n    \"\"\"\n    # tensor to store the logits of the next   sampled tokens\n    raw_logits = torch.zeros(\n        context.shape[0], n_steps, vocab_size, device=context.device\n    )\n    if kv_caches is None:\n        # when kv_caches is None, we are in prefilling step\n        logits, kv_caches = model(context, kv_caches)\n        # Sample the next token\n        # (batch_size, 1)\n        token = sample_token_from_logits(\n            logits[:, -1], temperature=temperature, greedy=greedy, top_k=top_k\n        )\n\n        context = torch.cat([context, token], dim=-1)\n\n        raw_logits[:, 0] = logits[:, -1]\n        # since we already did one step, we need to sample n_steps - 1\n        steps = range(1, n_steps)\n    else:\n        steps = range(n_steps)\n\n    for i in steps:\n        # crop the context so that it doesn't exceed the seq_len\n        curr_context_len = context.shape[1]\n        n_crop = round_to_multiple(\n            max(0, curr_context_len - model.get_seq_len()), transition_dim\n        )\n        if n_crop &gt; 0:\n            # since we are cropping from the left, we need to update the kv_caches\n            kv_caches = [kv[:, n_crop:] for kv in kv_caches]\n        # Get the model's prediction\n        # (batch_size, 1) -&gt; (batch_size, 1, vocab_size)\n        logits, kv_caches = model(context[:, -1:], kv_caches)\n        # Sample the next token\n        # (batch_size, 1)\n        token = sample_token_from_logits(\n            logits[:, -1], temperature=temperature, greedy=greedy, top_k=top_k\n        )\n\n        context = torch.cat([context, token], dim=-1)\n\n        raw_logits[:, i] = logits[:, -1]\n    return context, kv_caches, raw_logits"
  },
  {
    "objectID": "content/offline_rl.html#beam-search",
    "href": "content/offline_rl.html#beam-search",
    "title": "Trajectory Transformer (TT) from code",
    "section": "10 Beam Search",
    "text": "10 Beam Search\nBeam search is a search algorithm used in sequence generation tasks (e.g., NLP, trajectory modeling) to find the most probable sequence by maintaining multiple candidate sequences (beams) at each step. Instead of greedily picking the best token at each step (as in greedy decoding), beam search explores multiple top-k candidates, allowing better long-horizon decision-making.\n\n\n\nbeam search - huggingface\n\n\nIn TT, beam search is used to generate high-reward trajectories by optimizing reward-to-go (RTG) during rollout. The goal is to predict future actions that maximize cumulative rewards.\n\n10.1 How Beam Search Works in TT:\n\nInitialize multiple beams: Start with an initial state and maintain multiple possible trajectory candidates.\nExpand beams: At each step, sample the top-k most probable tokens (actions) based on model predictions.\nScore beams: Rank the expanded beams using reward-to-go (RTG) as the scoring function.\nPrune beams: Keep only the top-k highest-scoring beams, discarding the rest.\nRepeat until termination: Continue the process until a trajectory reaches the desired length or terminal state.\n\nThe implementation may appear complex because it supports vectorized/batched beam search, meaning the context tensor’s first dimension represents batch_size. This allows multiple independent rollouts to be processed in parallel, significantly improving efficiency.\n\n\n10.2 Understanding Stochasticity in Beam Search\nBeam search is typically deterministic, but in TT, stochasticity is introduced through parameters like:\n\nobs_top_k, act_top_k, rew_top_k: Restrict the set of sampled tokens to the top-k most probable ones, controlling exploration.\ntemperature: Adjusts how uniform or peaked the probability distribution is. Higher values encourage exploration, lower values favor exploitation.\ngreedy: If True, always picks the most probable token (deterministic); otherwise, sampling introduces randomness.\n\n\n10.2.1 Key Clarification:\n\nHigher stochasticity: More diverse plans are explored.\nLower stochasticity: The model converges to a narrower, more deterministic trajectory.\n\n\n\n\n10.3 Beam Search Objectives in TT\nBeam search in TT can be performed based on different objectives:\n\n10.3.1 Reward Maximization (Implemented Here)\n\nSelects plans that maximize Reward-to-Go (RTG).\nThe search prioritizes high-return trajectories, making this useful for decision-time planning in offline RL.\n\n\n\n10.3.2 Policy Cloning (Alternative Objective)\n\nMimics the dataset used to train TT.\nInstead of maximizing rewards, the model selects the most likely action from its learned distribution.\nAchieved by maximizing action likelihood rather than RTG.\nThis effectively reproduces the behavior of the dataset, rather than searching for the highest return.\n\n\n# This functions are probably the most important functions in this notebook and\n# also the most complex.\n\n\ndef vec_beam_plan(\n    model: nn.Module,\n    discretizer: KBinsDiscretizer,\n    context: torch.Tensor,\n    beam_width: int,\n    beam_steps: int,\n    beam_context: int,\n    sample_expansion: int,\n    observation_dim: int,\n    action_dim: int,\n    reward_dim: int,\n    value_dim: int,\n    transition_dim: int,\n    obs_top_k: Optional[int] = None,\n    act_top_k: Optional[int] = None,\n    rew_top_k: Optional[int] = None,\n    temperature: float = 1.0,\n    greedy: bool = False,\n) -&gt; torch.Tensor:\n    \"\"\"\n    In the most simplest terms, this function is responsible for planning a sequence of actions\n    that maximizes the expected rewards conditioned on the context.\n\n    It uses beam search to explore the space of possible plans. Beam search is a heuristic search\n    algorithm that explores a graph by expanding the most promising nodes in a limited set called the beam.\n\n    The concept of beam search is simple, but the implementation can be a bit tricky mainly because\n    we are processing multiple sequences in parallel. This is where the complexity comes from.\n    \"\"\"\n    batch_size = context.shape[0]\n    tokens_context_size = beam_context * transition_dim\n    n_crop = round_to_multiple(\n        max(0, context.shape[1] - tokens_context_size), transition_dim\n    )\n    context = context[:, n_crop:]\n    # context shape (batch_size, seq_len) -&gt; (beam_width, beam_width, seq_len)\n    plan = context.unsqueeze(1).repeat(1, beam_width, 1)\n\n    # tensor to store the rewards obtained from environment\n    # the +1 is non-intuitive, but it is because we need to store the value at t+1\n    # you will see this later.\n    rewards = torch.zeros(batch_size, beam_width, beam_steps + 1, device=context.device)\n    discounts = discount_factor ** torch.arange(beam_steps + 1, device=context.device)\n\n    # because beam plan start with a fresh context, we need to prefill the model\n    # first with the context, hence kv_caches is None\n    kv_caches = None\n    for t in trange(beam_steps, desc=\"Beam Search\", leave=False):\n        # sample_expansion is not strictly necessary, but it is used to increase the number of samples\n        #   which should allow us to explore more diverse plans. The reason this works is because the way\n        #   we sample tokens is stochastic, so by sampling more tokens, we are able to explore more diverse plans.\n        # (batch_size, beam_width, n_tokens) -&gt; (batch_size, beam_width * sample_expansion, n_tokens)\n        #   -&gt; (batch_size * beam_width * sample_expansion, n_tokens)\n        plan = plan.repeat(1, sample_expansion, 1).flatten(0, 1)\n        # (batch_size, beam_width, beam_steps + 1) -&gt; (batch_size * beam_width * sample_expansion, beam_steps + 1)\n        rewards = rewards.repeat(1, sample_expansion, 1).flatten(0, 1)\n\n        if kv_caches is not None:\n            # When we are in inference mode, we need to expand the kv_caches\n            # (batch_size * beam_width, n_tokens, embedding_dim) -&gt; (batch_size * beam_width * sample_expansion, n_tokens, embedding_dim)\n            new_kv_caches = []\n            for kv in kv_caches:\n                _, n_tokens, embedding_dim = kv.shape\n                new_kv_cache = (\n                    kv.view(batch_size, beam_width, n_tokens, embedding_dim)\n                    .repeat(1, sample_expansion, 1, 1)\n                    .flatten(0, 1)\n                )\n                new_kv_caches.append(new_kv_cache)\n            kv_caches = new_kv_caches\n\n        # sample actions\n        # plan (batch_size * beam_width * sample_expansion, n_tokens) -&gt; (batch_size * beam_width * sample_expansion, n_tokens + action_dim)\n        # kv_caches is updated with the new action tokens\n        plan, kv_caches, _ = sample_tokens(\n            model,\n            plan,\n            kv_caches,\n            n_steps=action_dim,\n            top_k=act_top_k,\n            temperature=temperature,\n            greedy=greedy,\n        )\n\n        # sample rewards and values\n        # plan (batch_size * beam_width * sample_expansion, n_tokens) -&gt; (batch_size * beam_width * sample_expansion, n_tokens + reward_dim + value_dim)\n        # kv_caches is updated with the new reward and value tokens\n        # logits shape (batch_size * beam_width * sample_expansion, reward_dim + value_dim, vocab_size)\n        plan, kv_caches, logits = sample_tokens(\n            model,\n            plan,\n            kv_caches,\n            n_steps=reward_dim + value_dim,\n            top_k=rew_top_k,\n            temperature=temperature,\n            greedy=greedy,\n        )\n\n        # calculate probabilities from logits\n        probs = F.softmax(logits, dim=-1)\n\n        # calculate the expected rewards and values\n        # (batch_size * beam_width * sample_expansion, reward_dim + value_dim, vocab_size)\n        #   -&gt; (batch_size * beam_width * sample_expansion, reward_dim + value_dim)\n        rewards_and_values = discretizer.expectation(\n            probs, subslice=(transition_dim - reward_dim - value_dim, transition_dim)\n        )\n\n        rewards[..., t : t + reward_dim + value_dim] = rewards_and_values\n        # Did you notice that rewards contains rewards at t and values at t+1, why?\n        #   This is only a trick to make it easier to calculate the value at t. In the next step, the value at t+1\n        #   will be overwritten by the actual reward at t+1.\n\n        # Let's talk about how we calculate the value, the value here represents the rewards to go starting beginning of beam plan.\n        #   when we want to calculate value (rewards to go) at t, we need to consider discounted rewards from 0 to t\n        #   and also future discounted rewards from t+1 to end.\n        # (batch_size * beam_width * sample_expansion, beam_steps + 1) * (beam_steps + 1) -&gt; (batch_size * beam_width * sample_expansion)\n        # the reason we care of values is that it helps us to select the best plans\n        values = (rewards * discounts).sum(dim=-1)\n\n        # select the top-k values\n        values, idx = torch.topk(values.view(batch_size, -1), k=beam_width, dim=-1)\n        # (batch_size, beam_width) -&gt; (batch_size, beam_width, 1)\n        idx = idx.unsqueeze(-1)\n\n        # shape (batch_size * beam_width * sample_expansion, beam_steps + 1) -&gt; (batch_size, beam_width * sample_expansion, beam_steps + 1)\n        rewards = rewards.view(batch_size, beam_width * sample_expansion, -1)\n\n        # the gather operation is a bit tricky, but it is used to select the rewards corresponding to the top-k values\n        # for every batch, select the rewards corresponding to the top-k values\n        # since idx contains the indices along the beam_width * sample_expansion dimension\n        # we need to repeat the idx along the last dimension to match the rewards shape,\n        # and then use it to select the rewards\n        # (batch_size, beam_width * sample_expansion, beam_steps + 1) -&gt; (batch_size, beam_width, beam_steps + 1)\n        rewards = torch.gather(rewards, 1, idx.repeat(1, 1, beam_steps + 1))\n\n        # select the top-k plans\n        # shape (batch_size * beam_width * sample_expansion, n_tokens) -&gt; (batch_size, beam_width * sample_expansion, n_tokens)\n        plan = plan.view(batch_size, beam_width * sample_expansion, -1)\n        # shape (batch_size, beam_width * sample_expansion, n_tokens) -&gt; (batch_size, beam_width, n_tokens)\n        plan = torch.gather(plan, 1, idx.repeat(1, 1, plan.shape[-1]))\n\n        # select the top-k kv_caches\n        best_kv_caches = []\n        for kv in kv_caches:\n            _, n_tokens, embedding_dim = kv.shape\n            kv = kv.view(\n                batch_size, beam_width * sample_expansion, n_tokens, embedding_dim\n            )\n            # same idea as above, repeat idx along the last 2 dimensions\n            # kv shape (batch_size, beam_width * sample_expansion, n_tokens, embedding_dim) -&gt; (batch_size, beam_width, n_tokens, embedding_dim)\n            kv = torch.gather(\n                kv, 1, idx.unsqueeze(-1).repeat(1, 1, n_tokens, embedding_dim)\n            )\n            best_kv_caches.append(kv.flatten(0, 1))\n\n        if t &lt; beam_steps - 1:\n            # sample observations only if we are not at the last step, why?\n            # because beam plan has to end with a valid transition [...., obs, act, rew, val]\n\n            # plan (batch_size, beam_width, n_tokens) -&gt; (batch_size * beam_width, n_tokens)\n            plan = plan.view(batch_size * beam_width, -1)\n\n            # sample observations\n            # plan (batch_size * beam_width, n_tokens) -&gt; (batch_size * beam_width, n_tokens + observation_dim)\n            plan, kv_caches, _ = sample_tokens(\n                model,\n                plan,\n                best_kv_caches,\n                n_steps=observation_dim,\n                top_k=obs_top_k,\n                temperature=temperature,\n                greedy=greedy,\n            )\n            # plan (batch_size * beam_width, n_tokens + observation_dim) -&gt; (batch_size, beam_width, n_tokens + observation_dim)\n            plan = plan.view(batch_size, beam_width, -1)\n\n    # (batch_size, beam_width) -&gt; (batch_size)\n    # for each batch, select the plan with the highest value and return it's index\n    argmax = torch.argmax(values, dim=-1)\n\n    # select the best plan\n    # (batch_size, beam_width, n_tokens) -&gt; (batch_size, n_tokens)\n    best_plan = plan[torch.arange(batch_size), argmax]\n    # filter out the context tokens and return the best plan as obtained from the beam search\n    best_plan = best_plan[:, context.shape[1] :]\n    return best_plan"
  },
  {
    "objectID": "content/offline_rl.html#rollout",
    "href": "content/offline_rl.html#rollout",
    "title": "Trajectory Transformer (TT) from code",
    "section": "11 Rollout",
    "text": "11 Rollout\nA rollout in TT is the process of generating a trajectory by sampling actions, predicting future states and rewards, and interacting with the environment. The model acts as a policy, guiding decision-making by simulating future transitions based on its learned distribution of trajectories.\n\n11.1 In the context of TT, rollouts are used to:\n\nEvaluate learned policies in an environment.\nSimulate future trajectories without interacting with the real world (important in offline RL).\nPlan actions over multiple time steps using beam search.\n\n\n\n11.2 How a Rollout Uses the Policy Model & Beam Search\nDuring a rollout, TT acts as a policy model, generating actions step-by-step in an autoregressive manner. The key steps are:\n\n11.2.1 Conditioning on Context\n\nThe rollout starts with a history of past states, actions, and rewards (context).\nThis context is stored in a sequence of discrete tokens.\n\n\n\n11.2.2 Predicting the Next Step\n\nThe model samples the next token (state, action, reward) from its learned distribution.\nThis is done using beam search, which expands multiple possible future trajectories and selects the best one.\n\n\n\n11.2.3 Beam Search for Improved Decision-Making\n\nInstead of greedily picking the most probable action, TT explores multiple high-reward trajectories in parallel.\nBeam search ranks these trajectories based on:\n\nReward-to-Go (RTG) (for reward-maximizing planning).\nLikelihood of actions (for policy cloning).\n\nThe highest-ranked trajectory is selected for rollout.\n\n\n\n11.2.4 Executing the Action & Updating Context\n\nThe chosen action is executed in the environment (or simulated).\nThe next observed state and reward are added to the context.\nThe process repeats until the trajectory reaches a termination condition.\n\n\n\n\n11.3 How Rollouts Enable Long-Term Planning\n\nLooks Beyond Immediate Rewards: Unlike traditional RL policies, which optimize one-step actions, TT predicts entire sequences using beam search, making decisions that maximize long-term rewards.\nHandles Multi-Step Dependencies: TT’s Transformer-based architecture models long-range dependencies, ensuring that actions are chosen not just for short-term gain but for overall trajectory optimization.\nWorks Without Online Interaction: Since rollouts are model-generated, TT can plan offline, leveraging historical data to simulate high-return behaviors without real-time environment interaction.\n\n\n@torch.no_grad()\ndef vec_rollout(\n    model: nn.Module,\n    env: DummyVecEnv,\n    discretizer: KBinsDiscretizer,\n    beam_width: int,\n    beam_steps: int,\n    beam_context: int,\n    sample_expansion: int,\n    observation_dim: int,\n    action_dim: int,\n    reward_dim: int,\n    value_dim: int,\n    transition_dim: int,\n    max_steps: int,\n    plan_every: int,\n    obs_top_k: Optional[int] = None,\n    act_top_k: Optional[int] = None,\n    rew_top_k: Optional[int] = None,\n    temperature: float = 1.0,\n    greedy: bool = False,\n    device: torch.device = torch.device(\"cpu\"),\n):\n    \"\"\"\n    What is a rollout? A rollout is a simulation of an agent interacting with the environment\n    by following a plan. The plan is generated by the model using beam search. The model\n    predicts the next action, reward, and observation conditioned on the context. The context\n    is the history of the agent's interaction with the environment.\n\n    This function is responsible for performing a rollout using the model and the environment\n    and returning the total rewards obtained by the agent.\n\n    Similar to the vec_beam_plan function, this function is a bit complex because it processes\n    multiple sequences in parallel. The complexity comes from the fact that we are using a vectorized\n    environment, which means that we are processing multiple environments in parallel.\n    \"\"\"\n    assert (\n        plan_every &lt;= beam_steps\n    ), f\"plan_every {plan_every} should be less than or equal to beam_steps {beam_steps}\"\n\n    # reset the environment amd get the initial observation\n    # in most environments, the initial observation selected randomly.\n    obs = env.reset()\n\n    # obs shape (num_envs, observation_dim)\n    obs = flatten_space(obs, env.observation_space)\n    total_rewards = np.zeros(env.num_envs)\n    context = torch.zeros(\n        (env.num_envs, (max_steps + 1) * transition_dim),\n        device=device,\n        dtype=torch.long,\n    )\n    # context_idx is used to keep track of the current index in the context\n    context_idx = 0\n\n    # discretize the observation\n    # obs_token shape (num_envs, observation_dim)\n    obs_token = discretizer.encode(obs, subslice=(0, observation_dim))\n\n    value_placeholder = np.ones((env.num_envs, value_dim)) * 1e6\n\n    # update the context with the initial observation\n    context[:, :observation_dim] = torch.tensor(obs_token, device=device)\n\n    # tensor to keep track of which environments are done\n    dones = np.zeros(env.num_envs, dtype=np.bool)\n\n    # usually max_steps is set to default max_num_steps in the environment\n    for t in trange(max_steps, desc=\"Rollout\", leave=False):\n        # Process one step in the environment\n        # one step consists of selecting an action, taking a step in the environment,\n        # and updating the context with the new observation, action, reward, and value.\n        if t % plan_every == 0:\n            # every plan_every steps, we generate a new plan using beam search\n            # and store the predicted tokens in plan_buffer.\n            # higher plan_every means we are using the same plan for longer\n            # as a result, we are putting more trust in the model's prediction\n            # of the future states, actions, and rewards.\n\n            context_idx = (\n                ((t + 1) * transition_dim) - action_dim - reward_dim - value_dim\n            )\n            context_not_dones = context[~dones, :context_idx]\n\n            # generate a new plan using beam search\n            # predicted_tokens shape (num_envs, beam_steps * transition_dim)\n            predicted_tokens = vec_beam_plan(\n                model,\n                discretizer,\n                context_not_dones,\n                beam_width,\n                beam_steps,\n                beam_context,\n                sample_expansion,\n                observation_dim,\n                action_dim,\n                reward_dim,\n                value_dim,\n                transition_dim,\n                obs_top_k=obs_top_k,\n                act_top_k=act_top_k,\n                rew_top_k=rew_top_k,\n                temperature=temperature,\n                greedy=greedy,\n            )\n            plan_buffer = torch.zeros(\n                env.num_envs,\n                predicted_tokens.shape[-1],\n                device=device,\n                dtype=predicted_tokens.dtype,\n            )\n            plan_buffer[~dones] = predicted_tokens\n        else:\n            # if we are not generating a new plan, we use the plan_buffer\n            # to get the next transition_dim number of tokens\n            plan_buffer = plan_buffer[:, transition_dim:]\n\n        # get the action from the predicted tokens\n        # action_token shape (num_envs, action_dim)\n        action_token = plan_buffer[:, :action_dim].cpu().numpy()\n        # decode the action\n        # action shape (num_envs, action_dim)\n        action = discretizer.decode(\n            action_token, subslice=(observation_dim, observation_dim + action_dim)\n        )\n        action = unflatten_space(action, env.action_space)\n        next_obs, reward, done, _ = env.step(action)\n        # next_obs shape (num_envs, observation_dim)\n        next_obs = flatten_space(next_obs, env.observation_space)\n        # discretize the next observation\n        # next_obs_token shape (num_envs, observation_dim)\n        next_obs_token = discretizer.encode(\n            next_obs[~dones], subslice=(0, observation_dim)\n        )\n        # discretize the reward and value\n        # reward_value_tokens shape (num_envs, reward_dim + value_dim)\n        reward_value_tokens = discretizer.encode(\n            np.hstack([reward.reshape(-1, reward_dim), value_placeholder]),\n            subslice=(transition_dim - reward_dim - action_dim, transition_dim),\n        )\n\n        # update the context\n        context_idx = t * transition_dim\n        # add action\n        context[\n            ~dones,\n            context_idx + observation_dim : context_idx + observation_dim + action_dim,\n        ] = torch.as_tensor(action_token[~dones], device=device)\n        # add reward and value\n        context[\n            ~dones,\n            context_idx + observation_dim + action_dim : context_idx + transition_dim,\n        ] = torch.as_tensor(reward_value_tokens[~dones], device=device)\n        # add next observation\n        context[\n            ~dones,\n            context_idx\n            + transition_dim : context_idx\n            + transition_dim\n            + observation_dim,\n        ] = torch.as_tensor(next_obs_token, device=device)\n\n        total_rewards[~dones] += reward[~dones]\n\n        dones[done] = True\n        if np.all(dones):\n            break\n    return total_rewards, dones"
  },
  {
    "objectID": "content/offline_rl.html#model-training",
    "href": "content/offline_rl.html#model-training",
    "title": "Trajectory Transformer (TT) from code",
    "section": "12 Model Training",
    "text": "12 Model Training\n\n12.1 Loss Function: Cross-Entropy Loss, Action Weighting, and Masking\n\n12.1.1 Why Cross-Entropy Loss?\nTT models trajectory prediction as a sequence modeling problem, similar to language modeling. Instead of predicting the next word, TT predicts the next token in a trajectory (state, action, reward, value). Since each token is discretized into bins, the model’s output is a classification problem over a vocabulary of discrete bins.\nCross-entropy loss is used because it is the standard for classification tasks:\n\\[\nL = -\\sum p_{\\text{true}} \\log p_{\\text{pred}}\n\\]\nwhere:\n\n\\(p_{\\text{true}}\\) is the true probability distribution (one-hot encoded).\n\\(p_{\\text{pred}}\\) is the predicted probability distribution over the discrete bins.\n\n\n\n\nLoss\n\n\n\n\n12.1.2 Why are Actions Weighted Higher?\n\nActions are weighted higher (factor of 5) because errors in action prediction have the most direct impact on trajectory optimization.\nSince TT is used for decision-time planning, predicting incorrect actions leads to poor rollouts, making action prediction more critical than observations or rewards.\n\n\n\n12.1.3 Why Use a Loss Mask?\n\nThe model operates on sequences of different lengths.\nPadding is added to ensure equal sequence lengths in a batch.\nThe loss mask prevents the model from learning from padded tokens, avoiding training artifacts.\n\n\n\n\n12.2 Vectorized Environments in Gym: What and Why?\nA vectorized environment (from Gym’s DummyVecEnv) runs multiple independent environments in parallel.\n\nInstead of rolling out one episode at a time, we can execute multiple rollouts simultaneously.\nThis provides faster evaluation by utilizing batch processing on the GPU.\n\n\n\n12.3 Learning Rate Scheduler, Weight Decay, and Gradient Clipping\n\n12.3.1 Learning Rate Scheduler: Why It’s Needed\n\nTT is a Transformer-based model, meaning it benefits from a warmup-decay schedule.\nInstead of keeping a fixed learning rate, the scheduler adjusts it dynamically based on the training step.\nThis prevents instability early in training and helps the model converge smoothly.\n\n\n\n\nLearning Rate Schedule\n\n\n\n\n12.3.2 Weight Decay: Why Regularization Matters\n\nWeight decay penalizes large weights, helping the model avoid overfitting.\nSince TT learns from a fixed dataset (offline RL), overfitting is a major concern, making regularization crucial.\n\n\n\n12.3.3 Gradient Clipping: Why It’s Used\n\nTransformers can experience exploding gradients, especially when handling long trajectories.\nClipping prevents gradients from becoming too large, stabilizing training.\nWithout clipping, loss can spike suddenly, leading to divergence.\n\n\n\n\n12.4 Predictive Accuracy: What It Is and Why It’s Not Important for TT\nPredictive accuracy measures how often the model correctly predicts the next token in a trajectory. Computed as:\n\\[\n\\text{Accuracy} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}\n\\]\n\n12.4.1 Why It’s NOT Crucial for TT?\n\nUnlike supervised learning, TT does not optimize for direct action prediction.\nA high accuracy doesn’t necessarily mean better rollouts—what matters is how well the model generates high-return trajectories.\nUsed here mainly as a diagnostic tool to check if the model is learning meaningful representations.\n\nPredictive accuracy is a useful debugging metric, but rollout performance is the real measure of success in TT.\n\n\n\nPredictive Accuracy\n\n\n\n\n\n12.5 Evaluation in TT: Key Metrics & Purpose\nModel evaluation in Trajectory Transformer (TT) focuses on rollout performance, rather than accuracy or loss. Since TT is designed for trajectory generation and decision-time planning, we assess how well it produces high-return trajectories. The key metrics are:\n\n12.5.1 Mean Reward\n\nMeasures the average cumulative reward across rollouts.\n\nA higher mean reward indicates better trajectory optimization.\n\nThe trend of mean reward over training epochs suggests that training could potentially stop at ~30 epochs for this environment.\n\n\n\n\nMean Rewards\n\n\n\n\n12.5.2 Reward Standard Deviation (Std Dev)\n\nCaptures variability in rollouts.\n\nLow std means consistent performance, while high std suggests instability.\n\n\n\n12.5.3 Done Ratio\n\nTracks how often rollouts reach termination.\n\nOnly meaningful for environments with terminal conditions (e.g., AntMaze).\n\nIn environments like HalfCheetah, which do not naturally terminate, rollouts end only when max steps are reached.\n\n\n\n12.5.4 Mean Rollout Time\n\nMeasures inference efficiency, influenced by:\n\nBeam width\n\nSequence length\n\nSampling strategy\n\n\nFaster rollouts improve real-time decision-making.\n\n\n\n\nMean Rollout Time\n\n\n\n\n12.5.5 Total Rewards Distribution\n\nShows the spread of rewards from multiple episodes.\n\nHelps detect outliers and inconsistencies beyond the mean.\n\nOver training epochs, the distribution trends upward:\n\nEarly epochs: Most rollouts accumulate negative rewards.\n\nLater epochs: Rollouts converge to positive rewards, indicating TT learns a stable policy.\n\n\n\n\n\nRewards Distribution\n\n\n\n\n12.5.6 Conclusion\nThese metrics ensure TT is not just memorizing past data, but is truly capable of effective planning. Based on evaluation results, the trained TT model successfully navigates the environment.\nIs this the most optimal policy?\nThat question requires comparing TT with other RL methods. For further analysis, please refer to the original Trajectory Transformer paper—such comparisons are beyond the scope of this notebook.\n\n\nTraining loop, loss function, and evaluation functions\ndef calculate_loss(\n    model: nn.Module,\n    batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n    vocab_size: int,\n    transition_dim: int,\n    observation_dim: int,\n    action_dim: int,\n    reward_dim: int,\n    value_dim: int,\n    device: torch.device = torch.device(\"cpu\"),\n) -&gt; torch.Tensor:\n    # inputs shape (batch_size, seq_len)\n    # targets shape (batch_size, seq_len)\n    # loss_pad_mask shape (batch_size, seq_len)\n    inputs, targets, loss_pad_mask = batch\n    inputs = inputs.to(device)\n    targets = targets.to(device)\n    loss_pad_mask = loss_pad_mask.to(device)\n    # logits shape (batch_size, seq_len, vocab_size)\n    logits, _ = model(inputs)\n    # flatten the logits and targets to shape (batch_size * seq_len, vocab_size)\n    assert logits.shape[-1] == vocab_size, \"vocab_size mismatch\"\n    logits = logits.reshape(-1, vocab_size)\n    # flatten the targets to shape (batch_size * seq_len)\n    targets = targets.reshape(-1)\n    # loss shape (batch_size * seq_len)\n    loss = F.cross_entropy(logits, targets, reduction=\"none\")\n    assert loss.shape == (inputs.shape[0] * inputs.shape[1],), \"loss shape mismatch\"\n\n    n_states = math.ceil(inputs.shape[1] / transition_dim)\n    # weights shape (observation_dim + action_dim + reward_dim + value_dim)\n    weights = torch.cat(\n        [\n            torch.ones(observation_dim, device=inputs.device),\n            torch.ones(action_dim, device=inputs.device) * 5,\n            torch.ones(reward_dim, device=inputs.device) * 1,\n            torch.ones(value_dim, device=inputs.device) * 1,\n        ]\n    )\n    # remove the first element from weights because we are starting from the second token\n    weights = weights.repeat(n_states)[1:].repeat(inputs.shape[0], 1)\n    loss = loss * weights.view(-1)\n    # apply the loss pad mask to the loss because we don't want to calculate the loss for padded values\n    loss = (loss * loss_pad_mask.view(-1)).mean()\n    return loss\n\n\ndef vec_eval(\n    env: Env,\n    model: nn.Module,\n    discretizer: KBinsDiscretizer,\n    num_episodes: int,\n    beam_width: int,\n    beam_steps: int,\n    beam_context: int,\n    sample_expansion: int,\n    observation_dim: int,\n    action_dim: int,\n    reward_dim: int,\n    value_dim: int,\n    transition_dim: int,\n    plan_every: int,\n    obs_top_k: Optional[int] = None,\n    act_top_k: Optional[int] = None,\n    rew_top_k: Optional[int] = None,\n    temperature: float = 1.0,\n    greedy: bool = False,\n    device: torch.device = torch.device(\"cpu\"),\n):\n    model.eval()\n\n    # create a vectorized environment, this allows us to run multiple environments in parallel\n    vec_env = DummyVecEnv([lambda: gym.make(env.name) for _ in range(num_episodes)])\n    start_time = time.time()\n    total_rewards, dones = vec_rollout(\n        model,\n        vec_env,\n        discretizer,\n        beam_width,\n        beam_steps,\n        beam_context,\n        sample_expansion,\n        observation_dim,\n        action_dim,\n        reward_dim,\n        value_dim,\n        transition_dim,\n        (\n            vec_env.envs[0]._max_episode_steps\n            if not local\n            else vec_env.envs[0]._max_episode_steps\n        ),\n        plan_every,\n        obs_top_k=obs_top_k,\n        act_top_k=act_top_k,\n        rew_top_k=rew_top_k,\n        temperature=temperature,\n        greedy=greedy,\n        device=device,\n    )\n    end_time = time.time()\n    mean_rewards = np.mean(total_rewards)\n    std_rewards = np.std(total_rewards)\n\n    done_ratio = np.mean(dones)\n\n    model.train()\n    return (\n        mean_rewards,\n        std_rewards,\n        done_ratio,\n        end_time - start_time,\n        0,\n        total_rewards,\n    )\n\n\n@torch.no_grad()\ndef calculate_predictive_accuracy(\n    model: nn.Module,\n    dataloader: Subset,\n    device: torch.device = torch.device(\"cpu\"),\n) -&gt; float:\n    model.eval()\n    total_correct = 0\n    total_samples = 0\n\n    # sample 10% of the data\n    sampling_rate = 0.1\n    dataloader = DataLoader(\n        dataloader.dataset,\n        batch_size=dataloader.batch_size,\n        sampler=SubsetRandomSampler(\n            np.random.choice(\n                len(dataloader.dataset),\n                int(sampling_rate * len(dataloader.dataset)),\n                replace=False,\n            )\n        ),\n    )\n\n    for data in tqdm(dataloader, desc=\"Calculating predictive accuracy\", leave=False):\n        x, y, mask = data\n        x, y, mask = x.to(device), y.to(device), mask.to(device)\n        logits, _ = model(x)\n        # (batch_size, seq_len, vocab_size) -&gt; (batch_size * seq_len, vocab_size)\n        logits = logits.reshape(-1, logits.shape[-1])\n        y = y.reshape(-1)\n        mask = mask.reshape(-1)\n        # only consider the tokens that are not masked\n        mask_idx = mask.nonzero(as_tuple=True)[0]\n        y = y[mask_idx]\n        logits = logits[mask_idx]\n        # (batch_size * seq_len) -&gt; (batch_size)\n        y_pred = torch.argmax(logits, dim=-1)\n        correct = (y_pred == y).sum().item()\n        total_correct += correct\n        total_samples += y.shape[0]\n    model.train()\n    return total_correct / total_samples\n\n\ndef train(\n    model: nn.Module,\n    train_dataloader: DataLoader,\n    test_dataloader: DataLoader,\n    discretizer: KBinsDiscretizer,\n    optimizer: torch.optim.Optimizer,\n    scheduler: GPTScheduler,\n    vocab_size: int,\n    n_epochs: int,\n    writer: SummaryWriter,\n    device: torch.device = torch.device(\"cpu\"),\n    eval_every: int = 10,\n    checkpoint_path: Optional[str] = None,\n    clip_grad: Optional[float] = None,\n):\n    model.train()\n    step = 0\n    for epoch in trange(n_epochs, desc=\"Training\"):\n        start_time = time.time()\n        total_loss = 0\n        for batch in tqdm(\n            train_dataloader, desc=f\"Epoch {epoch + 1}/{n_epochs}\", leave=False\n        ):\n            loss = calculate_loss(\n                model,\n                batch,\n                vocab_size,\n                device=device,\n                transition_dim=transition_dim,\n                observation_dim=observation_dim,\n                action_dim=action_dim,\n                reward_dim=reward_dim,\n                value_dim=value_dim,\n            )\n\n            _batch_tokens = batch[0].reshape(-1).shape[0]\n\n            # write learning rate to tensorboard\n            writer.add_scalar(\"Learning rate\", scheduler.get_current_lr(), step)\n\n            scheduler.step(batch_size=_batch_tokens)\n            optimizer.zero_grad()\n            loss.backward()\n            if clip_grad is not None:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n            optimizer.step()\n\n            writer.add_scalar(\"Loss/train\", loss.item(), step)\n            total_loss += loss.item()\n            step += 1\n        writer.add_scalar(\"Epoch\", epoch, epoch)\n        end_time = time.time()\n        writer.add_scalar(\"Time/train\", end_time - start_time, epoch)\n\n        train_accuracy = calculate_predictive_accuracy(\n            model, train_dataloader, device=device\n        )\n        if len(test_dataloader) &gt; 0:\n            test_accuracy = calculate_predictive_accuracy(\n                model, test_dataloader, device=device\n            )\n        else:\n            test_accuracy = 0\n        writer.add_scalar(\"Accuracy/train\", train_accuracy, epoch)\n        writer.add_scalar(\"Accuracy/test\", test_accuracy, epoch)\n\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                writer.add_histogram(f\"weights/{name}\", param.data.cpu().numpy(), epoch)\n                writer.add_histogram(\n                    f\"gradients/{name}\", param.grad.cpu().numpy(), epoch\n                )\n\n        # +1 because this operation is pointless to do for first epoch.\n        if (epoch) % eval_every == 0:\n            start_time = time.time()\n\n            (\n                mean_rewards,\n                std_rewards,\n                done_ratio,\n                mean_rollout_time,\n                std_rollout_time,\n                total_rewards,  # total_rewards\n            ) = vec_eval(\n                env,\n                model,\n                discretizer,\n                num_episodes=10 if not local else 1,\n                beam_width=32 if not local else 2,\n                beam_steps=5 if not local else 2,\n                beam_context=5 if not local else 2,\n                sample_expansion=2 if not local else 1,\n                observation_dim=observation_dim,\n                action_dim=action_dim,\n                reward_dim=reward_dim,\n                value_dim=value_dim,\n                transition_dim=transition_dim,\n                plan_every=1 if not local else 2,\n                obs_top_k=1,\n                act_top_k=None,\n                rew_top_k=1,\n                temperature=1.0,\n                greedy=False,\n                device=device,\n            )\n            writer.add_scalar(\"Reward/mean\", mean_rewards, epoch)\n            writer.add_scalar(\"Reward/std\", std_rewards, epoch)\n            writer.add_scalar(\"Done ratio\", done_ratio, epoch)\n            writer.add_scalar(\"Rollout time/mean\", mean_rollout_time, epoch)\n            writer.add_scalar(\"Rollout time/std\", std_rollout_time, epoch)\n\n            # log total_rewards as histogram\n            total_rewards = np.array(total_rewards)\n            writer.add_histogram(\"Total rewards\", total_rewards, epoch)\n\n            end_time = time.time()\n            writer.add_scalar(\"Time/eval\", end_time - start_time, epoch)\n\n        if checkpoint_path:\n            torch.save(model.state_dict(), checkpoint_path + f\"model_{epoch}.pth\")\n            torch.save(optimizer.state_dict(), checkpoint_path + \"optimizer.pth\")\n\n        writer.flush()\n\n\n\n\n\n12.6 Hardware & Training Details\nThis notebook was developed and tested on different hardware setups to balance ease of development and computational efficiency:\n\n12.6.1 Development Setup:\n\nDevice: 2024 M3 MacBook Air\nSpecs: 16GB RAM\nLimitation: Model training and rollouts could not run due to insufficient compute power.\n\n\n\n12.6.2 Final Training & Rollout Setup:\n\nInstance: Rented from vast.ai at $0.2/hr\nSpecs:\n\nGPU: 1× RTX 4070S\nCPU: AMD Ryzen 9 7950X (16-Core)\nRAM: 64GB\nStorage: 49.2GB available disk space\n\n\n\n\n12.6.3 Training Time:\n\nFull training, including evaluations, took approximately 10 hours.\n\n\n\n\nTraining/Eval Times\n\n\n\n\nMain training runner\nprint(f\"Using device: {device}\")\nwriter = SummaryWriter()\n\ndataset.discretizer.to(device)\n# split the dataset into train and test\ntrain_size = int(len(dataset) * 1.0)\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = torch.utils.data.random_split(\n    dataset, [train_size, test_size]\n)\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    num_workers=8 if not local else 0,\n    shuffle=True,\n    pin_memory=True,\n)\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    shuffle=False,\n)\nmodel = TrajectoryTransformer(\n    seq_len,\n    embedding_dim,\n    n_heads,\n    transition_dim,\n    n_blocks,\n    vocab_size,\n    use_sep_heads=use_sep_heads,\n).to(device)\n\nwarmup_tokens = len(train_dataset) * seq_len\nfinal_tokens = n_epochs * warmup_tokens\n\n# write hyper parameters\nwriter.add_hparams(\n    {\n        \"seq_len\": seq_len,\n        \"embedding_dim\": embedding_dim,\n        \"n_heads\": n_heads,\n        \"transition_dim\": transition_dim,\n        \"n_blocks\": n_blocks,\n        \"vocab_size\": vocab_size,\n        \"use_sep_heads\": use_sep_heads,\n        \"weight_decay\": weight_decay,\n        \"lr\": lr,\n        \"betas\": torch.tensor(betas),\n        \"batch_size\": batch_size,\n        \"n_epochs\": n_epochs,\n        \"eval_every\": eval_every,\n        \"clip_grad\": clip_grad,\n        \"warmup_tokens\": warmup_tokens,\n        \"final_tokens\": final_tokens,\n        \"env_name\": env.name,\n    },\n    {},\n)\n\noptimizer = get_optimizer(model, weight_decay, lr, betas)\nscheduler = get_scheduler(optimizer, warmup_tokens, final_tokens)\nif load_checkpoint:\n    print(f\"Loading model from {checkpoint_path}\")\n    model_files = glob.glob(checkpoint_path + \"model*.pth\")\n    if model_files:\n        latest_model_file = max(model_files, key=os.path.getctime)\n        print(f\"Loading model from {latest_model_file}\")\n        model.load_state_dict(torch.load(latest_model_file, map_location=device))\n    else:\n        print(\"No model files found, starting training from scratch.\")\n        raise ValueError(\"No model files found, starting training from scratch.\")\n    optimizer.load_state_dict(\n        torch.load(checkpoint_path + \"optimizer.pth\", map_location=device)\n    )\nelse:\n    train(\n        model,\n        train_dataloader,\n        test_dataloader,\n        dataset.discretizer,\n        optimizer,\n        scheduler,\n        vocab_size,\n        n_epochs,\n        writer,\n        device=device,\n        eval_every=eval_every,\n        checkpoint_path=checkpoint_path,\n        clip_grad=clip_grad,\n    )\n    print(f\"Saved checkpoint to {checkpoint_path}\")\n\n\nUsing device: mps\n\n\n\n\n\n\n\n\n\n\n\n/Users/abrar/Library/Caches/pypoetry/virtualenvs/blog-FROQ9Grm-py3.11/lib/python3.11/site-packages/gymnasium/envs/registration.py:525: UserWarning: WARN: Using the latest versioned environment `HalfCheetah-v5` instead of the unversioned environment `HalfCheetah`.\n  logger.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSaved checkpoint to data/mujoco/halfcheetah/expert-v0/"
  },
  {
    "objectID": "content/offline_rl.html#future-work",
    "href": "content/offline_rl.html#future-work",
    "title": "Trajectory Transformer (TT) from code",
    "section": "13 Future Work",
    "text": "13 Future Work\n\n13.1 Expand to More Environments\n\nTest TT on diverse RL tasks: locomotion (Walker2D, Humanoid), navigation (AntMaze), and robotics (FrankaKitchen), environments with visual inputs.\n\n\n\n13.2 Speed Up Rollouts\n\nOptimize inference using:\nParallel token sampling to reduce autoregressive overhead.\nEfficient beam search with pruning or heuristics.\nCUDA/TensorRT acceleration for faster execution.\n\n\n\n13.3 Improve Training Efficiency\n\nReduce training time using:\nGradient checkpointing for memory efficiency.\nFlash Attention for optimized transformer computations.\nMixed-precision (FP16) for faster execution.\nBetter discretization to improve tokenization.\n\n\n\n13.4 Enable Visualization\n\nImplement env.render() to visualize rollouts.\n\nGenerate trajectory videos to analyze model behavior.\n\n\n\n13.5 Benchmark Against Other RL Methods\n\nCompare TT with:\nDecision Transformer (DT) – Sequence modeling baseline.\nCQL – Offline RL with conservative action selection. TD3+BC – Hybrid RL with behavioral cloning.\n\n\n\n13.6 Other learning techniques\n\nGoal conditioned\nImitation learning\n\nIf you are interested in contributing or collaborating on any of these, please open a issue on the github project."
  }
]